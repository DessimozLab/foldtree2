{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#read the afdb clusters file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "#autoreload\n",
    "from src import AFDB_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reps       entryId       repId    taxId\n",
      "0  A0A009E921  A0A009E921  1310605\n",
      "1  A0A009F5K6  A0A009E921  1310605\n",
      "2  A0A009E9H3  A0A009E9H3  1310605\n",
      "3  A0A484ZLT0  A0A009E9H3    82979\n",
      "4  A0A009ECR5  A0A009ECR5  1310605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read the afdb rep file\n",
    "reps = pd.read_table( 'afdbclusters/1-AFDBClusters-entryId_repId_taxId.tsv', header=None, names=['entryId', 'repId', 'taxId'] )\n",
    "print( 'reps' , reps.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A3D4Y7U3', 'A0A182QEX7', 'A0A1B6MIL6', 'A0A2X0NIQ0', 'A0A495JSP3', 'A0A3C0CY76', 'A0A1Z9J3Y7', 'F4IL37', 'A0A6M1NDP9', 'A0A368W5L6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structs = glob.glob( 'structs/*.pdb' )\n",
    "#remove the .pdb extension\n",
    "structs = [ s.split( '/' )[-1].split( '.' )[0] for s in structs ]\n",
    "print(structs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34870           entryId       repId    taxId\n",
      "7176   A0A011Q6F0  A0A011Q6F0  1454005\n",
      "7177   A0A838GEN5  A0A011Q6F0  2448782\n",
      "14470  A0A015KR17  A0A015KR17  1432141\n",
      "14471  A0A2N0NP60  A0A015KR17   588596\n",
      "14472  A0A2Z6S933  A0A015KR17    94130\n"
     ]
    }
   ],
   "source": [
    "#select the reps that have structures\n",
    "reps = reps[ reps['repId'].isin( structs ) ]\n",
    "print(  len(reps)  , reps.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a structure alignment directory\n",
    "if not os.path.exists( 'struct_align' ):\n",
    "    os.makedirs( 'struct_align' )\n",
    "\n",
    "#make a directory for each cluster representative\n",
    "for rep in reps['repId']:\n",
    "    if not os.path.exists( 'struct_align/' + rep  ):\n",
    "        os.makedirs( 'struct_align/' + rep  )\n",
    "    if not os.path.exists( 'struct_align/' + rep  + '/structs/'):\n",
    "        os.makedirs( 'struct_align/' + rep + '/structs/' )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2197/2197 [07:49<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#download n struct members for each cluster\n",
    "import tqdm\n",
    "n = 5\n",
    "for rep in tqdm.tqdm(reps.repId.unique() ):\n",
    "    subdf = reps[ reps['repId'] == rep ]\n",
    "    if len(subdf) < n:\n",
    "        n = len(subdf)\n",
    "    subdf = subdf.sample( n = n  )\n",
    "    subdf = subdf.head( n )\n",
    "    #download the structures\n",
    "    for uniID in subdf['entryId']:\n",
    "        AFDB_tools.grab_struct(uniID , structfolder='struct_align/' + rep  + '/structs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each folder in struct_align, align the structures with all vs all using foldseek\n",
    "from src import foldseek2tree\n",
    "import tqdm\n",
    "\n",
    "for rep in tqdm.tqdm(reps.repId.unique() ):\n",
    "    #align the structures\n",
    "    foldseek2tree.runFoldseek_allvall_EZsearch( infolder='struct_align/' + rep  + '/structs/', outpath='struct_align/' + rep + '/allvall.csv' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive embeddings for all structures in the struct_align folder\n",
    "#derive charatcters for 10,20,40,80,128,256,512 kmeans clusters\n",
    "charsets = [10,20,40,80,128,256,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t'}, 20: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13'}, 40: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\"}, 80: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O'}, 128: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f'}, 256: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f', 128: '\\x80', 129: '\\x81', 130: '\\x82', 131: '\\x83', 132: '\\x84', 133: '\\x85', 134: '\\x86', 135: '\\x87', 136: '\\x88', 137: '\\x89', 138: '\\x8a', 139: '\\x8b', 140: '\\x8c', 141: '\\x8d', 142: '\\x8e', 143: '\\x8f', 144: '\\x90', 145: '\\x91', 146: '\\x92', 147: '\\x93', 148: '\\x94', 149: '\\x95', 150: '\\x96', 151: '\\x97', 152: '\\x98', 153: '\\x99', 154: '\\x9a', 155: '\\x9b', 156: '\\x9c', 157: '\\x9d', 158: '\\x9e', 159: '\\x9f', 160: '\\xa0', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 173: '\\xad', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ'}, 512: {0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f', 128: '\\x80', 129: '\\x81', 130: '\\x82', 131: '\\x83', 132: '\\x84', 133: '\\x85', 134: '\\x86', 135: '\\x87', 136: '\\x88', 137: '\\x89', 138: '\\x8a', 139: '\\x8b', 140: '\\x8c', 141: '\\x8d', 142: '\\x8e', 143: '\\x8f', 144: '\\x90', 145: '\\x91', 146: '\\x92', 147: '\\x93', 148: '\\x94', 149: '\\x95', 150: '\\x96', 151: '\\x97', 152: '\\x98', 153: '\\x99', 154: '\\x9a', 155: '\\x9b', 156: '\\x9c', 157: '\\x9d', 158: '\\x9e', 159: '\\x9f', 160: '\\xa0', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 173: '\\xad', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 256: 'Ā', 257: 'ā', 258: 'Ă', 259: 'ă', 260: 'Ą', 261: 'ą', 262: 'Ć', 263: 'ć', 264: 'Ĉ', 265: 'ĉ', 266: 'Ċ', 267: 'ċ', 268: 'Č', 269: 'č', 270: 'Ď', 271: 'ď', 272: 'Đ', 273: 'đ', 274: 'Ē', 275: 'ē', 276: 'Ĕ', 277: 'ĕ', 278: 'Ė', 279: 'ė', 280: 'Ę', 281: 'ę', 282: 'Ě', 283: 'ě', 284: 'Ĝ', 285: 'ĝ', 286: 'Ğ', 287: 'ğ', 288: 'Ġ', 289: 'ġ', 290: 'Ģ', 291: 'ģ', 292: 'Ĥ', 293: 'ĥ', 294: 'Ħ', 295: 'ħ', 296: 'Ĩ', 297: 'ĩ', 298: 'Ī', 299: 'ī', 300: 'Ĭ', 301: 'ĭ', 302: 'Į', 303: 'į', 304: 'İ', 305: 'ı', 306: 'Ĳ', 307: 'ĳ', 308: 'Ĵ', 309: 'ĵ', 310: 'Ķ', 311: 'ķ', 312: 'ĸ', 313: 'Ĺ', 314: 'ĺ', 315: 'Ļ', 316: 'ļ', 317: 'Ľ', 318: 'ľ', 319: 'Ŀ', 320: 'ŀ', 321: 'Ł', 322: 'ł', 323: 'Ń', 324: 'ń', 325: 'Ņ', 326: 'ņ', 327: 'Ň', 328: 'ň', 329: 'ŉ', 330: 'Ŋ', 331: 'ŋ', 332: 'Ō', 333: 'ō', 334: 'Ŏ', 335: 'ŏ', 336: 'Ő', 337: 'ő', 338: 'Œ', 339: 'œ', 340: 'Ŕ', 341: 'ŕ', 342: 'Ŗ', 343: 'ŗ', 344: 'Ř', 345: 'ř', 346: 'Ś', 347: 'ś', 348: 'Ŝ', 349: 'ŝ', 350: 'Ş', 351: 'ş', 352: 'Š', 353: 'š', 354: 'Ţ', 355: 'ţ', 356: 'Ť', 357: 'ť', 358: 'Ŧ', 359: 'ŧ', 360: 'Ũ', 361: 'ũ', 362: 'Ū', 363: 'ū', 364: 'Ŭ', 365: 'ŭ', 366: 'Ů', 367: 'ů', 368: 'Ű', 369: 'ű', 370: 'Ų', 371: 'ų', 372: 'Ŵ', 373: 'ŵ', 374: 'Ŷ', 375: 'ŷ', 376: 'Ÿ', 377: 'Ź', 378: 'ź', 379: 'Ż', 380: 'ż', 381: 'Ž', 382: 'ž', 383: 'ſ', 384: 'ƀ', 385: 'Ɓ', 386: 'Ƃ', 387: 'ƃ', 388: 'Ƅ', 389: 'ƅ', 390: 'Ɔ', 391: 'Ƈ', 392: 'ƈ', 393: 'Ɖ', 394: 'Ɗ', 395: 'Ƌ', 396: 'ƌ', 397: 'ƍ', 398: 'Ǝ', 399: 'Ə', 400: 'Ɛ', 401: 'Ƒ', 402: 'ƒ', 403: 'Ɠ', 404: 'Ɣ', 405: 'ƕ', 406: 'Ɩ', 407: 'Ɨ', 408: 'Ƙ', 409: 'ƙ', 410: 'ƚ', 411: 'ƛ', 412: 'Ɯ', 413: 'Ɲ', 414: 'ƞ', 415: 'Ɵ', 416: 'Ơ', 417: 'ơ', 418: 'Ƣ', 419: 'ƣ', 420: 'Ƥ', 421: 'ƥ', 422: 'Ʀ', 423: 'Ƨ', 424: 'ƨ', 425: 'Ʃ', 426: 'ƪ', 427: 'ƫ', 428: 'Ƭ', 429: 'ƭ', 430: 'Ʈ', 431: 'Ư', 432: 'ư', 433: 'Ʊ', 434: 'Ʋ', 435: 'Ƴ', 436: 'ƴ', 437: 'Ƶ', 438: 'ƶ', 439: 'Ʒ', 440: 'Ƹ', 441: 'ƹ', 442: 'ƺ', 443: 'ƻ', 444: 'Ƽ', 445: 'ƽ', 446: 'ƾ', 447: 'ƿ', 448: 'ǀ', 449: 'ǁ', 450: 'ǂ', 451: 'ǃ', 452: 'Ǆ', 453: 'ǅ', 454: 'ǆ', 455: 'Ǉ', 456: 'ǈ', 457: 'ǉ', 458: 'Ǌ', 459: 'ǋ', 460: 'ǌ', 461: 'Ǎ', 462: 'ǎ', 463: 'Ǐ', 464: 'ǐ', 465: 'Ǒ', 466: 'ǒ', 467: 'Ǔ', 468: 'ǔ', 469: 'Ǖ', 470: 'ǖ', 471: 'Ǘ', 472: 'ǘ', 473: 'Ǚ', 474: 'ǚ', 475: 'Ǜ', 476: 'ǜ', 477: 'ǝ', 478: 'Ǟ', 479: 'ǟ', 480: 'Ǡ', 481: 'ǡ', 482: 'Ǣ', 483: 'ǣ', 484: 'Ǥ', 485: 'ǥ', 486: 'Ǧ', 487: 'ǧ', 488: 'Ǩ', 489: 'ǩ', 490: 'Ǫ', 491: 'ǫ', 492: 'Ǭ', 493: 'ǭ', 494: 'Ǯ', 495: 'ǯ', 496: 'ǰ', 497: 'Ǳ', 498: 'ǲ', 499: 'ǳ', 500: 'Ǵ', 501: 'ǵ', 502: 'Ƕ', 503: 'Ƿ', 504: 'Ǹ', 505: 'ǹ', 506: 'Ǻ', 507: 'ǻ', 508: 'Ǽ', 509: 'ǽ', 510: 'Ǿ', 511: 'ǿ'}}\n"
     ]
    }
   ],
   "source": [
    "submats = { c: np.zeros( ( c , c ) ) for c in charsets }\n",
    "#change the character number to an ascii character\n",
    "colmap = { c:{ i: chr(i) for i in range( c ) } for c in charsets }\n",
    "revcolmap = { c:{ chr(i): i for i in range( c ) } for c in charsets }\n",
    "print( colmap )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                | 0/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct_align\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s2 \u001b[38;5;129;01min\u001b[39;00m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct_align\u001b[39m\u001b[38;5;124m'\u001b[39m][s][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructs\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m             zstack\u001b[38;5;241m.\u001b[39mappend(\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstruct_align\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m)\n\u001b[1;32m     10\u001b[0m zstack \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(zstack)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "pdbfiles_structalign = glob.glob('./struct_align/*/structs/*.pdb')\n",
    "import h5py\n",
    "filename = 'structs_structalign_encoded.h5'\n",
    "zstack = []\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    for s in tqdm.tqdm(f['struct_align']):\n",
    "        for s2 in f['struct_align'][s]['structs']:\n",
    "            zstack.append(f['struct_align'][s]['structs'][s2]['z'])\n",
    "zstack = np.vstack(zstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for nclusters in [ 20, 50 , 80,  100, 200, 256]:\n",
    "    kmeans = KMeans(n_clusters=nclusters, random_state=0).fit( zstack )\n",
    "    centers = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    score = silhouette_score(zstack, kmeans.predict(zstack))\n",
    "    scores.append(score)\n",
    "    \n",
    "    #save each of the kmeans \n",
    "    if not os.path.exists(kmeans_dir):\n",
    "        os.makedirs(kmeans_dir)\n",
    "    with open(kmeans_dir + str(nclusters)+'_kmeans.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "    #get the covariance within clusters\n",
    "    cov = np.zeros((nclusters, zvals.shape[1], zvals.shape[1]))\n",
    "    for i in range(nclusters):\n",
    "        cov[i] = np.cov(zvals[labels == i].T)\n",
    "    print(cov.shape)\n",
    "\n",
    "    #get the means within clusters\n",
    "    means = np.zeros((nclusters, zvals.shape[1]))\n",
    "    for i in range(nclusters):\n",
    "        means[i] = np.mean(zvals[labels == i], axis=0)\n",
    "    print(means.shape)\n",
    "\n",
    "    #save the mean and covariance for clusters in .npy\n",
    "    np.save(kmeans_dir + str(nclusters)+'_means.npy', means)\n",
    "    np.save(kmeans_dir + str(nclusters)+'_cov.npy', cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import DataLoader\n",
    "#create a training loop for the GAE model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device( 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "encoder_save = 'encoder_mk2_aa_10'\n",
    "decoder_save = 'decoder_mk2_aa_20'\n",
    "\n",
    "#save the blank encoder and decoder\n",
    "with open(enconder_save + '.pkl' , 'rb') as encodeout:\n",
    "    encoder = pickle.loads( encodeout.read() )\n",
    "with open(decoder_save + '.pkl' , 'rb') as encodeout:\n",
    "    decoder = pickle.loads( encodeout.read() )\n",
    "\n",
    "if os.path.exists(encoder_save+ '.pth') and os.path.exists(decoder_save+ '.pth'):\n",
    "    encoder.load_state_dict(torch.load(encoder_save + '.pth'))\n",
    "    decoder.load_state_dict(torch.load(decoder_save + '.pth' ))\n",
    "\n",
    "#put encoder and decoder on the device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "for rep in tqdm.tqdm(reps.repId.unique() ):\n",
    "    #load the all vs all aln\n",
    "    aln_df = pd.read_tsv('struct_align/' + rep + '/allvall.csv')\n",
    "    #load the embedding of the structures\n",
    "    q = aln_df['s1'].unique()\n",
    "    t = aln_df['s2'].unique()\n",
    "    for q in aln_df['s1'].unique():\n",
    "        for t in aln_df['s2'].unique():\n",
    "            if q != t:\n",
    "                #align the structures\n",
    "                aln = aln_df[ (aln_df['s1'] == q) & (aln_df['s2'] == t) ]\n",
    "                qaln = aln.qaln\n",
    "                taln = aln.taln\n",
    "                for charset in charsets:\n",
    "                    #derive the embeddings\n",
    "                    \n",
    "                    with h5py.File('aln_embeds/' + rep + '.h5' , 'r') as hf:\n",
    "                        q_embeds = iter(hf[q][charset].decode())\n",
    "                        t_embeds = iter(hf[q][charset].decode())\n",
    "                    \n",
    "                    #transfer the alignments to the embeddings\n",
    "                    qaln_ft2 = ''.join([ next(q_embeds) if x == '-' else x for x in qaln ])\n",
    "                    taln_ft2 = ''.join([ next(t_embeds) if x == '-' else x for x in taln ])\n",
    "                    \n",
    "                    alnzip = zip( qaln_ft2 , taln_ft2 )\n",
    "                    for qchar, tchar in alnzip:\n",
    "                        if qchar != '-' and tchar != '-':\n",
    "                            submats[charset][ colmap[charset][qchar] , colmap[charset][tchar] ] += 1\n",
    "                            submats[charset][ colmap[charset][tchar] , colmap[charset][qchar] ] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the submats in raw form\n",
    "for charset in charsets:\n",
    "    np.save( 'submats/' + str(charset) + '.npy' , submats[charset] )\n",
    "    with open( 'submats/' + str(charset) + '.txt' , 'w' ) as f:\n",
    "        for i in range( charset ):\n",
    "            f.write( '\\t'.join( [ str(submats[charset][i,j]) for j in range( charset ) ] ) + '\\n' )\n",
    "\n",
    "#normalize the submats rows and columns to sum to 1\n",
    "for charset in charsets:\n",
    "    rowsums = submats[charset].sum( axis=1 )\n",
    "    colsums = submats[charset].sum( axis=0 )\n",
    "    for i in range( charset ):\n",
    "        submats[charset][i,:] = submats[charset][i,:] / rowsums[i]\n",
    "        submats[charset][:,i] = submats[charset][:,i] / colsums[i]\n",
    "\n",
    "#save the submats in normalized form\n",
    "for charset in charsets:\n",
    "    np.save( 'submats/' + str(charset) + '_norm.npy' , submats[charset] )\n",
    "    with open( 'submats/' + str(charset) + '_norm.txt' , 'w' ) as f:\n",
    "        for i in range( charset ):\n",
    "            f.write( '\\t'.join( [ str(submats[charset][i,j]) for j in range( charset ) ] ) + '\\n' )\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
