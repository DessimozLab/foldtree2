{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#read the afdb clusters file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "#autoreload\n",
    "from src import AFDB_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reps       entryId       repId    taxId\n",
      "0  A0A009E921  A0A009E921  1310605\n",
      "1  A0A009F5K6  A0A009E921  1310605\n",
      "2  A0A009E9H3  A0A009E9H3  1310605\n",
      "3  A0A484ZLT0  A0A009E9H3    82979\n",
      "4  A0A009ECR5  A0A009ECR5  1310605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read the afdb rep file\n",
    "reps = pd.read_table( 'afdbclusters/1-AFDBClusters-entryId_repId_taxId.tsv', header=None, names=['entryId', 'repId', 'taxId'] )\n",
    "print( 'reps' , reps.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A182QEX7', 'A0A1B6MIL6', 'A0A2X0NIQ0', 'A0A3C0CY76', 'A0A1Z9J3Y7', 'F4IL37', 'A0A093ZM93', 'A0A662CHN5', 'A0A1Z4SDS6', 'A0A5B7XUB5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structs = glob.glob( 'structs/*.pdb' )\n",
    "#remove the .pdb extension\n",
    "structs = [ s.split( '/' )[-1].split( '.' )[0] for s in structs ]\n",
    "print(structs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24081           entryId       repId    taxId\n",
      "7176   A0A011Q6F0  A0A011Q6F0  1454005\n",
      "7177   A0A838GEN5  A0A011Q6F0  2448782\n",
      "14470  A0A015KR17  A0A015KR17  1432141\n",
      "14471  A0A2N0NP60  A0A015KR17   588596\n",
      "14472  A0A2Z6S933  A0A015KR17    94130\n"
     ]
    }
   ],
   "source": [
    "#select the reps that have structures\n",
    "reps = reps[ reps['repId'].isin( structs ) ]\n",
    "print(  len(reps)  , reps.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a structure alignment directory\n",
    "if not os.path.exists( 'struct_align' ):\n",
    "    os.makedirs( 'struct_align' )\n",
    "\n",
    "#make a directory for each cluster representative\n",
    "for rep in reps['repId']:\n",
    "    if not os.path.exists( 'struct_align/' + rep  ):\n",
    "        os.makedirs( 'struct_align/' + rep  )\n",
    "    if not os.path.exists( 'struct_align/' + rep  + '/structs/'):\n",
    "        os.makedirs( 'struct_align/' + rep + '/structs/' )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                | 0/1400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1400/1400 [03:33<00:00,  6.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#download n struct members for each cluster\n",
    "import tqdm\n",
    "n = 5\n",
    "for rep in tqdm.tqdm(reps.repId.unique() ):\n",
    "    subdf = reps[ reps['repId'] == rep ]\n",
    "    if len(subdf) < n:\n",
    "        n = len(subdf)\n",
    "    subdf = subdf.sample( n = n  )\n",
    "    subdf = subdf.head( n )\n",
    "    #download the structures\n",
    "    for uniID in subdf['entryId']:\n",
    "        AFDB_tools.grab_struct(uniID , structfolder='struct_align/' + rep  + '/structs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each folder in struct_align, align the structures with all vs all using foldseek\n",
    "from src import foldseek2tree\n",
    "import tqdm\n",
    "\n",
    "for rep in tqdm.tqdm(reps.repId.unique() ):\n",
    "    #align the structures\n",
    "    foldseek2tree.runFoldseek_allvall_EZsearch( infolder='struct_align/' + rep  + '/structs/', outpath='struct_align/' + rep + '/allvall.csv' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive embeddings for all structures in the struct_align folder\n",
    "#derive charatcters for 10,20,40,80,128,256,512 kmeans clusters\n",
    "charsets = [10,20,40,80,128,256,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'charsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m submats \u001b[38;5;241m=\u001b[39m { c: np\u001b[38;5;241m.\u001b[39mzeros( ( c , c ) ) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcharsets\u001b[49m }\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#change the character number to an ascii character\u001b[39;00m\n\u001b[1;32m      3\u001b[0m colmap \u001b[38;5;241m=\u001b[39m { c:{ i: \u001b[38;5;28mchr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( c ) } \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m charsets }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'charsets' is not defined"
     ]
    }
   ],
   "source": [
    "submats = { c: np.zeros( ( c , c ) ) for c in charsets }\n",
    "#change the character number to an ascii character\n",
    "colmap = { c:{ i: chr(i) for i in range( c ) } for c in charsets }\n",
    "revcolmap = { c:{ chr(i): i for i in range( c ) } for c in charsets }\n",
    "print( colmap )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import DataLoader\n",
    "#create a training loop for the GAE model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device( 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "encoder_save = 'encoder_mk2_aa_10'\n",
    "decoder_save = 'decoder_mk2_aa_20'\n",
    "\n",
    "#save the blank encoder and decoder\n",
    "with open(enconder_save + '.pkl' , 'rb') as encodeout:\n",
    "    encoder = pickle.loads( encodeout.read() )\n",
    "with open(decoder_save + '.pkl' , 'rb') as encodeout:\n",
    "    decoder = pickle.loads( encodeout.read() )\n",
    "\n",
    "if os.path.exists(encoder_save+ '.pth') and os.path.exists(decoder_save+ '.pth'):\n",
    "    encoder.load_state_dict(torch.load(encoder_save + '.pth'))\n",
    "    decoder.load_state_dict(torch.load(decoder_save + '.pth' ))\n",
    "\n",
    "#put encoder and decoder on the device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "for rep in tqdm.tqdm(reps.repId.unique() ):\n",
    "    #load the all vs all aln\n",
    "    aln_df = pd.read_tsv('struct_align/' + rep + '/allvall.csv')\n",
    "    #load the embedding of the structures\n",
    "    q = aln_df['s1'].unique()\n",
    "    t = aln_df['s2'].unique()\n",
    "    for q in aln_df['s1'].unique():\n",
    "        for t in aln_df['s2'].unique():\n",
    "            if q != t:\n",
    "                #align the structures\n",
    "                aln = aln_df[ (aln_df['s1'] == q) & (aln_df['s2'] == t) ]\n",
    "                qaln = aln.qaln\n",
    "                taln = aln.taln\n",
    "                for charset in charsets:\n",
    "                    #derive the embeddings\n",
    "                    \n",
    "                    with h5py.File('aln_embeds/' + rep + '.h5' , 'r') as hf:\n",
    "                        q_embeds = iter(hf[q][charset].decode())\n",
    "                        t_embeds = iter(hf[q][charset].decode())\n",
    "                    \n",
    "                    #transfer the alignments to the embeddings\n",
    "                    qaln_ft2 = ''.join([ next(q_embeds) if x == '-' else x for x in qaln ])\n",
    "                    taln_ft2 = ''.join([ next(t_embeds) if x == '-' else x for x in taln ])\n",
    "                    \n",
    "                    alnzip = zip( qaln_ft2 , taln_ft2 )\n",
    "                    for qchar, tchar in alnzip:\n",
    "                        if qchar != '-' and tchar != '-':\n",
    "                            submats[charset][ colmap[charset][qchar] , colmap[charset][tchar] ] += 1\n",
    "                            submats[charset][ colmap[charset][tchar] , colmap[charset][qchar] ] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the submats in raw form\n",
    "for charset in charsets:\n",
    "    np.save( 'submats/' + str(charset) + '.npy' , submats[charset] )\n",
    "    with open( 'submats/' + str(charset) + '.txt' , 'w' ) as f:\n",
    "        for i in range( charset ):\n",
    "            f.write( '\\t'.join( [ str(submats[charset][i,j]) for j in range( charset ) ] ) + '\\n' )\n",
    "\n",
    "#normalize the submats rows and columns to sum to 1\n",
    "for charset in charsets:\n",
    "    rowsums = submats[charset].sum( axis=1 )\n",
    "    colsums = submats[charset].sum( axis=0 )\n",
    "    for i in range( charset ):\n",
    "        submats[charset][i,:] = submats[charset][i,:] / rowsums[i]\n",
    "        submats[charset][:,i] = submats[charset][:,i] / colsums[i]\n",
    "\n",
    "#save the submats in normalized form\n",
    "for charset in charsets:\n",
    "    np.save( 'submats/' + str(charset) + '_norm.npy' , submats[charset] )\n",
    "    with open( 'submats/' + str(charset) + '_norm.txt' , 'w' ) as f:\n",
    "        for i in range( charset ):\n",
    "            f.write( '\\t'.join( [ str(submats[charset][i,j]) for j in range( charset ) ] ) + '\\n' )\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
