{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydssp in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from pydssp) (1.24.3)\n",
      "Requirement already satisfied: torch in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from pydssp) (2.0.1)\n",
      "Requirement already satisfied: einops in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from pydssp) (0.6.1)\n",
      "Requirement already satisfied: tqdm in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from pydssp) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (4.6.3)\n",
      "Requirement already satisfied: sympy in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (1.12)\n",
      "Requirement already satisfied: networkx in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from torch->pydssp) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->pydssp) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->pydssp) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from triton==2.0.0->torch->pydssp) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from triton==2.0.0->torch->pydssp) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from jinja2->torch->pydssp) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from sympy->torch->pydssp) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import numpy as np\n",
    "import os\n",
    "#download an example pdb file\n",
    "url = 'https://files.rcsb.org/download/1EEI.pdb'\n",
    "filename = wget.download(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
      "       'P', 'S', 'T', 'W', 'Y', 'V'],\n",
      "      dtype='object')\n",
      "      A     R     N     D     C     Q     E     G     H     I     L     K  \\\n",
      "0  4.35  4.38  4.75  4.76  4.65  4.37  4.29  3.97  4.63  3.95  4.17  4.36   \n",
      "1  0.61  0.60  0.06  0.46  1.07  0.00  0.47  0.07  0.61  2.22  1.53  1.15   \n",
      "2  1.18  0.20  0.23  0.05  1.89  0.72  0.11  0.49  0.31  1.45  3.23  0.06   \n",
      "3  1.56  0.45  0.27  0.14  1.23  0.51  0.23  0.62  0.29  1.67  2.93  0.15   \n",
      "4  1.00  0.52  0.35  0.44  0.06  0.44  0.73  0.35  0.60  0.73  1.00  0.60   \n",
      "\n",
      "      M     F     P     S     T     W     Y     V  \n",
      "0  4.52  4.66  4.44  4.50  4.35  4.70  4.60  3.95  \n",
      "1  1.18  2.02  1.95  0.05  0.05  2.65  1.88  1.32  \n",
      "2  2.67  1.96  0.76  0.97  0.84  0.77  0.39  1.08  \n",
      "3  2.96  2.03  0.76  0.81  0.91  1.08  0.68  1.14  \n",
      "4  1.00  0.60  0.06  0.35  0.44  0.73  0.44  0.82  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "aaproperties = pd.read_csv('./aaindex1.csv', header=0)\n",
    "\n",
    "aaproperties.drop( [ 'description' , 'reference'  ], axis=1, inplace=True)\n",
    "print(aaproperties.columns)\n",
    "\n",
    "\n",
    "#one hot encoding\n",
    "onehot= np.fill_diagonal(np.zeros((20,20)), 1)\n",
    "\n",
    "#append to the dataframe\n",
    "aaproperties = pd.concat([aaproperties, pd.DataFrame(onehot) ], axis=1)\n",
    "\n",
    "\n",
    "print(aaproperties.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prody\n",
      "  Downloading ProDy-2.4.1.tar.gz (21.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy<1.24,>=1.10 (from prody)\n",
      "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Collecting biopython<=1.79 (from prody)\n",
      "  Downloading biopython-1.79-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from prody) (3.0.9)\n",
      "Requirement already satisfied: scipy in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from prody) (1.10.1)\n",
      "Requirement already satisfied: setuptools in /home/cacutskid/miniconda3/envs/ml2/lib/python3.11/site-packages (from prody) (67.8.0)\n",
      "Building wheels for collected packages: prody\n",
      "  Building wheel for prody (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prody: filename=ProDy-2.4.1-cp311-cp311-linux_x86_64.whl size=22152078 sha256=906cdb5a009437a51e7cb276730dc1ad5650b17648336b32de435df038dfd4bc\n",
      "  Stored in directory: /home/cacutskid/.cache/pip/wheels/6c/b1/85/e303653053b48dfb79b141ab7628dc80f0b7e345bafb881fc9\n",
      "Successfully built prody\n",
      "Installing collected packages: numpy, biopython, prody\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: biopython\n",
      "    Found existing installation: biopython 1.81\n",
      "    Uninstalling biopython-1.81:\n",
      "      Successfully uninstalled biopython-1.81\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bio 1.5.9 requires biopython>=1.80, but you have biopython 1.79 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed biopython-1.79 numpy-1.23.5 prody-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install prody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pydssp\n",
    "from Bio.PDB import PDBParser   \n",
    "import numpy as np\n",
    "filename = './1eei.pdb'\n",
    "    \n",
    "def read_pdb(filename):\n",
    "    #silence all warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    with warnings.catch_warnings():        \n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(filename, filename)\n",
    "        chains = [ c for c in structure.get_chains()]\n",
    "        return chains\n",
    "\n",
    "#return the phi, psi, and omega angles for each residue in a chain\n",
    "def get_angles(chain):\n",
    "\n",
    "    phi_psi_angles = []\n",
    "\n",
    "    chain = [ r for r in chain if PDB.is_aa(r)]\n",
    "    #sliding window of 3 residues\n",
    "    polypeptides = [ chain[i:i+3] for i in range(len(chain)) if len(chain[i:i+4]) >= 3]\n",
    "    #translate to single letter code\n",
    "    print(polypeptides)\n",
    "\n",
    "\n",
    "    for poly_index, poly in enumerate(polypeptides):\n",
    "        phi = None\n",
    "        psi = None\n",
    "\n",
    "        if len(poly) >= 3:\n",
    "            c_minus_1 = poly[len(poly) - 3][\"C\"].get_vector()\n",
    "            n = poly[len(poly) - 2][\"N\"].get_vector()\n",
    "            ca = poly[len(poly) - 2][\"CA\"].get_vector()\n",
    "            c = poly[len(poly) - 2][\"C\"].get_vector()\n",
    "\n",
    "            # Calculate phi angle\n",
    "            phi = PDB.calc_dihedral(c_minus_1, n, ca, c)\n",
    "            n = poly[len(poly) - 2][\"N\"].get_vector()\n",
    "            ca = poly[len(poly) - 2][\"CA\"].get_vector()\n",
    "            c = poly[len(poly) - 2][\"C\"].get_vector()\n",
    "            n_plus_1 = poly[len(poly) - 1][\"N\"].get_vector()\n",
    "\n",
    "            # Calculate psi angle\n",
    "            psi = PDB.calc_dihedral(n, ca, c, n_plus_1)\n",
    "        residue = poly[0]\n",
    "        residue_id = residue.get_full_id()\n",
    "\n",
    "        phi_psi_angles.append({\n",
    "            \"Chain\": residue_id[2],\n",
    "            \"Residue_Number\": residue_id[3][1],\n",
    "            \"Residue_Name\": residue.get_resname(),\n",
    "            #translate 3 letter to 1 letter code\n",
    "            \"single_letter_code\": PDB.Polypeptide.three_to_one(residue.get_resname()),\n",
    "            \"Phi_Angle\": phi,\n",
    "            \"Psi_Angle\": psi\n",
    "        })\n",
    "    #transform phi and psi angles into a dataframe\n",
    "    phi_psi_angles = pd.DataFrame(phi_psi_angles)\n",
    "    #transform the residue names into single letter code\n",
    "    return phi_psi_angles    \n",
    "\n",
    "def get_contact_points(chain, distance):\n",
    "    contact_mat = np.zeros((len(chain), len(chain)))\n",
    "    for i,r1 in enumerate(chain):\n",
    "        for j,r2 in enumerate(chain):\n",
    "            if i< j:\n",
    "                if 'CA' in r1 and 'CA' in r2:\n",
    "                    if r1['CA'] - r2['CA'] < distance:\n",
    "                        contact_mat[i,j] =  r1['CA'] - r2['CA']\n",
    "    contact_mat = contact_mat + contact_mat.T\n",
    "    return contact_mat\n",
    "\n",
    "\n",
    "def ret_hbonds(chain , verbose = False):\n",
    "    #loop through all atoms in a structure\n",
    "    struct = PDBParser().get_structure('1eei', filename)\n",
    "\n",
    "    #get the number of atoms in the chain\n",
    "    #create a numpy array of zeros with the shape of (1, length, atoms, xyz)\n",
    "    output = np.zeros((1, len(chain), len(typeindex), 3 ))\n",
    "    for c, res in enumerate(chain):\n",
    "        atoms = res.get_atoms()\n",
    "        for at,atom in enumerate(atoms):\n",
    "            if atom.get_name() in typeindex:\n",
    "                output[ 0, c ,  typeindex[atom.get_name()] , : ]  = atom.get_coord()\n",
    "    output = torch.tensor(output)\n",
    "    if verbose:\n",
    "        print(output.shape)\n",
    "    mat =  pydssp.get_hbond_map(output[0])\n",
    "    return mat\n",
    "\n",
    "\n",
    "#add the amino acid properties to the angles dataframe\n",
    "#one hot encode the amino acid properties\n",
    "\n",
    "\n",
    "def add_aaproperties(angles, aaproperties):\n",
    "    nodeprops = angles.merge(aaproperties, left_on='single_letter_code', right_index=True, how='left')\n",
    "    nodeprops = nodeprops.dropna()\n",
    "\n",
    "    #generate 1 hot encoding for each amino acid\n",
    "\n",
    "    one_hot = pd.get_dummies(nodeprops['single_letter_code'])\n",
    "    nodeprops = nodeprops.join(one_hot)\n",
    "    nodeprops = nodeprops.drop(columns=['single_letter_code'])\n",
    "\n",
    "    return nodeprops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> 4630 atoms and 1 coordinate set(s) were parsed in 0.37s.\n",
      "@> Hessian was built in 0.56s.\n",
      "@> 20 modes were calculated in 1.57s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1545, 1545)\n"
     ]
    }
   ],
   "source": [
    "from prody import *\n",
    "from pylab import *\n",
    "\n",
    "def anm_analysis(filename):\n",
    "    prot = parsePDB( filename)\n",
    "    calphas2 = prot.select('calpha')\n",
    "    anm = ANM('ANM analysis')\n",
    "    anm.buildHessian(calphas2)\n",
    "    anm.calcModes()\n",
    "\n",
    "    cov = anm.getCovariance()\n",
    "\n",
    "    return eigvals, eigvecs, cov\n",
    "\n",
    "eigvals, eigvecs, cov = anm_analysis(filename)\n",
    "print(cov.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "prody.showProtein(prot)\n",
    "prody.checkUpdates()\n",
    "import py3Dmol\n",
    "prody.showProtein(prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features from a chain\n",
    "\n",
    "def create_features(chain, aaproperties, verbose = False):\n",
    "    angles = get_angles(chain)\n",
    "    angles = add_aaproperties(angles, aaproperties)\n",
    "    angles = angles.dropna()\n",
    "    angles = angles.reset_index(drop=True)\n",
    "    angles = angles.drop(['Residue_Name', 'single_letter_code'], axis=1)\n",
    "    angles = angles.set_index(['Chain', 'Residue_Number'])\n",
    "    angles = angles.sort_index()\n",
    "    angles = angles.dropna()\n",
    "    angles = angles.reset_index()\n",
    "    angles = angles.drop(['Chain', 'Residue_Number'], axis=1)\n",
    "    angles = angles.values\n",
    "    angles = angles.astype('float32')\n",
    "    angles = torch.tensor(angles)\n",
    "    \n",
    "    if verbose:\n",
    "        print(angles.shape)\n",
    "    backbone\n",
    "    contact_points\n",
    "    hbond_mat\n",
    "    \n",
    "    \n",
    "    return angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backbone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m adjacency_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([backbone, contact_points, hbond_mat], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m( adjacency_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'backbone' is not defined"
     ]
    }
   ],
   "source": [
    "adjacency_tensor = np.stack([backbone, contact_points, hbond_mat], axis=0)\n",
    "print( adjacency_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiGraph with 181 nodes and 8062 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "#transform the contact matrices into a networkx multigraph \n",
    "import colour \n",
    "\n",
    "def tensor_to_multigraph(adjacency_tensor):\n",
    "    # Initialize a MultiGraph\n",
    "    G = nx.MultiGraph()\n",
    "    num_nodes = adjacency_tensor.shape[1]\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    colors = [ c.hex_l for c in  colour.Color('red').range_to(colour.Color('green'), adjacency_tensor.shape[0]) ]\n",
    "    # Iterate through the adjacency matrices in the tensor\n",
    "    for i, adj_matrix in enumerate(adjacency_tensor):\n",
    "        # Add nodes to the MultiGraph\n",
    "        # Iterate through the rows and columns of the adjacency matrix to add edges\n",
    "        for row in range(num_nodes):\n",
    "            for col in range(num_nodes):\n",
    "                if adjacency_tensor[i,row, col] != 0:\n",
    "                    # Add an edge with weight (if needed) to the MultiGraph\n",
    "                    G.add_edge(row, col, weight=adjacency_tensor[i,row, col] , color = colors[i],  layer= i )\n",
    "    return G\n",
    "\n",
    "def draw_graph(G , adjacency_tensor ):\n",
    "    # Get the color of each edge\n",
    "    edge_colors = [e[2]['color'] for e in G.edges(data=True)]\n",
    "    # Get the weight of each edge\n",
    "    edge_weights = [e[2]['weight'] for e in G.edges(data=True)]\n",
    "\n",
    "    # Get the positions of each node in the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    # Draw the nodes\n",
    "\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color='black', node_size=100)\n",
    "    # Draw the edges\n",
    "\n",
    "    for i range(0, adjacency_tensor.shape[0]):\n",
    "        # Draw the edges with a curved arc\n",
    "\n",
    "        nx.draw_networkx_edges( G, pos, width=edge_weights, edge_color=edge_colors, edgelist=[e for e in G.edges(data=True) if e[2]['layer'] == layer], connectionstyle='arc3, rad = 0.1')\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "G = tensor_to_multigraph(adjacency_tensor)\n",
    "print(G)\n",
    "draw_graph(G, adjacency_tensor=adjacency_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import scipy.sparse\n",
    "import torch\n",
    "\n",
    "def sparse2pairs(sparsemat):\n",
    "    sparsemat = scipy.sparse.find(sparsemat)\n",
    "    return np.vstack([sparsemat[0],sparsemat[1]])\n",
    "\n",
    "def struct2pyg(pdbchain):\n",
    "    data = HeteroData()\n",
    "\n",
    "    #transform a structure chain into a pytorch geometric graph\n",
    "    #get the adjacency matrices\n",
    "    backbone = scipy.sparse.coo_matrix(  get_backbone(pdbchain) )\n",
    "    contact_points = scipy.sparse.coo_matrix( get_contact_points(pdbchain, 8) )\n",
    "    hbond_mat = scipy.sparse.coo_matrix(  ret_hbonds(pdbchain) )\n",
    "    \n",
    "    #get the adjacency matrices into tensors\n",
    "    data['res','backbone','res'].edge_index = torch.tensor(backbone,  dtype=torch.long )\n",
    "    data['res','contact_points', 'res'].edge_index = torch.tensor(contact_points,  dtype=torch.long )    \n",
    "    data['res','hbond_mat', 'res'].edge_index = torch.tensor(hbond_mat,  dtype=torch.long )\n",
    "\n",
    "    #get the node features\n",
    "    angles = get_angles(pdbchain)\n",
    "    angles = add_aaproperties(angles, aaproperties)\n",
    "    angles = angles.set_index('Residue_Number')\n",
    "    angles = angles.sort_index()\n",
    "    angles = angles.drop(['Residue_Name', 'single_letter_code'], axis=1)\n",
    "    angles = angles.drop(['Chain'], axis=1)\n",
    "    angles = torch.tensor(angles.values, dtype=torch.float)\n",
    "    data['res'].x = angles\n",
    "\n",
    "    #get the edge features\n",
    "    data['res','backbone','res'].edge_attr = torch.tensor(backbone.data, dtype=torch.float)\n",
    "    data['res','contact_points', 'res'].edge_attr = torch.tensor(contact_points.data, dtype=torch.float)\n",
    "    data['res','hbond_mat', 'res'].edge_attr = torch.tensor(hbond_mat.data, dtype=torch.float)\n",
    "\n",
    "    #add self loops\n",
    "    data['res','backbone','res'].edge_index = torch_geometric.utils.add_self_loops(data['res','backbone','res'].edge_index)[0]\n",
    "    data['res','contact_points', 'res'].edge_index = torch_geometric.utils.add_self_loops(data['res','contact_points', 'res'].edge_index)[0]\n",
    "    data['res','hbond_mat', 'res'].edge_index = torch_geometric.utils.add_self_loops(data['res','hbond_mat', 'res'].edge_index)[0]\n",
    "\n",
    "    #normalize features\n",
    "\n",
    "    data['res'].x = torch_geometric.utils.normalize_features(data['res'].x)\n",
    "    data['res','backbone','res'].edge_attr = torch_geometric.utils.normalize_edge_attr(data['res','backbone','res'].edge_attr)\n",
    "    data['res','contact_points', 'res'].edge_attr = torch_geometric.utils.normalize_edge_attr(data['res','contact_points', 'res'].edge_attr)\n",
    "    data['res','hbond_mat', 'res'].edge_attr = torch_geometric.utils.normalize_edge_attr(data['res','hbond_mat', 'res'].edge_attr)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "\n",
    "class StructgraphDataset(Dataset):\n",
    "    def __init__(self, root, raw_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self, datadir ):\n",
    "        return glob.glob(datadir + '/*.pdb')\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return glob.glob(self.processed_dir + '/*.pt')\n",
    "\n",
    "    def download(self,clusterscsv):\n",
    "        def download_pdb(rep):\n",
    "            url = f'https://alphafold.ebi.ac.uk/files/AF-{rep}-F1-model_v4.pdb'\n",
    "            #check if file exists\n",
    "            if os.path.exists(self.raw_dir + rep + '.pdb'):\n",
    "                return self.raw_dir + rep + '.pdb'\n",
    "            filename = wget.download(url, out=self.raw_dir + rep + '.pdb')\n",
    "            return filename\n",
    "        clusterdf = pd.read_csv(clusterscsv)\n",
    "        clusterdf.columns = 'repID,isDark,nMem,repLen,avgLen,repPlddt,avgPlddt,LCAtaxID'.split(',')\n",
    "        reps = clusterdf.repID.unique()\n",
    "        #download the pdb files using mp map async and wget\n",
    "        with mp.Pool(20) as p:\n",
    "            filenames = p.map(download_pdb, reps)\n",
    "\n",
    "    def process(self, batch_size=100):\n",
    "        idx = 0\n",
    "        batch_list = []\n",
    "        for raw_path in tqdm.tqdm(glob.glob(self.raw_dir + '*.pdb)')):\n",
    "            #read the pdb file\n",
    "            pdbchain = read_pdb(raw_path)\n",
    "            #transform the pdb file into a pytorch geometric graph\n",
    "            data = struct2pyg(pdbchain)\n",
    "            #save the graph to disk\n",
    "            batch_list.append(data)\n",
    "            \n",
    "            if len(batch_list) == batch_size:\n",
    "                batch = torch_geometric.data.Batch.from_data_list(batch_list)    \n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "                torch.save(batch, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "                idx += 1\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "dataset = StructgraphDataset(root='./data')\n",
    "#load a single graph\n",
    "data = dataset.get(0)\n",
    "print(data)\n",
    "\n",
    "import os.path as osp\n",
    "from torch_geometric.data import DataLoader\n",
    "from your_dataset_module import StructgraphDataset  # Replace with the actual module name\n",
    "\n",
    "# Define the root directory where the dataset is stored.\n",
    "root = 'path_to_root_directory'\n",
    "\n",
    "# Create an instance of the StructgraphDataset.\n",
    "dataset = StructgraphDataset(root)\n",
    "\n",
    "# Set your batch size.\n",
    "batch_size = 64  # You can adjust this value as needed.\n",
    "\n",
    "# Create a DataLoader with the dataset and batch size.\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"torch-cluster\" \"pytorch-lightning>=1.4, <2.0.0\" \"torch-geometric\" \"torch-spline-conv\" \"lightning>=2.0.0rc0\" \"torch>=1.8.1, <1.14.0\" \"torch-sparse\" \"torch-scatter\" \"torchmetrics>=0.7, <0.12\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "\n",
    "# For downloading pre-trained models\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning as L\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch geometric\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "# PL callbacks\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch import Tensor\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "# Path to the folder where the datasets are/should be downloaded\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/GNNs/\")\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in: Dimension of input features\n",
    "            c_hidden: Dimension of hidden features\n",
    "            c_out: Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers: Number of hidden layers\n",
    "            dp_rate: Dropout rate to apply throughout the network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers - 1):\n",
    "            layers += [nn.Linear(in_channels, out_channels), nn.ReLU(inplace=True), nn.Dropout(dp_rate)]\n",
    "            in_channels = c_hidden\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features per node\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv , SAGEConv , TransformerConv, GATConv\n",
    "from torch import ModuleDict\n",
    "#import lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "#geometric lightning\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# Define autoencoder model\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels , choke , nlayers=2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        #initialize the encoder and decoder as module lists\n",
    "\n",
    "        self.encoder = ModuleDict()\n",
    "        self.decoder = ModuleDict()\n",
    "\n",
    "        #generate an hourglass shape for the encoder and decoder\n",
    "        delta = (hidden_channels - choke) / (nlayers +1)\n",
    "        layersizes = [ int(choke + delta * i) for i in range(0, nlayers+1) ]\n",
    "\n",
    "        #add the first layer of the encoder\n",
    "        self.encoder['input'] = TransformerConv(in_channels, hidden_channels)\n",
    "        for i in range(2, nlayers):\n",
    "            #add the second layer of the encoder\n",
    "            self.encoder['layer_'+str(i)] = TransformerConv(layersizes[i], layersizes[i+1])\n",
    "        for i in range(2, layers):\n",
    "            #add the second layer of the encoder\n",
    "            self.encoder['layer_'+str(i)] = TransformerConv(layersizes[nlayers-(i+1)], layersizes[nlayers-i])\n",
    "        self.decoder['output'] = TransformerConv(hidden_channels, in_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.encoder.values():\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "        z = x\n",
    "        for layer in self.decoder.values():\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "        x_hat = x\n",
    "        \n",
    "        return z, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, in_channels, hidden_channels, choke, nlayers, learning_rate):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.model = Autoencoder(in_channels, hidden_channels, choke, nlayers)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.model(x, edge_index)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, edge_index = batch\n",
    "        z, x_hat = self(x, edge_index)\n",
    "        loss = F.mse_loss(x, x_hat)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, edge_index = batch\n",
    "        z, x_hat = self(x, edge_index)\n",
    "        loss = F.mse_loss(x, x_hat)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, edge_index = batch\n",
    "        z, x_hat = self(x, edge_index)\n",
    "        loss = F.mse_loss(x, x_hat)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model_name, dataloader, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = dataloader\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "        accelerator=\"auto\",\n",
    "        devices=AVAIL_GPUS,\n",
    "        max_epochs=200,\n",
    "        enable_progress_bar=True,\n",
    "    )  # 0 because epoch size is 1\n",
    "\n",
    "    \n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"NodeLevel%s.ckpt\" % model_name)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        L.seed_everything()\n",
    "        model = NodeLevelGNN(\n",
    "            model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs\n",
    "        )\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, dataloaders=node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc, \"val\": val_acc, \"test\": test_result[0][\"test_acc\"]}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
