{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "#download an example pdb file\n",
    "url = 'https://files.rcsb.org/download/1EEI.pdb'\n",
    "#filename = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './1eei (1).pdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "\n",
    "# For downloading pre-trained models\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as L\n",
    "\n",
    "# PyTorch\n",
    "\n",
    "import torch\n",
    "import scipy.sparse\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch geometric\n",
    "\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# PL callbacks\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "aaproperties = pd.read_csv('./aaindex1.csv', header=0)\n",
    "aaproperties.drop( [ 'description' , 'reference'  ], axis=1, inplace=True)\n",
    "\n",
    "print(aaproperties.columns)\n",
    "print( len(aaproperties) )\n",
    "\n",
    "#one hot encoding\n",
    "onehot= np.fill_diagonal(np.zeros((20,20)), 1)\n",
    "onehot = pd.DataFrame(onehot)\n",
    "#change to integers instead of bool\n",
    "onehot = onehot.astype(int)\n",
    "#append to the dataframe\n",
    "aaproperties = aaproperties.T\n",
    "aaproperties = pd.concat([aaproperties, onehot ], axis=1)\n",
    "aaproperties[aaproperties.isna() == True] = 0\n",
    "print(aaproperties)\n",
    "\n",
    "plt.figure( figsize=(10,10) )\n",
    "plt.imshow(aaproperties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pydssp\n",
    "from Bio.PDB import PDBParser   \n",
    "import numpy as np\n",
    "filename = './1eei.pdb'\n",
    "\n",
    "def read_pdb(filename):\n",
    "    #silence all warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    with warnings.catch_warnings():        \n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(filename, filename)\n",
    "        chains = [ c for c in structure.get_chains()]\n",
    "        return chains\n",
    "\n",
    "#return the phi, psi, and omega angles for each residue in a chain\n",
    "def get_angles(chain):\n",
    "\n",
    "    phi_psi_angles = []\n",
    "\n",
    "    chain = [ r for r in chain if PDB.is_aa(r)]\n",
    "    #sliding window of 3 residues\n",
    "    polypeptides = [ chain[i:i+3] for i in range(len(chain)) if len(chain[i:i+4]) >= 3]\n",
    "    #translate to single letter code\n",
    "\n",
    "\n",
    "    residue = chain[0]\n",
    "    residue_id = residue.get_full_id()\n",
    "\n",
    "    phi_psi_angles.append({\n",
    "            \"Chain\": residue_id[2],\n",
    "            \"Residue_Number\": residue_id[3][1],\n",
    "            \"Residue_Name\": residue.get_resname(),\n",
    "            #translate 3 letter to 1 letter code\n",
    "            \"single_letter_code\": PDB.Polypeptide.three_to_one(residue.get_resname()),\n",
    "            \"Phi_Angle\": 0,\n",
    "            \"Psi_Angle\": 0\n",
    "        })\n",
    "\n",
    "\n",
    "    for poly_index, poly in enumerate(polypeptides):\n",
    "        phi = None\n",
    "        psi = None\n",
    "\n",
    "        if len(poly) >= 3:\n",
    "            c_minus_1 = poly[len(poly) - 3][\"C\"].get_vector()\n",
    "            n = poly[len(poly) - 2][\"N\"].get_vector()\n",
    "            ca = poly[len(poly) - 2][\"CA\"].get_vector()\n",
    "            c = poly[len(poly) - 2][\"C\"].get_vector()\n",
    "\n",
    "            # Calculate phi angle\n",
    "            phi = PDB.calc_dihedral(c_minus_1, n, ca, c)\n",
    "            n = poly[len(poly) - 2][\"N\"].get_vector()\n",
    "            ca = poly[len(poly) - 2][\"CA\"].get_vector()\n",
    "            c = poly[len(poly) - 2][\"C\"].get_vector()\n",
    "            n_plus_1 = poly[len(poly) - 1][\"N\"].get_vector()\n",
    "\n",
    "            # Calculate psi angle\n",
    "            psi = PDB.calc_dihedral(n, ca, c, n_plus_1)\n",
    "        residue = poly[0]\n",
    "        residue_id = residue.get_full_id()\n",
    "\n",
    "        phi_psi_angles.append({\n",
    "            \"Chain\": residue_id[2],\n",
    "            \"Residue_Number\": residue_id[3][1],\n",
    "            \"Residue_Name\": residue.get_resname(),\n",
    "            #translate 3 letter to 1 letter code\n",
    "            \"single_letter_code\": PDB.Polypeptide.three_to_one(residue.get_resname()),\n",
    "            \"Phi_Angle\": phi,\n",
    "            \"Psi_Angle\": psi\n",
    "        })\n",
    "\n",
    "    residue = chain[-1]\n",
    "    residue_id = residue.get_full_id()\n",
    "\n",
    "    phi_psi_angles.append({\n",
    "            \"Chain\": residue_id[2],\n",
    "            \"Residue_Number\": residue_id[3][1],\n",
    "            \"Residue_Name\": residue.get_resname(),\n",
    "            #translate 3 letter to 1 letter code\n",
    "            \"single_letter_code\": PDB.Polypeptide.three_to_one(residue.get_resname()),\n",
    "            \"Phi_Angle\": 0,\n",
    "            \"Psi_Angle\": 0\n",
    "        })\n",
    "    \n",
    "    #transform phi and psi angles into a dataframe\n",
    "    phi_psi_angles = pd.DataFrame(phi_psi_angles)\n",
    "    #transform the residue names into single letter code\n",
    "    return phi_psi_angles    \n",
    "\n",
    "def get_contact_points(chain, distance=25):\n",
    "    contact_mat = np.zeros((len(chain), len(chain)))\n",
    "    for i,r1 in enumerate(chain):\n",
    "        for j,r2 in enumerate(chain):\n",
    "            if i< j:\n",
    "                if 'CA' in r1 and 'CA' in r2:\n",
    "                    if r1['CA'] - r2['CA'] < distance:\n",
    "                        contact_mat[i,j] =  r1['CA'] - r2['CA']\n",
    "    contact_mat = contact_mat + contact_mat.T\n",
    "    return contact_mat\n",
    "\n",
    "\n",
    "def get_closest(chain):\n",
    "    contact_mat = np.zeros((len(chain), len(chain)))\n",
    "    for i,r1 in enumerate(chain):\n",
    "        for j,r2 in enumerate(chain):\n",
    "            contact_mat[i,j] =  r1['CA'] - r2['CA']\n",
    "    #go through each row and select min\n",
    "    for r in contact_mat.shape[0]:\n",
    "        contact_mat[r, :][ contact_mat[r, :] != np.amin(contact_mat)] =  0\n",
    "    return contact_mat\n",
    "\n",
    "\n",
    "def get_backbone(chain):\n",
    "    contact_mat = np.zeros((len(chain), len(chain)))\n",
    "    #fill diagonal with 1s\n",
    "    np.fill_diagonal(contact_mat, 1)\n",
    "    return contact_mat\n",
    "\n",
    "def ret_hbonds(chain , verbose = False):\n",
    "    #loop through all atoms in a structure\n",
    "    struct = PDBParser().get_structure('1eei', filename)\n",
    "\n",
    "    #N,CA,C,O\n",
    "    typeindex = {'N':0, 'CA':1 , 'C':2, 'O':3}\n",
    "    #get the number of atoms in the chain\n",
    "    #create a numpy array of zeros with the shape of (1, length, atoms, xyz)\n",
    "    output = np.zeros((1, len(chain), len(typeindex), 3 ))\n",
    "    for c, res in enumerate(chain):\n",
    "        atoms = res.get_atoms()\n",
    "        for at,atom in enumerate(atoms):\n",
    "            if atom.get_name() in typeindex:\n",
    "                output[ 0, c ,  typeindex[atom.get_name()] , : ]  = atom.get_coord()\n",
    "    output = torch.tensor(output)\n",
    "    if verbose:\n",
    "        print(output.shape)\n",
    "    mat =  pydssp.get_hbond_map(output[0])\n",
    "    return mat\n",
    "\n",
    "#add the amino acid properties to the angles dataframe\n",
    "#one hot encode the amino acid properties\n",
    "def add_aaproperties(angles, aaproperties = aaproperties , verbose = False):\n",
    "    if verbose == True:\n",
    "        print(aaproperties , angles )\n",
    "    nodeprops = angles.merge(aaproperties, left_on='single_letter_code', right_index=True, how='left')\n",
    "    nodeprops = nodeprops.dropna()\n",
    "\n",
    "    #generate 1 hot encoding for each amino acid\n",
    "    #one_hot = pd.get_dummies(nodeprops['single_letter_code']).astype(int)\n",
    "    #nodeprops = nodeprops.join(one_hot)\n",
    "    #nodeprops = nodeprops.drop(columns=['single_letter_code'])\n",
    "    return nodeprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prody import *\n",
    "from pylab import *\n",
    "import warnings\n",
    "def anm_analysis(filename):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        prot = parsePDB( filename  , )\n",
    "        calphas2 = prot.select('calpha')\n",
    "        anm = ANM('ANM analysis')\n",
    "        anm.buildHessian(calphas2)\n",
    "        anm.calcModes()\n",
    "        cov = anm.getCovariance()\n",
    "        cov[ cov < 0] = -cov[ cov < 0]\n",
    "        logcov = np.log(cov)\n",
    "        #get the top .5% of the covariance matrix\n",
    "        top = np.percentile(logcov, 99.5)\n",
    "        logcov[ logcov < top] = 0\n",
    "        \n",
    "        \n",
    "        return logcov\n",
    "\n",
    "cov = anm_analysis(filename)\n",
    "print(cov.shape)\n",
    "\n",
    "plt.imshow(cov)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "#print the number of 0 entries\n",
    "print( np.sum(cov != 0) /np.sum(cov == 0))\n",
    "print( np.sum(cov != 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features from a monomer pdb file\n",
    "from scipy import sparse\n",
    "from copy import deepcopy\n",
    "\n",
    "def create_features(monomerpdb, aaproperties, distance = 8, verbose = False):\n",
    "    chain = read_pdb(monomerpdb)[0]\n",
    "    chain = [ r for r in chain if PDB.is_aa(r)]\n",
    "    angles = get_angles(chain)\n",
    "    if len(angles) ==0:\n",
    "        return None\n",
    "    angles = add_aaproperties(angles, aaproperties)\n",
    "    angles = angles.dropna()\n",
    "    angles = angles.reset_index(drop=True)\n",
    "    angles = angles.set_index(['Chain', 'Residue_Number'])\n",
    "    angles = angles.sort_index()\n",
    "    angles = angles.reset_index()\n",
    "    angles = angles.drop(['Chain', 'Residue_Number' , 'Residue_Name'], axis=1)\n",
    "    vals = deepcopy(angles)\n",
    "    vals = vals.dropna()\n",
    "    vals = vals.drop( ['single_letter_code'] , axis = 1 )\n",
    "    vals = vals.values\n",
    "    vals = vals.astype('float32')\n",
    "    if verbose:\n",
    "        print('vals',vals.shape)   \n",
    "        plt.imshow(vals)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    contact_points = get_contact_points(chain, distance)\n",
    "    if verbose:\n",
    "        print('contacts' , contact_points.shape)\n",
    "        plt.imshow(contact_points)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    hbond_mat = np.array(ret_hbonds(chain, verbose))\n",
    "    if verbose:\n",
    "        print('hbond' , hbond_mat.shape)\n",
    "        plt.imshow(hbond_mat)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    #return the angles, amino acid properties, contact points, and hydrogen bonds\n",
    "    #backbone is just the amino acid chain\n",
    "    backbone = get_backbone(chain)\n",
    "    #springmat = anm_analysis(monomerpdb)\n",
    "    \"\"\"if verbose:\n",
    "        print('spring' , springmat.shape)\n",
    "        plt.imshow(springmat)\n",
    "        plt.colorbar()\n",
    "        plt.show()\"\"\"\n",
    "    #change the contac matrices to sparse matrices\n",
    "    contact_points = sparse.csr_matrix(contact_points)\n",
    "    #springmat = sparse.csr_matrix(springmat)\n",
    "    backbone = sparse.csr_matrix(backbone)\n",
    "    hbond_mat = sparse.csr_matrix(hbond_mat)\n",
    "\n",
    "    return angles, contact_points, 0 , hbond_mat, backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features(filename, aaproperties, distance = 15, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#write a function to store sparse matrices in an hdf5 file for many pdb files\n",
    "import h5py\n",
    "def store_features( pdbfiles, aaproperties, filename, verbose = False):\n",
    "    #create a hdf5 file\n",
    "    with h5py.File(filename, 'pdbfiles') as f:\n",
    "        for pdbfile in pdbfiles:\n",
    "            if verbose:\n",
    "                print(pdbfile)\n",
    "            angles, contact_points, springmat, backbone = create_features(pdbfile, aaproperties, verbose)\n",
    "            #store the features in the hdf5 file\n",
    "            f.create_dataset(pdbfile + '_angles', data=angles)\n",
    "            f.create_dataset(pdbfile + '_contact_points', data=contact_points)\n",
    "            f.create_dataset(pdbfile + '_springmat', data=springmat)\n",
    "            f.create_dataset(pdbfile + '_backbone', data=backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = 'repId_isDark_nMem_repLen_avgLen_repPlddt_avgPlddt_LCAtaxId'.split('_')\n",
    "repdf = pd.read_table( './afdbclusters/2-repId_isDark_nMem_repLen_avgLen_repPlddt_avgPlddt_LCAtaxId.tsv')\n",
    "repdf.columns = cols\n",
    "print(repdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import wget \n",
    "\n",
    "def download_pdb(rep ,structdir = './structs/'):\n",
    "    url = f'https://alphafold.ebi.ac.uk/files/AF-{rep}-F1-model_v4.pdb'\n",
    "    #check if file exists\n",
    "    if os.path.exists( structdir + rep + '.pdb'):\n",
    "        return self.raw_dir + rep + '.pdb'\n",
    "    filename = wget.download(url, out=structdir + rep + '.pdb')\n",
    "    return filename\n",
    "\n",
    "def download(repdf , nreps = 100 , structdir = './structs/'):\n",
    "    if not os.path.exists(structdir):\n",
    "        os.makedirs(structdir)\n",
    "            \n",
    "    reps = repdf.repId.unique()\n",
    "    if nreps:\n",
    "        #select a random sample of representatives\n",
    "        reps = np.random.choice(reps, nreps)\n",
    "    with mp.Pool(20) as p:\n",
    "        filenames = p.map(download_pdb, tqdm.tqdm(reps))\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#download(repdf, nreps = 100 , structdir = './structs/' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "# Path to the folder where the datasets are/should be downloaded\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/GNNs/\")\n",
    "\n",
    "#create the h5 dataset from the pdb files\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sparse2pairs(sparsemat):\n",
    "    sparsemat = scipy.sparse.find(sparsemat)\n",
    "    return np.vstack([sparsemat[0],sparsemat[1]])\n",
    "\n",
    "\n",
    "metadata = { 'edge_types': [ ('res','backbone','res') , ('res','contactPoints', 'res') , ('res','hbond', 'res') ] }\n",
    "\n",
    "\n",
    "def struct2pyg(pdbchain , aaproperties= aaproperties):\n",
    "    data = HeteroData()\n",
    "    #transform a structure chain into a pytorch geometric graph\n",
    "    #get the adjacency matrices\n",
    "    \n",
    "    xdata = create_features(pdbchain, aaproperties)\n",
    "    if data is not None:\n",
    "        angles, contact_points, springmat , hbond_mat , backbone = xdata\n",
    "    else:\n",
    "        return None\n",
    "    if len(angles) ==0 :\n",
    "        return None    \n",
    "    angles = add_aaproperties(angles, aaproperties)\n",
    "    angles = angles.drop(['single_letter_code'], axis=1)    \n",
    "    angles.fillna(0, inplace=True)\n",
    "    angles = torch.tensor(angles.values, dtype=torch.float32)\n",
    "    data['res'].x = angles\n",
    "\n",
    "    #get the edge features\n",
    "    data['res','backbone','res'].edge_attr = torch.tensor(backbone.data, dtype=torch.float32)\n",
    "    data['res','contactPoints', 'res'].edge_attr = torch.tensor(contact_points.data, dtype=torch.float32)\n",
    "    data['res','hbond', 'res'].edge_attr = torch.tensor(hbond_mat.data, dtype=torch.float)\n",
    "    #data['res','springMat', 'res'].edge_attr = torch.tensor(springmat.data, dtype=torch.float32)\n",
    "\n",
    "    backbone = sparse2pairs(backbone)\n",
    "    contact_points = sparse2pairs(contact_points)\n",
    "    hbond_mat = sparse2pairs(hbond_mat)\n",
    "    springmat = sparse2pairs(springmat)\n",
    "    \n",
    "    #get the adjacency matrices into tensors\n",
    "    data['res','backbone','res'].edge_index = torch.tensor(backbone,  dtype=torch.long )\n",
    "    data['res','contactPoints', 'res'].edge_index = torch.tensor(contact_points,  dtype=torch.long )    \n",
    "    data['res','hbond', 'res'].edge_index = torch.tensor(hbond_mat,  dtype=torch.long )\n",
    "    #data['res','springMat', 'res'].edge_index = torch.tensor(springmat,  dtype=torch.long )\n",
    "    \n",
    "\n",
    "    #add self loops\n",
    "    data['res','backbone','res'].edge_index = torch_geometric.utils.add_self_loops(data['res','backbone','res'].edge_index)[0]\n",
    "    #data['res','contactPoints', 'res'].edge_index = torch_geometric.utils.add_self_loops(data['res','contactPoints', 'res'].edge_index)[0]\n",
    "    #data['res','hbond_mat', 'res'].edge_index = torch_geometric.utils.add_self_loops(data['res','hbond_mat', 'res'].edge_index)[0]\n",
    "\n",
    "    #normalize features\n",
    "\n",
    "    #data['res'].x = torch_geometric.transforms.NormalizeFeatures(data['res'].x)\n",
    "    #data['res','contact_points', 'res'].edge_attr = torch_geometric.transforms.normalize_edge_attr(data['res','contact_points', 'res'].edge_attr)\n",
    "    #data['res','spring_mat', 'res'].edge_index = torch_geometric.transforms.normalize_edge_attr(data['res','spring_mat', 'res'].edge_attr)\n",
    "    #data['res','hbond_mat', 'res'].edge_attr = torch_geometric.transforms.normalize_edge_attr(data['res','hbond_mat', 'res'].edge_attr)\n",
    "\n",
    "    return data\n",
    "\n",
    "#create a function to store the pytorch geometric data in a hdf5 file\n",
    "def store_pyg(pdbfiles, aaproperties, filename, verbose = True):\n",
    "    with h5py.File(filename , mode = 'w') as f:\n",
    "        for pdbfile in tqdm.tqdm(pdbfiles):\n",
    "            if verbose:\n",
    "                print(pdbfile)\n",
    "            hetero_data = struct2pyg(pdbfile, aaproperties)\n",
    "            if hetero_data:\n",
    "                f.create_group(pdbfile)\n",
    "\n",
    "                for node_type in hetero_data.node_types:\n",
    "                    if hetero_data[node_type].x is not None:\n",
    "                        node_group = f.create_group(f'{pdbfile}/node/{node_type}')\n",
    "                        node_group.create_dataset('x', data=hetero_data[node_type].x.numpy())\n",
    "                        \n",
    "                # Iterate over edge types and their connections\n",
    "                for edge_type in hetero_data.edge_types:\n",
    "                    # edge_type is a tuple: (src_node_type, relation_type, dst_node_type)\n",
    "                    edge_group = f.create_group(f'{pdbfile}/edge/{edge_type[0]}_{edge_type[1]}_{edge_type[2]}')\n",
    "                    if hetero_data[edge_type].edge_index is not None:\n",
    "                        edge_group.create_dataset('edge_index', data=hetero_data[edge_type].edge_index.numpy())\n",
    "                    \n",
    "                    # If there are edge features, save them too\n",
    "                    if hasattr(hetero_data[edge_type], 'edge_attr') and hetero_data[edge_type].edge_attr is not None:\n",
    "                        edge_group.create_dataset('edge_attr', data=hetero_data[edge_type].edge_attr.numpy())\n",
    "\n",
    "\n",
    "                    #todo. store some other data. sequence. uniprot info etc.\n",
    "            else:\n",
    "                print('err' , pdbfile )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbfiles = glob.glob('./structs/*.pdb')\n",
    "#store_pyg(pdbfiles, aaproperties, filename='structs.h5', verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureDataset(Dataset):\n",
    "    def __init__(self, h5dataset):\n",
    "        super().__init__()\n",
    "        #keys should be the structures\n",
    "        self.structures = h5dataset['structs']\n",
    "        self.structlist = list(self.structures.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.structures)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == str:\n",
    "            f = self.structures[idx]\n",
    "        elif type(idx) == int:\n",
    "            f = self.structures[self.structlist[idx]]\n",
    "        else:\n",
    "            raise 'use a structure filename or integer'\n",
    "        data = {}\n",
    "        hetero_data = HeteroData()\n",
    "        if 'node' in f.keys():\n",
    "            for node_type in f['node'].keys():\n",
    "                node_group = f['node'][node_type]\n",
    "                # Assuming 'x' exists\n",
    "                if 'x' in node_group.keys():\n",
    "                    hetero_data[node_type].x = torch.tensor(node_group['x'][:])\n",
    "        # Edge data\n",
    "        if 'edge' in f.keys():\n",
    "            for edge_name in f['edge'].keys():\n",
    "                edge_group = f['edge'][edge_name]\n",
    "                src_type, link_type, dst_type = edge_name.split('_')\n",
    "                edge_type = (src_type, link_type, dst_type)\n",
    "                # Assuming 'edge_index' exists\n",
    "                if 'edge_index' in edge_group.keys():\n",
    "                    hetero_data[edge_type].edge_index = torch.tensor(edge_group['edge_index'][:])\n",
    "                \n",
    "                # If there are edge attributes, load them too\n",
    "                if 'edge_attr' in edge_group.keys():\n",
    "                    hetero_data[edge_type].edge_attr = torch.tensor(edge_group['edge_attr'][:])\n",
    "        #return pytorch geometric heterograph\n",
    "        return hetero_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import h5py\n",
    "f = h5py.File('./structs.h5' , 'r')\n",
    "struct_dat = StructureDataset(f)\n",
    "print( len(struct_dat) )\n",
    "start = time.time()\n",
    "print( struct_dat[10] )\n",
    "print( time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "ndim = struct_dat[20]['res'].x.shape[1]\n",
    "print( time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import HeteroData\n",
    "import networkx as nx\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "\n",
    "\n",
    "def remove_self_loops_edge_index(edge_index):\n",
    "    \"\"\"\n",
    "    Removes self-loops from the edge_index of a graph.\n",
    "    \"\"\"\n",
    "    mask = edge_index[0] != edge_index[1]\n",
    "    return edge_index[:, mask]\n",
    "\n",
    "def convert_hetero_to_networkx(hetero_data):\n",
    "    G = nx.MultiDiGraph()  # Use MultiDiGraph to support multiple edge types\n",
    "    # Add nodes with type as an attribute\n",
    "    for node_type in hetero_data.node_types:\n",
    "        for node_id in range(hetero_data[node_type].num_nodes):\n",
    "            # Node identifier format: (node_type, node_id)\n",
    "            G.add_node((node_type, node_id), node_type=node_type)\n",
    "    # Add edges\n",
    "    for edge_type in hetero_data.edge_types:\n",
    "        src_type, _, dst_type = edge_type\n",
    "        edge_indices = hetero_data[edge_type].edge_index\n",
    "        edge_indices = remove_self_loops_edge_index(edge_indices)\n",
    "        for i in range(edge_indices.shape[1]):  # Iterate through each edge\n",
    "            src_id, dst_id = edge_indices[:, i].tolist()\n",
    "            # Edge identifier format: ((src_type, src_id), (dst_type, dst_id))\n",
    "            G.add_edge((src_type, src_id), (dst_type, dst_id), edge_type=edge_type)\n",
    "\n",
    "    return G\n",
    "\n",
    "def plot_hetero_graph_with_curved_edges(data):\n",
    "    print(data)\n",
    "    G = convert_hetero_to_networkx(data)\n",
    "    print(G)\n",
    "    pos = nx.spring_layout(G)  # General layout if no positions are provided\n",
    "\n",
    "    # Calculate offset for curved edges to avoid overlap\n",
    "    edge_count = {}\n",
    "    for src, dst, key in G.edges(keys=True):\n",
    "        edge_count[(src, dst)] = edge_count.get((src, dst), 0) + 1\n",
    "    # Draw nodes\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=70)\n",
    "    \n",
    "    edge_type_colors = {}\n",
    "    unique_edge_types = data.edge_types\n",
    "    # Generate a color map from matplotlib, or use a predefined set of colors\n",
    "    colors = plt.get_cmap('tab20')(range(len(unique_edge_types)))\n",
    "    for i, edge_type in enumerate(unique_edge_types):\n",
    "        edge_type_colors[edge_type] = colors[i]\n",
    "\n",
    "    # Draw edges with curvature\n",
    "    for (src, dst), count in edge_count.items():\n",
    "        for i in range(count):\n",
    "            edge_key = list(G[src][dst])[i]\n",
    "            style = G[src][dst][edge_key]\n",
    "            curvature = 0.1 * (i - count // 2)\n",
    "            nx.draw_networkx_edges(\n",
    "                G, pos, edgelist=[(src, dst)],\n",
    "                connectionstyle=f'arc3, rad={curvature}',\n",
    "                arrowstyle='-|>',\n",
    "                edge_color = edge_type_colors.get(edge_type, 'black'),  # Default color is black\n",
    "                width=style.get('weight', 1),\n",
    "                alpha = .25\n",
    "            )\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
    "    plt.title('Heterogeneous Graph with Curved Edges')\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGAE_Encoder(torch.nn.Module):\n",
    "    #define a vanilla autoencoder\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, metadata={}):\n",
    "        super(HeteroGAE_Encoder, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        for i in range(len(hidden_channels)):\n",
    "            self.convs.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(in_channels if i == 0 else hidden_channels[i-1], hidden_channels[i])\n",
    "                    for edge_type in metadata['edge_types']\n",
    "                })\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels[-1], out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index_dict):\n",
    "        for i, convs in enumerate(self.convs):\n",
    "            # Apply the graph convolutions and average over all edge types\n",
    "            x = [ conv(x, edge_index_dict[tuple( edge_type.split('_') ) ]) for edge_type, conv in convs.items() ] \n",
    "            #turn into a tensor\n",
    "            x = torch.stack(x, dim=0).mean(dim=0)\n",
    "            x = F.relu(x) if i < len(self.hidden_channels) - 1 else x\n",
    "        return self.lin(x)\n",
    "    \n",
    "class HeteroGAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_out_channels , xdim , hidden_channels = [20,20,20] , out_channels= 10 , Xdecoder_hidden = 100, metadata={}):\n",
    "        super(HeteroGAE_Decoder, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = encoder_out_channels\n",
    "        for i in range(len(self.hidden_channels)):\n",
    "            self.convs.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(self.in_channels if i == 0 else self.hidden_channels[i-1], self.hidden_channels[i])\n",
    "                    for edge_type in [ ( 'res','backbone','res') ]\n",
    "                })\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels[-1], out_channels)\n",
    "        #sigmoid to predict the edge probabilities after graph conv\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # add stack of dense layers to reconstruct the node features\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels , Xdecoder_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Xdecoder_hidden, Xdecoder_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Xdecoder_hidden , xdim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z , edge_index , backbone=None, **kwargs):\n",
    "        # Transform the latent space if necessary\n",
    "        edge_probs = {}\n",
    "        for layer in self.convs:\n",
    "            for edge_type, conv in layer.items():\n",
    "                z = conv(z, backbone)\n",
    "                z = F.relu(z)\n",
    "        z = self.lin(z)\n",
    "\n",
    "        sim_matrix =  (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "\n",
    "        edge_probs = self.sigmoid(sim_matrix)\n",
    "        \n",
    "        #turn into connectivity with two columns for edge index\n",
    "        #edge_probs = torch.stack( torch.where( edge_probs > 0.5 ) , dim=0)\n",
    "        #reconstruct the node features with decoder\n",
    "        x_r = self.decoder(z)\n",
    "        return x_r , edge_probs\n",
    "\n",
    "EPS = 1e-10\n",
    "def recon_loss( z: Tensor, pos_edge_index: Tensor , backbone:Tensor = None , decoder = None ) -> Tensor:\n",
    "    r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
    "    entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
    "    sampled edges.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "        pos_edge_index (torch.Tensor): The positive edges to train against.\n",
    "        neg_edge_index (torch.Tensor, optional): The negative edges to\n",
    "            train against. If not given, uses negative sampling to\n",
    "            calculate negative edges. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    \n",
    "    pos =decoder(z, pos_edge_index, backbone )[1]\n",
    "    #turn pos edge index into a binary matrix\n",
    "    pos_loss = -torch.log( pos + EPS).mean()\n",
    "    neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "    neg = decoder(z ,  neg_edge_index, backbone )[1]\n",
    "    neg_loss = -torch.log( ( 1 - neg) + EPS ).mean()\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "#define loss for x reconstruction   \n",
    "def x_reconstruction_loss(x, recon_x):\n",
    "    \"\"\"\n",
    "    compute the loss over the node feature reconstruction.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(recon_x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): HeteroVGAE_Encoder(\n",
       "    (convs): ModuleList(\n",
       "      (0): ModuleDict(\n",
       "        (res_backbone_res): TransformerConv(1134, 50, heads=1)\n",
       "        (res_contactPoints_res): TransformerConv(1134, 50, heads=1)\n",
       "        (res_hbond_res): TransformerConv(1134, 50, heads=1)\n",
       "      )\n",
       "      (1): ModuleDict(\n",
       "        (res_backbone_res): TransformerConv(50, 50, heads=1)\n",
       "        (res_contactPoints_res): TransformerConv(50, 50, heads=1)\n",
       "        (res_hbond_res): TransformerConv(50, 50, heads=1)\n",
       "      )\n",
       "    )\n",
       "    (mus): Linear(in_features=50, out_features=25, bias=True)\n",
       "    (log_vars): Linear(in_features=50, out_features=25, bias=True)\n",
       "  )\n",
       "  (decoder): HeteroGraphDecoder(\n",
       "    (transform): ModuleDict(\n",
       "      (res_backbone_res): Linear(in_features=25, out_features=100, bias=True)\n",
       "      (res_contactPoints_res): Linear(in_features=25, out_features=100, bias=True)\n",
       "      (res_hbond_res): Linear(in_features=25, out_features=100, bias=True)\n",
       "    )\n",
       "    (sigmoid_dict): ModuleDict(\n",
       "      (res_backbone_res): Sigmoid()\n",
       "      (res_contactPoints_res): Sigmoid()\n",
       "      (res_hbond_res): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import DataLoader\n",
    "#create a training loop for the GAE model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device( 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "encoder = HeteroGAE_Encoder(in_channels=ndim, hidden_channels=[20,20 , 20 ], out_channels=5, metadata=metadata)\n",
    "decoder = HeteroGAE_Decoder(encoder_out_channels = encoder.out_channels, xdim=ndim, hidden_channels=[30,30, 20 , 20 , 10, 10, 5] , out_channels=10, metadata=metadata)\n",
    "# Create a DataLoader for training\n",
    "train_loader = DataLoader(struct_dat, batch_size=10, shuffle=True)\n",
    "total_loss_x = 0\n",
    "total_loss_edge = 0\n",
    "# Training loop\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:15<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 135016.2334, Edge Loss: 139.5992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:15<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 110932.5048, Edge Loss: 114.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████████████████▍                                                                                                                        | 32/80 [00:06<00:09,  4.83it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "for epoch in range(500):\n",
    "    for data in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        z = encoder.forward(data['res'].x, data.edge_index_dict)\n",
    "        edgeloss = recon_loss(z , data.edge_index_dict[( 'res','contactPoints','res')] , data.edge_index_dict[( 'res','backbone','res')] , decoder)\n",
    "        recon_x, edge_probs = decoder(z, data.edge_index_dict[( 'res','contactPoints','res')] , data.edge_index_dict[( 'res','backbone','res')] )\n",
    "        xloss = x_reconstruction_loss(data['res'].x, recon_x)\n",
    "        loss = xloss + 500*edgeloss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_edge += edgeloss.item()\n",
    "        total_loss_x += xloss.item()    \n",
    "    print(f'Epoch {epoch}, Loss: {total_loss_x:.4f}, Edge Loss: {total_loss_edge:.4f}')\n",
    "    total_loss_x = 0\n",
    "    total_loss_edge = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels=ndim\n",
    "hidden_channels=50\n",
    "out_channels = 25\n",
    "nlayers=2\n",
    "\n",
    "encoder = HeteroVGAE_Encoder(in_channels=in_channels,  out_channels=out_channels, metadata=metadata)\n",
    "decoder = HeteroGraphDecoder(hidden_channels=100 , encoder_out_channels = out_channels , metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import models\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import (negative_sampling, remove_self_loops, add_self_loops)\n",
    "\n",
    "class GraphAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, model_name, in_channels, out_channels, hidden_channels, nlayers, learning_rate, encoder_cls, decoder_cls, metadata):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = HeteroVGAE_Encoder(in_channels=in_channels,hidden_channels=hidden_channels,  out_channels=out_channels, metadata=metadata, nlayers=nlayers)\n",
    "        self.decoder = HeteroGraphDecoder(hidden_channels, out_channels, metadata['edge_types']) if decoder_cls else None\n",
    "        self.model = models.VGAE(self.encoder) if self.decoder is None else models.VGAE(self.encoder , self.decoder) \n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = model_name\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        return self.model.encode(x_dict, edge_index_dict), self.model.decode(self.model.__mu__, self.model.__logvar__)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_dict, edge_index_dict = batch.x_dict, batch.edge_index_dict\n",
    "        z, edge_recon = self.forward(x_dict, edge_index_dict)\n",
    "        # Reconstruction loss\n",
    "        loss = self.reconstruction_loss(edge_index_dict, edge_recon)\n",
    "        # KL divergence loss\n",
    "        kl_loss = 0.5 / z.size(0) * torch.mean(torch.sum(1 + 2 * self.model.__logvar__ - self.model.__mu__**2 - self.model.__logvar__.exp()**2, dim=1))\n",
    "        \n",
    "        total_loss = loss - kl_loss\n",
    "        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return total_loss\n",
    "\n",
    "    def reconstruction_loss(self, edge_index_dict, edge_recon):\n",
    "        # Implement the specific reconstruction loss for your graph data\n",
    "        loss = 0\n",
    "        for edge_type, edge_index in edge_index_dict.items():\n",
    "            src, dst = edge_type.split('_')[:2]\n",
    "            pos_loss = -torch.log(edge_recon[edge_type] + 1e-15).mean()\n",
    "            neg_edge_index = negative_sampling(edge_index, edge_recon[edge_type].size(0))\n",
    "            neg_loss = -torch.log(1 - edge_recon[edge_type][neg_edge_index[0], neg_edge_index[1]] + 1e-15).mean()\n",
    "            loss += pos_loss + neg_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Similar to training_step, calculate the loss for validation data\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Similar to training_step, calculate the loss for test data\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model_name, dataloader, in_channels=ndim , hidden_channels=50, out_channels = 10, nlayers=2, learning_rate=0.001, encoder=encoder, decoder=decoder , metadata=metadata ,**model_kwargs):\n",
    "    node_data_loader = dataloader\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "        accelerator=\"auto\",\n",
    "        devices=AVAIL_GPUS,\n",
    "        max_epochs=200,\n",
    "        enable_progress_bar=True,\n",
    "    )  # 0 because epoch size is 1\n",
    "    \n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"NodeLevel%s.ckpt\" % model_name)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphAutoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = GraphAutoencoder(model_name=model_name, in_channels=in_channels, hidden_channels=hidden_channels, out_channels = out_channels , nlayers=nlayers, learning_rate=learning_rate , encoder_cls=encoder, decoder_cls=decoder , metadata=metadata )\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        model = GraphAutoencoder.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, dataloaders=node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc, \"val\": val_acc, \"test\": test_result[0][\"test_acc\"]}\n",
    "    return model, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat a dataloader\n",
    "from torch_geometric.data import DataLoader\n",
    "def create_dataloader(dataset, batch_size=5, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "dataloader = create_dataloader(struct_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model on the dataset\n",
    "\"\"\"\n",
    "model, result = train_autoencoder( 'autoencoder', dataloader, in_channels=ndim , hidden_channels=100, out_channels = 10 ,  nlayers=2, learning_rate=0.001, encoder=encoder, decoder=decoder , metadata = metadata )\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "\n",
    "#load the model\n",
    "model.load_state_dict(torch.load('autoencoder.pth'))\n",
    "\n",
    "#evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        data = data.to(model.device)\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        #get the reconstruction\n",
    "        recon = model.decode(z, data.edge_index)\n",
    "        #get the loss\n",
    "        loss = F.mse_loss(recon, data.x)\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss( edge_index_dict, edge_recon):\n",
    "        # Implement the specific reconstruction loss for your graph data\n",
    "        loss = 0\n",
    "        for edge_type, edge_index in edge_index_dict.items():\n",
    "            src, dst = edge_type[:2]\n",
    "            pos_loss = -torch.log(edge_recon[edge_type] + 1e-15).mean()\n",
    "            neg_edge_index = negative_sampling(edge_index, edge_recon[edge_type].size(0))\n",
    "            neg_loss = -torch.log(1 - edge_recon[edge_type][neg_edge_index[0], neg_edge_index[1]] + 1e-15).mean()\n",
    "            loss += pos_loss + neg_loss\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop without pytorch lightning\n",
    "#for global graph\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "#init device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device( 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Create a VGAE model\n",
    "model = VGAE(encoder, decoder)\n",
    "\n",
    "model = model.to(device)\n",
    "# Create a DataLoader for training\n",
    "train_loader = DataLoader(struct_dat, batch_size=10, shuffle=True)\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    total_loss = 0\n",
    "    for data in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{200}\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #forward pass\n",
    "        z = model.encode(data['res'].x, data.edge_index_dict)\n",
    "        recon = model.decode(z)\n",
    "        loss =reconstruction_loss( data.edge_index_dict , recon)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        optimizer.step()\n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{200}, Loss: {total_loss/len(train_loader)}\")\n",
    "    total_loss = 0\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), 'autoencoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop without pytorch lightning\n",
    "#for local graph\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "#init device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device( 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Create a VGAE model\n",
    "model = VGAE(encoder, decoder)\n",
    "\n",
    "model = model.to(device)\n",
    "# Create a DataLoader for training\n",
    "train_loader = DataLoader(struct_dat, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "for epoch in range(200):\n",
    "    total_loss = 0\n",
    "    for data in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{200}\", leave=False):\n",
    "\n",
    "\n",
    "        local_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 30 neighbors for each node for 2 iterations\n",
    "        num_neighbors=[5] * 2,\n",
    "        # Use a batch size of 128 for sampling training nodes\n",
    "        batch_size=12,\n",
    "        input_nodes= list(data.edge_index_dict[('res' , 'contactPoints','res')][0]),\n",
    "        )   \n",
    "\n",
    "        for local_data in local_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            #forward pass\n",
    "            z = model.encode(data['res'].x, data.edge_index_dict)\n",
    "            recon = model.decode(z)\n",
    "            loss =reconstruction_loss( data.edge_index_dict , recon)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "            optimizer.step()\n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{200}, Loss: {total_loss/len(train_loader)}\")\n",
    "    total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "for data in test_loader:\n",
    "    data = data.to(model.device)\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    recon = model.decode(z, data.edge_index)\n",
    "    loss = F.mse_loss(recon, data.x)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.script(model)\n",
    "#save the model\n",
    "model.save('autoencoder.pt')\n",
    "\n",
    "#save only the encoder\n",
    "encoder = model.encoder\n",
    "encoder = torch.jit.script(encoder)\n",
    "encoder.save('encoder.pt')\n",
    "\n",
    "#save only the decoder\n",
    "decoder = model.decoder\n",
    "decoder = torch.jit.script(decoder)\n",
    "decoder.save('decoder.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#load the encoder\n",
    "\n",
    "encoder = torch.jit.load('encoder.pt')\n",
    "encoder.eval()\n",
    "\n",
    "#pass the training data through the encoder\n",
    "embeddings = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        data = data.to(model.device)\n",
    "        #get the embeddings in numpy\n",
    "        z = encoder(data.x, data.edge_index).detach().numpy()\n",
    "        embeddings.append(z)\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a kmeans clustering algorithm to cluster the embeddings\n",
    "#this will define our alphabet\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "def cluster_embeddings(embeddings, nclusters = 256):\n",
    "    kmeans = KMeans(n_clusters=nclusters, random_state=0).fit(embeddings)\n",
    "    return kmeans\n",
    "\n",
    "#use the kmeans model to predict the clusters for a set of embeddings\n",
    "def predict_clusters(kmeans, embeddings):\n",
    "    return kmeans.predict(embeddings)\n",
    "\n",
    "def clusternum2ascii(cluster):\n",
    "    return chr(cluster + 65)\n",
    "\n",
    "def struct2ascii(embeddings, kmeans):\n",
    "    clusters = list(predict_clusters(kmeans, embeddings))\n",
    "    asciistr = ''.join([ clusternum2ascii(cluster) for cluster in clusters ])\n",
    "    return asciistr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the kmans model\n",
    "kmeans = cluster_embeddings(embeddings, nclusters = 256)\n",
    "#save the kmeans model\n",
    "import pickle\n",
    "with open('kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def structs2ascii(outfile , structures, kmeans, model, aaproperties , threads = 1 , output_contacts = False):\n",
    "    #turn structures into a fasta with ascii strings\n",
    "    embeddings = encode_structures(model, structures, aaproperties, threads = threads,  verbose = False)\n",
    "    if output_contacts == True:\n",
    "        contactpts = structs2contactpts(structures, radius = 8 , threads = threads )\n",
    "    with open(outfile, 'w') as f:\n",
    "        for i,structure in enumerate(structures):\n",
    "            contacts = contactpts[i]\n",
    "            asciistr = struct2ascii(embeddings[i], kmeans)\n",
    "            f.write('>' + structure + '\\n')\n",
    "            #comma separated contact points\n",
    "            if output_contacts == True:\n",
    "                f.write('#x:' + ','.join( contacts[0] ) + '\\n' )\n",
    "                f.write('#y:' + ','.join( contacts[1] ) + '\\n' )\n",
    "            #ascii string of structural embeddings            \n",
    "            f.write(asciistr + '\\n')\n",
    "    return outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download some examples from each cluster\n",
    "\n",
    "#use explicit aln to comput submats\n",
    "\n",
    "#use explicit aln to compute the compatibility matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a decoder from a graph with prot alphabet to amino acid letters\n",
    "#the contact mat is available for stuff from the aln \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text aligner with mafft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output links into potts model with graph \n",
    "\n",
    "#calculate ancestral sequences\n",
    "\n",
    "#calculate the state transition energies\n",
    "\n",
    "#calculate the state transition probabilities\n",
    "\n",
    "import phylotreelib as pt\n",
    "tree1 = pt.Tree.randtree(ntips=50, randomlen=True)\n",
    "tree2 = tree1.copy_treeobject()\n",
    "for i in range(5):\n",
    "    tree2.spr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
