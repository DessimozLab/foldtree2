{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#get known complexes\n",
    "\n",
    "#embed structures\n",
    "\n",
    "\n",
    "#use decoder and generate z vecs\n",
    "\n",
    "#use decoder sigmoid to get contact proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-29 15:56:12--  https://shmoo.weizmann.ac.il/elevy/3dcomplexV6/dataV6/BU_all_renum.tar.gz\n",
      "Resolving shmoo.weizmann.ac.il (shmoo.weizmann.ac.il)... 132.77.150.157\n",
      "Connecting to shmoo.weizmann.ac.il (shmoo.weizmann.ac.il)|132.77.150.157|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18052384562 (17G) [application/x-gzip]\n",
      "Saving to: ‘BU_all_renum.tar.gz’\n",
      "\n",
      "BU_all_renum.tar.gz  98%[==================> ]  16.61G  2.39MB/s    eta 88s    "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tarfile\n",
    "#iterate over all files in the tar.gz\n",
    "def extract_files(tar_gz_file,n):\n",
    "    with tarfile.open(tar_gz_file, \"r:gz\") as tar:\n",
    "        members = tar.getmembers():\n",
    "        member = members[n]\n",
    "        f = tar.extractfile(member)\n",
    "        if f is not None:\n",
    "            content = f.read()\n",
    "            return content\n",
    "\n",
    "#files are pdbs. transform content into a pdb file\n",
    "def write_pdb(content):\n",
    "    with open(\"temp.pdb\", \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "#extract the first file$\n",
    "content = extract_files(\"BU_all_renum.tar.gz\",0)\n",
    "write_pdb(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / self.num_embeddings, 1 / self.num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        flat_x = x.view(-1, self.embedding_dim)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_x**2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embeddings.weight**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_x, self.embeddings.weight.t()))\n",
    "\n",
    "        # Get the encoding that has the min distance\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=x.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize the latents\n",
    "        quantized = torch.matmul(encodings, self.embeddings.weight).view_as(x)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        return quantized, loss\n",
    "\n",
    "    def discretize_z(self, x):\n",
    "        # Flatten input\n",
    "        flat_x = x.view(-1, self.embedding_dim)\n",
    "        # Compute distances between input and codebook embeddings\n",
    "        distances = (torch.sum(flat_x**2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embeddings.weight**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_x, self.embeddings.weight.t()))\n",
    "        # Get the encoding that has the minimum distance\n",
    "        closest_indices = torch.argmin(distances, dim=1)\n",
    "        \n",
    "        # Convert indices to characters\n",
    "        char_list = [chr(idx.item()) for idx in closest_indices]\n",
    "        return closest_indices, char_list\n",
    "\n",
    "    def string_to_embedding(self, s):\n",
    "        \n",
    "        # Convert characters back to indices\n",
    "        indices = torch.tensor([ord(c) for c in s], dtype=torch.long, device=self.embeddings.weight.device)\n",
    "        \n",
    "        # Retrieve embeddings from the codebook\n",
    "        embeddings = self.embeddings(indices)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class HeteroGAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_embeddings, commitment_cost, metadata={}):\n",
    "        super(HeteroGAE_Encoder, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        for i in range(len(hidden_channels)):\n",
    "            self.convs.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(in_channels if i == 0 else hidden_channels[i-1], hidden_channels[i])\n",
    "                    for edge_type in metadata['edge_types']\n",
    "                })\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels[-1], out_channels)\n",
    "        self.vector_quantizer = VectorQuantizer(num_embeddings, out_channels, commitment_cost)\n",
    "\n",
    "    def forward(self, x, edge_index_dict):\n",
    "        for i, convs in enumerate(self.convs):\n",
    "            # Apply the graph convolutions and average over all edge types\n",
    "            x = [conv(x, edge_index_dict[tuple(edge_type.split('_'))]) for edge_type, conv in convs.items()]\n",
    "            x = torch.stack(x, dim=0).mean(dim=0)\n",
    "            x = F.relu(x) if i < len(self.hidden_channels) - 1 else x\n",
    "        x = self.lin(x)\n",
    "        z_quantized, vq_loss = self.vector_quantizer(x)\n",
    "        return z_quantized, vq_loss\n",
    "\n",
    "class HeteroGAE_VariationalQuantizedEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_embeddings, commitment_cost, metadata={}):\n",
    "        super(HeteroGAE_VariationalQuantizedEncoder, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        latent_dim = out_channels\n",
    "        self.latent_dim = out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        for i in range(len(hidden_channels)):\n",
    "            self.convs.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(in_channels if i == 0 else hidden_channels[i-1], hidden_channels[i])\n",
    "                    for edge_type in metadata['edge_types']\n",
    "                })\n",
    "            )\n",
    "        self.fc_mu = Linear(hidden_channels[-1], latent_dim)\n",
    "        self.fc_logvar = Linear(hidden_channels[-1], latent_dim)\n",
    "        self.vector_quantizer = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)\n",
    "\n",
    "    def forward(self, x, edge_index_dict):\n",
    "        for i, convs in enumerate(self.convs):\n",
    "            # Apply the graph convolutions and average over all edge types\n",
    "            x = [conv(x, edge_index_dict[tuple(edge_type.split('_'))]) for edge_type, conv in convs.items()]\n",
    "            x = torch.stack(x, dim=0).mean(dim=0)\n",
    "            x = F.relu(x) if i < len(self.hidden_channels) - 1 else x\n",
    "        \n",
    "        # Obtain the mean and log variance for the latent variables\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Vector quantization\n",
    "        z_quantized, vq_loss = self.vector_quantizer(z)\n",
    "        \n",
    "        return z_quantized, vq_loss, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeteroGAE_Pairwise_Decoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_out_channels, xdim=20, hidden_channels={'res_backbone_res': [20, 20, 20]}, out_channels_hidden=20, Xdecoder_hidden=100, metadata={}):\n",
    "        super(HeteroGAE_Decoder, self).__init__()\n",
    "        self.convs1 = torch.nn.ModuleList()\n",
    "        self.convs2 = torch.nn.ModuleList()\n",
    "\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels_hidden = out_channels_hidden\n",
    "        self.in_channels = encoder_out_channels\n",
    "\n",
    "        for i in range(len(self.hidden_channels1[('res', 'backbone', 'res')])):\n",
    "            self.convs1.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(self.in_channels if i == 0 else self.hidden_channels[edge_type][i-1], self.hidden_channels[edge_type][i])\n",
    "                    for edge_type in [('res', 'backbone', 'res')]\n",
    "                })\n",
    "            )\n",
    "\n",
    "        self.lin = Linear(hidden_channels[('res', 'backbone', 'res')][-1], self.out_channels_hidden)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #detect interaction\n",
    "        self.detecter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.out_channels_hidden*2, Xdecoder_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Xdecoder_hidden, Xdecoder_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Xdecoder_hidden, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, z1, ,z2 ,  edge_index, backbones, **kwargs):\n",
    "        for layer in self.convs:\n",
    "            for edge_type, conv in layer.items():\n",
    "                for z in [z1, z2]:\n",
    "                    z = conv(z, backbones[tuple(edge_type.split('_'))])\n",
    "                    z = F.relu(z)\n",
    "                z = self.lin(z)\n",
    "        \n",
    "        #edge index is intraprotein in this case\n",
    "        sim_matrix = (z1[edge_index[0]] * z2[edge_index[1]]).sum(dim=1)\n",
    "\n",
    "        edge_probs = self.sigmoid(sim_matrix)\n",
    "        x_interact = self.detecter( torch z)\n",
    "        edge_probs = edge_probs*x_interact\n",
    "        \n",
    "        return x_interact,  edge_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = HeteroGAE_Encoder(in_channels=ndim, hidden_channels=[ 100 ]*3 , out_channels= 10, metadata=metadata , num_embeddings=256, commitment_cost= 1.25 )\n",
    "encoder.load_state_dict(torch.load(encoder_save))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder = HeteroGAE_Pairwise_Decoder(encoder_out_channels = encoder.out_channels , \n",
    "                            hidden_channels={ ( 'res','backbone','res'):[ 40 ] * 7  } , out_channels_hidden= 20 , metadata=metadata , amino_mapper = aaindex  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "\n",
    "def get_structure(pdb_file):\n",
    "    parser = PDB.PDBParser()\n",
    "    structure = parser.get_structure(\"X\", pdb_file)\n",
    "    return structure\n",
    "\n",
    "def get_chains(structure):\n",
    "    chains = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            chains.append(chain)\n",
    "    return chains\n",
    "\n",
    "def ret_pairwise(c1, c2 , threshold=10):\n",
    "    #return all pairwise distances between beta carbons\n",
    "    c1_atoms = [a for a in c1.get_atoms() if a.get_id() == \"CB\"]\n",
    "    c2_atoms = [a for a in c2.get_atoms() if a.get_id() == \"CB\"]\n",
    "    dists = np.zeros((len(c1_atoms), len(c2_atoms)))\n",
    "    for i, a1 in enumerate(c1_atoms):\n",
    "        for j, a2 in enumerate(c2_atoms):\n",
    "            dists[i,j] = a1 - a2\n",
    "    if threshold is not None:\n",
    "        np.clip(dists, 0, threshold, out=dists)\n",
    "    return dists\n",
    "\n",
    "def get_all_pairwise(chains):\n",
    "    #find all distance matrices between chains in complex\n",
    "    dists = {}\n",
    "    for i, c1 in enumerate(chains):\n",
    "        for j, c2 in enumerate(chains):\n",
    "            if i != j:\n",
    "                dists[(i,j)] = ret_pairwise(c1, c2)\n",
    "    return dists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pdb fixer on the pdb file\n",
    "from pdbfixer import PDBFixer\n",
    "def fix_pdb(pdb_file):\n",
    "    fixer = PDBFixer(pdb_file)\n",
    "    fixer.findMissingResidues()\n",
    "    fixer.findNonstandardResidues()\n",
    "    fixer.findMissingAtoms()\n",
    "    fixer.addMissingAtoms()\n",
    "    fixer.addMissingHydrogens()\n",
    "    #output fixed file\n",
    "    fixer.writePdb(pdb.split('.')[0] + \"_fixed.pdb\")\n",
    "    return pdb.split('.')[0] + \"_fixed.pdb\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use embedding to get z vecs\n",
    "from torch_geometric.data import HeteroData\n",
    "#import sageconv\n",
    "from torch_geometric.nn import SAGEConv , Linear\n",
    "#import module dict and module list\n",
    "from torch.nn import ModuleDict, ModuleList\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "#import negative sampling\n",
    "from torch_geometric.utils import negative_sampling\n",
    "#import graph pooling \n",
    "from torch_geometric.nn import global_mean_pool , SAGPooling\n",
    "\n",
    "class HeteroGAE_pair_Decoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_out_channels , xdim=20 , hidden_channels = [20,20,20] , pooling = [.5,.5,.1] , out_channels= 10 , Xdecoder_hidden = 100, metadata={}):\n",
    "        super(HeteroGAE_Decoder, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = encoder_out_channels\n",
    "        for i in range(len(self.hidden_channels)):\n",
    "            self.convs.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(self.in_channels if i == 0 else self.hidden_channels[i-1], self.hidden_channels[i])\n",
    "                    for edge_type in [ ( 'res','backbone','res') ]\n",
    "                })\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels[-1], out_channels)\n",
    "        #sigmoid to predict the edge probabilities after graph conv\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #global pooling layer\n",
    "\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        for i in range(len(pooling)):\n",
    "            if i == 0 :\n",
    "                self.pools.append( SAGPooling(hidden_channels[-1], ratio=pooling[i] , GNN=SAGEConv) ) \n",
    "            else:\n",
    "                self.pools.append( SAGPooling(pooling[i-1], ratio=pooling[i] , GNN=SAGEConv) )\n",
    "        self.pools.append( global_mean_pool() )\n",
    "        #sigmoid to predict the probability of interaction\n",
    "        self.interaction = linear(out_channels, 1)\n",
    "        self.interaction_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # add stack of dense layers to reconstruct the node features\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels , Xdecoder_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Xdecoder_hidden, Xdecoder_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Xdecoder_hidden , xdim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z , edge_index , backbone=None, **kwargs):\n",
    "        # Transform the latent space if necessary\n",
    "        edge_probs = {}\n",
    "        for layer in self.convs:\n",
    "            for edge_type, conv in layer.items():\n",
    "                z = conv(z, backbone)\n",
    "                z = F.relu(z)\n",
    "        zpool = z\n",
    "        #global pooling using backbone as node features\n",
    "        for i, pool in enumerate(self.pools):\n",
    "            zpool, edge_index, _, batch, _, _ = pool(zpool, edge_index, batch)\n",
    "        \n",
    "        #sigmoid to predict the probability of interaction\n",
    "        interaction = self.sigmoid(self.interaction(zpool))\n",
    "\n",
    "        z = self.lin(z)\n",
    "        sim_matrix =  (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "        edge_probs = self.sigmoid(sim_matrix)\n",
    "\n",
    "        #output the probability of interaction\n",
    "        #turn into connectivity with two columns for edge index\n",
    "        #edge_probs = torch.stack( torch.where( edge_probs > 0.5 ) , dim=0)\n",
    "        #reconstruct the node features with decoder\n",
    "        x_r = self.decoder(z)\n",
    "        return x_r , edge_probs , interaction\n",
    "    \n",
    "    def forward_retz(self, z , edge_index , backbone=None, **kwargs):\n",
    "        # Transform the latent space if necessary\n",
    "        edge_probs = {}\n",
    "        for layer in self.convs:\n",
    "            for edge_type, conv in layer.items():\n",
    "                z = conv(z, backbone)\n",
    "                z = F.relu(z)\n",
    "        z = self.lin(z)\n",
    "        sim_matrix =  (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "        edge_probs = self.sigmoid(sim_matrix)\n",
    "        #turn into connectivity with two columns for edge index\n",
    "        #edge_probs = torch.stack( torch.where( edge_probs > 0.5 ) , dim=0)\n",
    "        #reconstruct the node features with decoder\n",
    "        x_r = self.decoder(z)\n",
    "\n",
    "        return x_r , edge_probs , z\n",
    "\n",
    "EPS = 1e-10\n",
    "def recon_loss( z: Tensor, pos_edge_index: Tensor , backbone:Tensor = None , decoder = None ) -> Tensor:\n",
    "    r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
    "    entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
    "    sampled edges.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "        pos_edge_index (torch.Tensor): The positive edges to train against.\n",
    "        neg_edge_index (torch.Tensor, optional): The negative edges to\n",
    "            train against. If not given, uses negative sampling to\n",
    "            calculate negative edges. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    \n",
    "    pos =decoder(z, pos_edge_index, backbone )[1]\n",
    "    #turn pos edge index into a binary matrix\n",
    "    pos_loss = -torch.log( pos + EPS).mean()\n",
    "    neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "    neg = decoder(z ,  neg_edge_index, backbone )[1]\n",
    "    neg_loss = -torch.log( ( 1 - neg) + EPS ).mean()\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "#define loss for x reconstruction   \n",
    "def x_reconstruction_loss(x, recon_x):\n",
    "    \"\"\"\n",
    "    compute the loss over the node feature reconstruction.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(recon_x, x)\n",
    "\n",
    "\n",
    "#amino acid onehot loss for x reconstruction\n",
    "def aa_reconstruction_loss(x, recon_x):\n",
    "    \"\"\"\n",
    "    compute the loss over the node feature reconstruction.\n",
    "    using categorical cross entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    return F.cross_entropy(recon_x, x)\n",
    "\n",
    "def interaction_loss(pred_x, x):\n",
    "    \"\"\"\n",
    "    compute the binary cross entropy loss.\n",
    "    \"\"\"\n",
    "    return F.binary_cross_entropy(pred_x, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
