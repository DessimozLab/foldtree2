{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     code ch1 ch2  nres  len1  len2 dom_s1      dom_p1 dom_s2      dom_p2  \\\n",
      "0  4rk1_1   A   B  31.0   267   268  53822   PF13377.1  53822   PF13377.1   \n",
      "1  2gqn_1   D   B  53.0   392   392  53383  PF01053.15  53383  PF01053.15   \n",
      "2  2gqn_1   D   C  17.0   392   391  53383  PF01053.15  53383  PF01053.15   \n",
      "3  2gqn_1   A   C  52.0   391   391  53383  PF01053.15  53383  PF01053.15   \n",
      "4  2gqn_1   A   D  28.5   391   392  53383  PF01053.15  53383  PF01053.15   \n",
      "\n",
      "   ident  homo  \n",
      "0      1   1.0  \n",
      "1      1   1.0  \n",
      "2      1   1.0  \n",
      "3      1   1.0  \n",
      "4      1   1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "#get known complexes\n",
    "datadir = '/home/dmoi/datasets/foldtree2/complexes/'\n",
    "complexdf = pd.read_csv(datadir+'contactDefinition.txt' , sep='\\t')\n",
    "print(complexdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexdf['accession'] = complexdf.code.map(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173593\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "pdbs = glob.glob(datadir+'BU_all_renum/*.pdb')\n",
    "#remove fixed \n",
    "pdbs = [p for p in pdbs if 'fixed' not in p]\n",
    "print(len(pdbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     code ch1 ch2  nres  len1  len2 dom_s1      dom_p1 dom_s2      dom_p2  \\\n",
      "0  4rk1_1   A   B  31.0   267   268  53822   PF13377.1  53822   PF13377.1   \n",
      "1  2gqn_1   D   B  53.0   392   392  53383  PF01053.15  53383  PF01053.15   \n",
      "2  2gqn_1   D   C  17.0   392   391  53383  PF01053.15  53383  PF01053.15   \n",
      "3  2gqn_1   A   C  52.0   391   391  53383  PF01053.15  53383  PF01053.15   \n",
      "4  2gqn_1   A   D  28.5   391   392  53383  PF01053.15  53383  PF01053.15   \n",
      "\n",
      "   ident  homo accession                                            pdbfile  \n",
      "0      1   1.0      4rk1  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "1      1   1.0      2gqn  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "2      1   1.0      2gqn  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "3      1   1.0      2gqn  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "4      1   1.0      2gqn  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n"
     ]
    }
   ],
   "source": [
    "codes = { f.split('/')[-1].split('.')[0]:f for f in pdbs}\n",
    "complexdf['pdbfile'] = complexdf['code'].map(codes)\n",
    "print(complexdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       code ch1 ch2  nres  len1  len2 dom_s1      dom_p1 dom_s2      dom_p2  \\\n",
      "219  2bs5_1   A   C  15.5    90    90    NaN   PF07938.7    NaN   PF07938.7   \n",
      "220  2bs5_1   A   B  15.5    90    90    NaN   PF07938.7    NaN   PF07938.7   \n",
      "221  2bs5_1   C   B  15.5    90    90    NaN   PF07938.7    NaN   PF07938.7   \n",
      "356  3oak_2   C   B  19.0    31   151    NaN         NaN    NaN   PF08711.6   \n",
      "463  2f9n_1   D   C  10.5   243   243  50494  PF00089.21  50494  PF00089.21   \n",
      "\n",
      "     ident  homo accession                                            pdbfile  \n",
      "219      1   1.0      2bs5  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "220      1   1.0      2bs5  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "221      1   1.0      2bs5  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "356      0   NaN      3oak  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "463      1   1.0      2f9n  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "#sample 2000 codes\n",
    "import random\n",
    "sub = complexdf.code.unique().tolist()\n",
    "random.shuffle(sub)\n",
    "sub = sub[:2000]\n",
    "sub = complexdf[complexdf.code.isin(sub)]\n",
    "print(sub.head())\n",
    "print( len(sub) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2bs5_1.pdb'\n",
      " '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/3oak_2.pdb'\n",
      " '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2f9n_1.pdb' ...\n",
      " '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/5drw_1.pdb'\n",
      " '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/5cnx_1.pdb'\n",
      " '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/5gg4_2.pdb']\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print( sub.pdbfile.unique() )\n",
    "print( len(sub.pdbfile.unique() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run pdbfixer on all pdbs\n",
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile\n",
    "import tqdm\n",
    "import os\n",
    "import pebble \n",
    "import concurrent.futures\n",
    "\n",
    "def fix_pdb(pdbfile):\n",
    "    try:\n",
    "        if os.path.exists(pdbfile.replace('.pdb', '_fixed.pdb')):\n",
    "            return pdbfile.replace('.pdb', '_fixed.pdb')\n",
    "        \n",
    "        fixer = PDBFixer(filename=pdbfile)  \n",
    "        fixer.findMissingResidues()\n",
    "        fixer.findNonstandardResidues()\n",
    "        fixer.replaceNonstandardResidues()\n",
    "        fixer.removeHeterogens(True)\n",
    "        fixer.findMissingAtoms()\n",
    "        fixer.addMissingAtoms()\n",
    "        outfile = pdbfile.replace('.pdb', '_fixed.pdb')\n",
    "        with open(outfile, 'w') as w:\n",
    "            PDBFile.writeFile(fixer.topology, fixer.positions, w)\n",
    "        return outfile\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "\n",
    "fix_pdbs = False\n",
    "if fix_pdbs == True:\n",
    "    with pebble.ProcessPool() as pool:\n",
    "        futures = pool.map(fix_pdb, tqdm.tqdm(sub.pdbfile.unique().tolist()), timeout=60, workers=8, chunksize=8)\n",
    "        results = []\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures)):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "                results.append(result)\n",
    "            except TimeoutError as e:\n",
    "                print(e)\n",
    "            except Exception as e:\n",
    "                print(\"Error processing PDB:\", e)\n",
    "\n",
    "    # Add fixed filepaths to sub\n",
    "    sub['fixed_pdb'] = results\n",
    "    print( sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2712\n",
      "['/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2dft_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2qwv_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/1kpv_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/4huk_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/5g2z_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/1eqg_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2i3v_7_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/1pjh_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/3kx8_3_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/4bxc_2_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2uzl_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/3hsq_3_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/4i0x_6_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/3c6s_3_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/3x13_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/1jik_2_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/4b65_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/3ow1_2_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/2rg9_1_fixed.pdb', '/home/dmoi/datasets/foldtree2/complexes/BU_all_renum/1l2t_2_fixed.pdb']\n"
     ]
    }
   ],
   "source": [
    "fixed = glob.glob(datadir+'BU_all_renum/*_fixed.pdb')\n",
    "print( len(fixed) )\n",
    "print( fixed[:20] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664\n",
      "       code ch1 ch2  nres  len1  len2                         dom_s1  \\\n",
      "16   1z42_2   A   B  21.0   337   337                          51395   \n",
      "41   4yiq_2   D   C  17.5   108   107                            NaN   \n",
      "89   3wgo_1   A   B  56.0   323   324        51735_f1;55347;51735_f2   \n",
      "112  1t8m_6   A   D   7.0   239    58                          50494   \n",
      "113  3zmt_1   A   B  48.0   665   133  46689;51905_f1;54373;51905_f2   \n",
      "\n",
      "                    dom_p1                   dom_s2      dom_p2  ident  homo  \\\n",
      "16              PF00724.15                    51395  PF00724.15      1   1.0   \n",
      "41              PF07686.12                      NaN  PF07686.12      0   1.0   \n",
      "89              PF01113.15  51735_f1;55347;51735_f2  PF01113.15      1   1.0   \n",
      "112             PF00089.21                    57362  PF00014.18      0   0.0   \n",
      "113  PF04433.12;PF01593.19             267603;46689  PF00249.26      0   0.0   \n",
      "\n",
      "    accession                                            pdbfile  \\\n",
      "16       1z42  /home/dmoi/datasets/foldtree2/complexes/BU_all...   \n",
      "41       4yiq  /home/dmoi/datasets/foldtree2/complexes/BU_all...   \n",
      "89       3wgo  /home/dmoi/datasets/foldtree2/complexes/BU_all...   \n",
      "112      1t8m  /home/dmoi/datasets/foldtree2/complexes/BU_all...   \n",
      "113      3zmt  /home/dmoi/datasets/foldtree2/complexes/BU_all...   \n",
      "\n",
      "                                             fixed_pdb  \n",
      "16   /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "41   /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "89   /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "112  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "113  /home/dmoi/datasets/foldtree2/complexes/BU_all...  \n",
      "17686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69048/343266994.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub['fixed_pdb'] = sub.accession.map(fixed)\n"
     ]
    }
   ],
   "source": [
    "#get pdb ids from fixed\n",
    "fixed = {f.split('/')[-1].split('_')[0]:f for f in fixed}\n",
    "print( len(fixed) )\n",
    "#filter complex df to only fixed pdbs\n",
    "sub = complexdf[complexdf.accession.isin(fixed.keys())]\n",
    "sub['fixed_pdb'] = sub.accession.map(fixed)\n",
    "print(sub.head())\n",
    "print( len(sub) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmoi/projects/foldtree2\n"
     ]
    }
   ],
   "source": [
    "cd /home/dmoi/projects/foldtree2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a complex dataset of pytorch geometric objects\n",
    "import sys\n",
    "sys.path.append('/home/dmoi/projects/foldtree2/')\n",
    "\n",
    "import foldtree2_ecddcd as ft2\n",
    "import importlib\n",
    "importlib.reload(ft2)\n",
    "\n",
    "converter = ft2.PDB2PyG()\n",
    "store_complexes = False\n",
    "if store_complexes == True:\n",
    "    converter.store_pyg_complexdata(sub.fixed_pdb.unique() , datadir+'pyg_complexes.h5' , verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2659\n",
      "{'0': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[359, 20] },\n",
      "  plddt={ x=[359, 1] },\n",
      "  positions={ x=[359, 256] },\n",
      "  res={ x=[359, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1075],\n",
      "    edge_attr=[358],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 717],\n",
      "    edge_attr=[358],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3602],\n",
      "    edge_attr=[3602],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 236],\n",
      "    edge_attr=[236],\n",
      "  }\n",
      "), '10': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      "), '12': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      "), '14': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      "), '2': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      "), '4': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      "), '6': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      "), '8': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  AA={ x=[350, 20] },\n",
      "  plddt={ x=[350, 1] },\n",
      "  positions={ x=[350, 256] },\n",
      "  res={ x=[350, 844] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 1048],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 699],\n",
      "    edge_attr=[349],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 3514],\n",
      "    edge_attr=[3514],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 228],\n",
      "    edge_attr=[228],\n",
      "  }\n",
      ")} {'0_12': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 350] }\n",
      "), '0_4': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '0_6': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '0_8': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 24] }\n",
      "), '10_12': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '10_14': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '2_10': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 24] }\n",
      "), '2_14': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 350] }\n",
      "), '2_4': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '2_6': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '4_14': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 24] }\n",
      "), '4_8': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 350] }\n",
      "), '6_10': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 350] }\n",
      "), '6_12': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 24] }\n",
      "), '8_12': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      "), '8_14': HeteroData(\n",
      "  identifier='1al7_2_fixed',\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 1007] }\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "#load complex dataset\n",
    "comp_dataset = ft2.ComplexDataset( datadir+'pyg_complexes.h5' )\n",
    "print(len(comp_dataset.structlist))\n",
    "chains,complexes = comp_dataset[20]\n",
    "print(chains, complexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "import random \n",
    "import torch\n",
    "\n",
    "def find_pairs(chains, comlexes):\n",
    "    #go through all pairs in complexes. select those with an edge index that is not empty\n",
    "    pairs = {}\n",
    "    for c1 in chains:\n",
    "        print(c1,c2)\n",
    "        print(complexes[c1 +'_'+ c2][('res', 'contactPointsComplex', 'res')] ) \n",
    "        print(complexes[c1 +'_'+ c2])\n",
    "        edge_index = complexes[c1 +'_'+ c2][('res', 'contactPointsComplex', 'res')].edge_index\n",
    "        if edge_index.size(1) > 0:\n",
    "            pairs[c1+'_'+c2] = complexes[c1 +'_'+ c2]\n",
    "    \n",
    "    #select a random pair\n",
    "    pairs = {k:pairs[k] for k in random.sample(list(pairs.keys()), 1)}    \n",
    "    return pairs\n",
    "\n",
    "def complex_loader(comp_dataset , neg_samples=.50 ,  batchsize = 10):\n",
    "\n",
    "    allstructs = comp_dataset.structlist \n",
    "    while True:\n",
    "        if bernoulli.rvs(neg_samples) == 1:\n",
    "            #positive sample\n",
    "            s = random.choice(allstructs)\n",
    "            chains,complexes = comp_dataset[s]\n",
    "            complexes = find_pairs(chains, complexes)\n",
    "            chains = {k.split('_')[0]:chains[k.split('_')[0]] for k in pairs}\n",
    "            yield chains,complexes,torch.tensor([1.0])\n",
    "            #sample a complex\n",
    "        else:\n",
    "            #negative sample\n",
    "            s1 = random.choice(allstructs)\n",
    "            s2 = random.choice(allstructs)\n",
    "            while s1 == s2:\n",
    "                s2 = random.choice(allstructs)\n",
    "            chains1,complexes1 = comp_dataset[s1]\n",
    "            chains2,complexes2 = comp_dataset[s2]\n",
    "            #select a random chain from each complex\n",
    "            chain1 = random.choice(list(chains1.keys()))\n",
    "            chain2 = random.choice(list(chains2.keys()))\n",
    "            chains = {chain1:chains1[chain1], chain2:chains2[chain2]}\n",
    "            #complexes has just an empty tensor with 2 dimensions\n",
    "            complexes = {chain1+'_'+chain2:torch.tensor([])}\n",
    "            #draw two random chains from different complexes\n",
    "            yield chains,complexes,torch.tensor([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "{'edge_index': tensor([], size=(2, 0), dtype=torch.int64)}\n",
      "HeteroData(\n",
      "  identifier='1a4h_1_fixed',\n",
      "  (res, contactPointsComplex, res)={},\n",
      "  (res, contactPointsComplex, res)={ edge_index=[2, 0] }\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loader \u001b[38;5;241m=\u001b[39m complex_loader(comp_dataset)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m )\n",
      "Cell \u001b[0;32mIn[117], line 30\u001b[0m, in \u001b[0;36mcomplex_loader\u001b[0;34m(comp_dataset, neg_samples, batchsize)\u001b[0m\n\u001b[1;32m     28\u001b[0m s \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(allstructs)\n\u001b[1;32m     29\u001b[0m chains,complexes \u001b[38;5;241m=\u001b[39m comp_dataset[s]\n\u001b[0;32m---> 30\u001b[0m complexes \u001b[38;5;241m=\u001b[39m \u001b[43mfind_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m chains \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]:chains[k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m pairs}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m chains,complexes,torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[117], line 19\u001b[0m, in \u001b[0;36mfind_pairs\u001b[0;34m(chains, comlexes)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 pairs[c1\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mc2] \u001b[38;5;241m=\u001b[39m complexes[c1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m c2]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#select a random pair\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m pairs \u001b[38;5;241m=\u001b[39m {k:pairs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m}    \n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pairs\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.8/random.py:363\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    361\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    365\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "loader = complex_loader(comp_dataset)\n",
    "print( next(loader) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "#import sageconv\n",
    "from torch_geometric.nn import SAGEConv , Linear , FiLMConv , TransformerConv , FeaStConv , GATConv , GINConv , GatedGraphConv\n",
    "#import module dict and module list\n",
    "from torch.nn import ModuleDict, ModuleList , L1Loss\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "#import negative sampling\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "def recon_loss_complex( z1: Tensor,z2: Tensor, pos_edge_index: Tensor , backbone1:Tensor = None ,backbone2:Tensor = None , decoder = None , poslossmod = 1 , neglossmod= 1) -> Tensor:\n",
    "    r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
    "    entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
    "    sampled edges.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "        pos_edge_index (torch.Tensor): The positive edges to train against.\n",
    "        neg_edge_index (torch.Tensor, optional): The negative edges to\n",
    "            train against. If not given, uses negative sampling to\n",
    "            calculate negative edges. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    \n",
    "    pos =decoder(z1,z2, pos_edge_index, { ( 'res','backbone','res'): backbone1 } , { ( 'res','backbone','res'): backbone2 } )[1]\n",
    "    #turn pos edge index into a binary matrix\n",
    "    pos_loss = -torch.log( pos + EPS).mean()\n",
    "    neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "    neg = decoder(z1 , z2, neg_edge_index, { ( 'res','backbone','res'): backbone1 } , { ( 'res','backbone','res'): backbone2 } )[1]\n",
    "    neg_loss = -torch.log( ( 1 - neg) + EPS ).mean()\n",
    "    return poslossmod*pos_loss + neglossmod*neg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv , Linear\n",
    "from torch.nn import Sigmoid, ModuleDict\n",
    "\n",
    "class HeteroGAE_Pairwise_Decoder(torch.nn.Module):\n",
    "    #we don't need to decode to aa... just contact probs\n",
    "    def __init__(self, encoder_out_channels, xdim=20, hidden_channels={'res_backbone_res': [20, 20, 20]}, out_channels_hidden=20, nheads = 1 , Xdecoder_hidden=30, metadata={}, amino_mapper= None):\n",
    "        super(HeteroGAE_Pairwise_Decoder, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels_hidden = out_channels_hidden\n",
    "        self.in_channels = encoder_out_channels\n",
    "        for i in range(len(self.hidden_channels[('res', 'backbone', 'res')])):\n",
    "            self.convs.append(\n",
    "                torch.nn.ModuleDict({\n",
    "                    '_'.join(edge_type): SAGEConv(self.in_channels if i == 0 else self.hidden_channels[edge_type][i-1], self.hidden_channels[edge_type][i]  )\n",
    "                    for edge_type in [('res', 'backbone', 'res')]\n",
    "                })\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels[('res', 'backbone', 'res')][-1], self.out_channels_hidden)\n",
    "        self.sigmoid = Sigmoid()\n",
    "    def forward(self, z1, z2, edge_index, backbones, **kwargs):\n",
    "        zs = []\n",
    "        for z in [z1,z2]:\n",
    "            inz = z\n",
    "            for layer in self.convs:\n",
    "                for edge_type, conv in layer.items():\n",
    "                    z = conv(z, backbones[tuple(edge_type.split('_'))])\n",
    "                    z = F.relu(z)\n",
    "            z = self.lin(z)\n",
    "            zs.append(z)\n",
    "        sim_matrix = (zs[0][edge_index[0]] * zs[1][edge_index[1]]).sum(dim=1)\n",
    "        edge_probs = self.sigmoid(sim_matrix)\n",
    "        \n",
    "        return edge_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "ndim = 844\n",
    "converter = ft2.PDB2PyG()\n",
    "with open( 'model.pkl' , 'rb' ) as f:\n",
    "    encoder, d = pickle.load( f )\n",
    "alphabetsize = encoder.vector_quantizer.embedding_dim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "#create new decoder\n",
    "decoder = HeteroGAE_Pairwise_Decoder(encoder_out_channels = encoder.out_channels , \n",
    "                            hidden_channels={ ( 'res','backbone','res'):[ 300 ] * 5  } , \n",
    "                            out_channels_hidden= 150 , metadata={} , Xdecoder_hidden=100 )\n",
    "\n",
    "decoder_save = 'decoder_complex1st'\n",
    "#put encoder and decoder on the device\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Create a DataLoader for training\n",
    "total_loss_x = 0\n",
    "total_loss_edge = 0\n",
    "total_vq=0\n",
    "total_kl = 0\n",
    "total_plddt=0\n",
    "# Training loop\n",
    "if os.path.exists(decoder_save):\n",
    "    decoder.load_state_dict(torch.load(decoder_save))\n",
    "    print(\"loaded encoder and decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(struct_dat, batch_size=40, shuffle=True)\n",
    "optimizer = torch.optim.Adam( list(decoder.parameters()), lr=0.001)\n",
    "encoder.eval()\n",
    "decoder.train()\n",
    "edgelosses = []\n",
    "edgeweight = 1\n",
    "for epoch in range(1000):\n",
    "    for nsteps,data in tqdm.tqdm(enumerate(train_loader)):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z1 = encoder(data['res'].x, data['AA'].x , data.edge_index_dict)\n",
    "        z2 = encoder(data['res'].x, data['AA'].x , data.edge_index_dict)\n",
    "        #add positional encoding to give to the decoder\n",
    "        edgeloss = recon_loss(z1,z2, , data.edge_index_dict[( 'res','contactPoints','res')] , data.edge_index_dict[( 'res','backbone','res')], decoder)\n",
    "        loss =  edgeweight*edgeloss\n",
    "        total_loss_edge += edgeloss.item()\n",
    "        if nsteps % batchsize == 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    if epoch % 100 == 0 :\n",
    "        #save model\n",
    "        torch.save(decoder.state_dict(), decoder_save)\n",
    "        with open('decoder_complex.pkl' , 'wb') as out:\n",
    "            out.write( pickle.dumps( decoder ) )\n",
    "    print(f'Epoch {epoch}, Edge Loss: {total_loss_edge:.4f}') \n",
    "    total_loss_edge = 0\n",
    "    #total_plddt = 0\n",
    "torch.save(decoder.state_dict(), decoder_save)\n",
    "with open('decoder_complex.pkl' , 'wb') as out:\n",
    "    out.write( pickle.dumps( decoder ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brainstorming to get complex graphs...\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class GraphGenerationModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, nhead, num_encoder_layers, num_decoder_layers, num_classes):\n",
    "        super(GraphGenerationModel, self).__init__()\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=input_dim, nhead=nhead)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.decoder_layer = TransformerDecoderLayer(d_model=output_dim, nhead=nhead)\n",
    "        self.transformer_decoder = TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.node_mlp = nn.Linear(output_dim, num_classes + 1)  # +1 for EOS token\n",
    "        self.edge_mlp = nn.Linear(2 * output_dim, 1)\n",
    "        self.embedding = nn.Embedding(num_classes + 1, output_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.transformer_encoder(x)\n",
    "    \n",
    "    def decode(self, z, memory):\n",
    "        return self.transformer_decoder(z, memory)\n",
    "    \n",
    "    def forward(self, x, max_length=20):\n",
    "        # Encode the input sequence using the transformer encoder\n",
    "        memory = self.encode(x)  # Shape: [batch_size, seq_len, input_dim]\n",
    "        \n",
    "        # Start with an initial input (e.g., a start token)\n",
    "        start_token = torch.zeros((memory.size(0), 1, memory.size(-1)), device=x.device)\n",
    "        z = start_token\n",
    "        \n",
    "        node_logits = []\n",
    "        for _ in range(max_length):\n",
    "            z = self.decode(z, memory)\n",
    "            node_logit = self.node_mlp(z[:, -1, :])  # Take the last token's output\n",
    "            node_logits.append(node_logit)\n",
    "            \n",
    "            # Get the next input token (embedding of the predicted class)\n",
    "            next_token = node_logit.argmax(dim=-1).unsqueeze(1)\n",
    "            next_token_embedding = self.embedding(next_token)\n",
    "            z = torch.cat([z, next_token_embedding], dim=1)\n",
    "            \n",
    "            if (next_token == num_classes).all():  # EOS token\n",
    "                break\n",
    "        \n",
    "        node_logits = torch.stack(node_logits, dim=1)\n",
    "        \n",
    "        # Decode edges (fully connected example, customize as needed)\n",
    "        num_nodes = node_logits.size(1)\n",
    "        edge_index = torch.combinations(torch.arange(num_nodes), r=2).t().contiguous()\n",
    "        edge_features = torch.cat([node_logits[:, edge_index[0]], node_logits[:, edge_index[1]]], dim=-1)\n",
    "        edge_logits = self.edge_mlp(edge_features).squeeze(-1)\n",
    "        edge_probs = torch.sigmoid(edge_logits)\n",
    "        \n",
    "        return node_logits, edge_index, edge_probs\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10   # Dimension of input embeddings\n",
    "hidden_dim = 32  # Dimension of hidden layer\n",
    "output_dim = 16  # Dimension of node features\n",
    "nhead = 2        # Number of heads in the multi-head attention\n",
    "num_encoder_layers = 2  # Number of transformer encoder layers\n",
    "num_decoder_layers = 2  # Number of transformer decoder layers\n",
    "num_classes = 5  # Number of node categories (excluding EOS token)\n",
    "\n",
    "model = GraphGenerationModel(input_dim, hidden_dim, output_dim, nhead, num_encoder_layers, num_decoder_layers, num_classes)\n",
    "\n",
    "# Example input: batch of sequences (batch_size, seq_len, input_dim)\n",
    "x = torch.randn(5, 10, input_dim)  # Batch of 5 sequences, each of length 10\n",
    "\n",
    "node_logits, edge_index, edge_probs = model(x)\n",
    "print(\"Node logits:\\n\", node_logits)\n",
    "print(\"Edge index:\\n\", edge_index)\n",
    "print(\"Edge probabilities:\\n\", edge_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████▊                                            | 36180/89764 [10:45<15:56, 56.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m graphs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(complexdf\u001b[38;5;241m.\u001b[39mcode\u001b[38;5;241m.\u001b[39munique()):\n",
      "\u001b[0;32m----> 6\u001b[0m     sub \u001b[38;5;241m=\u001b[39m complexdf[\u001b[43mcomplexdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m]\n",
      "\u001b[1;32m      7\u001b[0m     chains \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(sub\u001b[38;5;241m.\u001b[39mch1\u001b[38;5;241m.\u001b[39munique())\u001b[38;5;241m.\u001b[39munion( \u001b[38;5;28mset\u001b[39m( sub\u001b[38;5;241m.\u001b[39mch2\u001b[38;5;241m.\u001b[39munique() ))\n",
      "\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#add chains to graph\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.8/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n",
      "\u001b[1;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[1;32m     79\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n",
      "\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.8/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n",
      "\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n",
      "\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.8/site-packages/pandas/core/series.py:6096\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n",
      "\u001b[1;32m   6093\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m   6095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;32m-> 6096\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   6098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:293\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n",
      "\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n",
      "\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;32m--> 293\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    296\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:82\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n",
      "\u001b[1;32m     80\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n",
      "\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 82\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "chunksize = 10000\n",
    "graphs = {}\n",
    "for code in tqdm.tqdm(complexdf.code.unique()):\n",
    "    sub = complexdf[complexdf.code == code]\n",
    "    chains = set(sub.ch1.unique()).union( set( sub.ch2.unique() ))\n",
    "    #add chains to graph\n",
    "    G = nx.MultiGraph()\n",
    "    G.add_nodes_from(chains)\n",
    "    for i, row in sub.iterrows():\n",
    "        #homology link\n",
    "        if row.homo == 1:\n",
    "            #add homology type edge\n",
    "            ekey = G.add_edge(row.ch1, row.ch2, key='homology' )\n",
    "        #add contact type edge\n",
    "        ekey = G.add_edge(row.ch1, row.ch2, key='contact' )\n",
    "    graphs[code] = G\n",
    "\n",
    "with open('complexgraphs.pkl' , 'wb') as graphsout:\n",
    "    graphsout.write( pickle.dumps( graphs ) )\n",
    "\n",
    "#embed all structures\n",
    "#use decoder and generate z vecs\n",
    "#use decoder sigmoid to get contact proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, epochs=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    edge_loss_fn = nn.BCELoss()  # Loss function for edge probabilities\n",
    "    node_loss_fn = nn.NLLLoss()  # Loss function for node classification\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_edge_loss = 0\n",
    "        total_node_loss = 0\n",
    "\n",
    "        for sequences, graphs, node_labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            node_logits, edge_index, edge_probs = model(sequences)\n",
    "\n",
    "            # Flatten the edge probabilities and create a target tensor\n",
    "            target_edge_probs = torch.cat([graph.edge_index for graph in graphs], dim=1)\n",
    "            target = torch.ones(target_edge_probs.size(1))  # Example target, customize as needed\n",
    "\n",
    "            # Compute edge loss\n",
    "            edge_loss = edge_loss_fn(edge_probs, target)\n",
    "\n",
    "            # Compute node classification loss\n",
    "            node_labels = node_labels.view(-1)\n",
    "            node_classes = node_logits.view(-1, num_classes + 1)  # +1 for EOS token\n",
    "            node_loss = node_loss_fn(node_classes, node_labels)\n",
    "\n",
    "            # Total loss\n",
    "            loss = edge_loss + node_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_edge_loss += edge_loss.item()\n",
    "            total_node_loss += node_loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Edge Loss: {total_edge_loss / len(dataloader)}, Node Loss: {total_node_loss / len(dataloader)}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
