{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c660247e",
   "metadata": {},
   "source": [
    "# Multi-Family Multi-Model Information Content Benchmark\n",
    "\n",
    "This notebook performs comprehensive information-theoretic benchmarks on **multiple protein families** using **multiple FoldTree2 models** with different alphabet sizes, comparing them against standard amino acid representations.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Evaluate how well discrete structural alphabets (DSR - Discrete Structural Representations) capture biologically relevant information compared to amino acid sequences, specifically testing:\n",
    "\n",
    "1. **Fold Discrimination**: Can k-mers distinguish protein folds better?\n",
    "2. **Entropy Rate**: How compressible are the representations?\n",
    "3. **Position-Specific Conservation**: Does entropy reflect functional constraints?\n",
    "4. **Cross-Representation Information**: How much structural information predicts sequence?\n",
    "\n",
    "## Family Data Sources\n",
    "\n",
    "This notebook supports **two family data sources**:\n",
    "\n",
    "### Option 1: Custom Families (USE_MAKESUBMAT_FAMILIES = False)\n",
    "Manual curation of protein families in separate folders:\n",
    "```\n",
    "FAMILIES_DIR/\n",
    "\t├── rhodopsin/\n",
    "\t│   ├── structure1.pdb\n",
    "\t│   └── structure2.pdb\n",
    "\t├── kinase/\n",
    "\t│   ├── structure3.pdb\n",
    "\t│   └── structure4.pdb\n",
    "\t└── ...\n",
    "```\n",
    "\n",
    "### Option 2: AFDB Cluster Families (USE_MAKESUBMAT_FAMILIES = True) ⭐ RECOMMENDED\n",
    "**Uses the same ~200+ families as `makesubmat.py`** from AlphaFold Database clusters:\n",
    "```\n",
    "struct_align/              # Created by makesubmat.py\n",
    "\t├── AF-A0A024RBG1-F1/   # AFDB cluster representative ID\n",
    "\t│   └── structs/\n",
    "\t│       ├── AF-A0A024RBG1-F1-model_v4.pdb\n",
    "\t│       ├── AF-P12345-F1-model_v4.pdb\n",
    "\t│       └── ...\n",
    "\t├── AF-A0A075B6H9-F1/\n",
    "\t│   └── structs/\n",
    "\t│       └── ...\n",
    "\t└── ... (200+ families)\n",
    "```\n",
    "\n",
    "**Prerequisites for Option 2**:\n",
    "1. Run `makesubmat.py --download_structs` to fetch AFDB cluster structures\n",
    "2. Point `MAKESUBMAT_BASE_DIR` to your datasets directory\n",
    "3. Set `USE_MAKESUBMAT_FAMILIES = True` in Cell #3\n",
    "\n",
    "**Advantages of Option 2**:\n",
    "- **Consistency**: Benchmark on same data used for substitution matrices\n",
    "- **Scale**: ~200+ diverse protein families automatically\n",
    "- **Reproducibility**: Standard dataset from AlphaFold Database clusters\n",
    "- **No manual curation**: Families pre-defined by structural clustering\n",
    "\n",
    "## Benchmarks Performed (Per Family, Per Model)\n",
    "\n",
    "### 1. **K-mer Fold Discrimination**\n",
    "- Compute k-mer frequency distributions (k=1,2,3,4)\n",
    "- Perform KMeans clustering\n",
    "- Calculate silhouette scores as discrimination metric\n",
    "- **Higher scores** = better fold separation\n",
    "\n",
    "### 2. **Entropy Rate Estimation**  \n",
    "- Build k-order Markov models (orders 0-3)\n",
    "- Cross-validation for entropy estimation\n",
    "- **Lower entropy** = more predictable/compressible representation\n",
    "\n",
    "### 3. **Per-Position Entropy**\n",
    "- Create Multiple Sequence Alignments (MSAs)\n",
    "- Apply Henikoff sequence reweighting\n",
    "- Calculate position-specific entropy\n",
    "- **Entropy profile** reveals conserved vs variable positions\n",
    "\n",
    "### 4. **Cross-Representation Mutual Information**\n",
    "- Train Ridge regression: FoldTree2 features → AA features\n",
    "- Cross-validation R² scores\n",
    "- Spearman correlation between representations\n",
    "- **Higher scores** = more shared information\n",
    "\n",
    "## Outputs\n",
    "\n",
    "1. **JSON**: `all_families_results.json` - Complete results\n",
    "2. **CSV**: `all_families_results.csv` - Flattened table\n",
    "3. **Plots**:\n",
    "\t - `multi_family_benchmark_comparison.png` - Line plots across families\n",
    "\t - `model_family_heatmaps.png` - Performance heatmaps\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Configure** (Cell #3):\n",
    "\t - **Choose data source**: Set `USE_MAKESUBMAT_FAMILIES = True` (AFDB) or `False` (custom)\n",
    "\t - **Set paths**: \n",
    "\t\t - If using AFDB: `MAKESUBMAT_BASE_DIR` (datasets directory)\n",
    "\t\t - If custom: `FAMILIES_DIR` (your families directory)\n",
    "\t - **Optional**: Set `MAX_FAMILIES` to limit number (useful for testing)\n",
    "\t - Set `MODELS` to list of FoldTree2 model paths\n",
    "\t - Adjust `BENCHMARK_PARAMS` if needed\n",
    "\n",
    "2. **Run cells sequentially** (Cells 1-9)\n",
    "\n",
    "3. **View results**:\n",
    "\t - Terminal output shows progress and summary statistics\n",
    "\t - Plots display comparative performance\n",
    "\t - CSV/JSON files for detailed analysis\n",
    "\n",
    "## Quick Start with AFDB Families\n",
    "\n",
    "```python\n",
    "# In Cell #3:\n",
    "USE_MAKESUBMAT_FAMILIES = True\n",
    "MAKESUBMAT_BASE_DIR = '/path/to/your/datasets'  # Contains struct_align/\n",
    "MAX_FAMILIES = 50  # Start with 50 families for quick test (or None for all)\n",
    "\n",
    "# Then run all cells\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cf7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\t\t\t\t\t\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import FoldTree2 modules\n",
    "sys.path.insert(0, '/home/dmoi/projects/foldtree2')\n",
    "from foldtree2.src import encoder as ft2\n",
    "from foldtree2.src.pdbgraph import PDB2PyG, StructureDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2490a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using makesubmat AFDB cluster families from: /mnt/data2/datasets/struct_align\n",
      "\n",
      "Configuration loaded:\n",
      "  Models: 4\n",
      "  Families directory: /mnt/data2/datasets/struct_align\n",
      "  Output: /home/dmoi/projects/foldtree2/benchmark_output\n",
      "  Device: cuda\n",
      "Scanning directories in /mnt/data2/datasets/struct_align...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discovering families: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discovering families: 2508it [00:15, 159.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m families\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Discover families\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m FAMILIES \u001b[38;5;241m=\u001b[39m \u001b[43mdiscover_families\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mFAMILIES_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m\t\u001b[49m\u001b[43muse_makesubmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_MAKESUBMAT_FAMILIES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_families\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_FAMILIES\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDiscovered \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(FAMILIES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m protein families:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(FAMILIES) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 116\u001b[0m, in \u001b[0;36mdiscover_families\u001b[0;34m(families_dir, use_makesubmat, max_families)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_makesubmat:\n\u001b[1;32m    114\u001b[0m \t\u001b[38;5;66;03m# Makesubmat structure: {repId}/structs/*.pdb\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \tstructs_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(family_base_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructs_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    117\u001b[0m \t\tpbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m \t\t\u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Model paths (list of models to benchmark)\n",
    "MODELS = [\n",
    "\t'/home/dmoi/projects/foldtree2/models/model_10_embeddings',\n",
    "\t'/home/dmoi/projects/foldtree2/models/model_20_embeddings',\n",
    "\t'/home/dmoi/projects/foldtree2/models/model_30_embeddings',\n",
    "\t'/home/dmoi/projects/foldtree2/models/model_40_embeddings',\n",
    "]\n",
    "\n",
    "# ==================== FAMILY DATA SOURCE CONFIGURATION ====================\n",
    "# Choose one of two modes:\n",
    "#   1. Custom families in separate folders (USE_MAKESUBMAT_FAMILIES = False)\n",
    "#   2. AFDB cluster families from makesubmat (USE_MAKESUBMAT_FAMILIES = True)\n",
    "\n",
    "USE_MAKESUBMAT_FAMILIES = True  # Set to True to use makesubmat AFDB families\n",
    "\n",
    "if USE_MAKESUBMAT_FAMILIES:\n",
    "\t# Path to the struct_align directory created by makesubmat.py\n",
    "\t# This contains one subfolder per AFDB cluster representative\n",
    "\t# Structure: MAKESUBMAT_BASE_DIR/struct_align/{repId}/structs/*.pdb\n",
    "\tMAKESUBMAT_BASE_DIR = '/mnt/data2/datasets'\n",
    "\tFAMILIES_DIR = os.path.join(MAKESUBMAT_BASE_DIR, 'struct_align')\n",
    "\t\n",
    "\t# Optional: Limit number of families (useful for quick tests)\n",
    "\t# Set to None to use all available families\n",
    "\tMAX_FAMILIES = None  # or e.g., 50 for first 50 families\n",
    "\t\n",
    "\tprint(f\"Using makesubmat AFDB cluster families from: {FAMILIES_DIR}\")\n",
    "else:\n",
    "\t# Custom families directory containing subfolders for each protein family\n",
    "\t# Each subfolder should contain PDB files for that family\n",
    "\t# Example structure:\n",
    "\t#   FAMILIES_DIR/\n",
    "\t#     rhodopsin/\n",
    "\t#       structure1.pdb\n",
    "\t#       structure2.pdb\n",
    "\t#     kinase/\n",
    "\t#       structure3.pdb\n",
    "\t#       structure4.pdb\n",
    "\tFAMILIES_DIR = '/home/dmoi/projects/foldtree2/families/examples'\n",
    "\tMAX_FAMILIES = None\n",
    "\t\n",
    "\tprint(f\"Using custom families from: {FAMILIES_DIR}\")\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = '/home/dmoi/projects/foldtree2/benchmark_output'\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_PARAMS = {\n",
    "\t'k_mer_sizes': [1, 2, 3, 4],\n",
    "\t'markov_orders': [0, 1, 2, 3],\n",
    "\t'cv_folds': 5,\n",
    "\t'alpha_smoothing': 0.001,\n",
    "\t'occupancy_threshold': 0.7,\n",
    "\t'reweight_threshold': 0.8,\n",
    "}\n",
    "\n",
    "# Standard amino acid alphabet\n",
    "AA_ALPHABET = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "print(f\"\\nConfiguration loaded:\")\n",
    "print(f\"  Models: {len(MODELS)}\")\n",
    "print(f\"  Families directory: {FAMILIES_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "\n",
    "# ==================== FAMILY DISCOVERY ====================\n",
    "\n",
    "def discover_families(families_dir, use_makesubmat=False, max_families=None):\n",
    "\t\"\"\"\n",
    "\tDiscover protein families from directory structure.\n",
    "\t\n",
    "\tSupports two modes:\n",
    "\t1. Custom families: Each subfolder is a family\n",
    "\t2. Makesubmat AFDB: Each subfolder contains a 'structs' subdirectory\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tfamilies_dir: Base directory containing families\n",
    "\t\tuse_makesubmat: If True, expect makesubmat structure (repId/structs/)\n",
    "\t\tmax_families: Maximum number of families to include (None = all)\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tdict: {family_name: {'path': str, 'n_structures': int}}\n",
    "\t\"\"\"\n",
    "\tfamilies = {}\n",
    "\t\n",
    "\tif not os.path.isdir(families_dir):\n",
    "\t\tprint(f\"ERROR: Families directory not found: {families_dir}\")\n",
    "\t\treturn families\n",
    "\t\n",
    "\tprint(f\"Scanning directories in {families_dir}...\")\n",
    "\t\n",
    "\t# Use os.scandir for efficient directory traversal\n",
    "\tfamily_count = 0\n",
    "\twith tqdm.tqdm(desc=\"Discovering families\") as pbar:\n",
    "\t\tfor entry in os.scandir(families_dir):\n",
    "\t\t\tif not entry.is_dir():\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Check if we've reached the max_families limit\n",
    "\t\t\tif max_families is not None and family_count >= max_families:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tfamily_base_path = entry.path\n",
    "\t\t\t\n",
    "\t\t\t# Determine the actual structures directory\n",
    "\t\t\tif use_makesubmat:\n",
    "\t\t\t\t# Makesubmat structure: {repId}/structs/*.pdb\n",
    "\t\t\t\tstructs_dir = os.path.join(family_base_path, 'structs')\n",
    "\t\t\t\tif not os.path.isdir(structs_dir):\n",
    "\t\t\t\t\tpbar.update(1)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfamily_path = structs_dir\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Custom structure: {family_name}/*.pdb\n",
    "\t\t\t\tfamily_path = family_base_path\n",
    "\t\t\t\n",
    "\t\t\t# Count PDB files using os.scandir (much faster than glob)\n",
    "\t\t\tpdb_count = 0\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfor file_entry in os.scandir(family_path):\n",
    "\t\t\t\t\tif file_entry.is_file() and file_entry.name.endswith('.pdb'):\n",
    "\t\t\t\t\t\tpdb_count += 1\n",
    "\t\t\texcept (PermissionError, OSError):\n",
    "\t\t\t\tpbar.update(1)\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Only include if there are structures\n",
    "\t\t\tif pdb_count > 0:\n",
    "\t\t\t\tfamilies[entry.name] = {\n",
    "\t\t\t\t\t'path': family_path,\n",
    "\t\t\t\t\t'n_structures': pdb_count\n",
    "\t\t\t\t}\n",
    "\t\t\t\tfamily_count += 1\n",
    "\t\t\t\n",
    "\t\t\tpbar.update(1)\n",
    "\t\n",
    "\treturn families\n",
    "\n",
    "# Discover families\n",
    "FAMILIES = discover_families(\n",
    "\tFAMILIES_DIR, \n",
    "\tuse_makesubmat=USE_MAKESUBMAT_FAMILIES,\n",
    "\tmax_families=MAX_FAMILIES\n",
    ")\n",
    "\n",
    "print(f\"\\nDiscovered {len(FAMILIES)} protein families:\")\n",
    "if len(FAMILIES) == 0:\n",
    "\tprint(f\"ERROR: No families found!\")\n",
    "\tif USE_MAKESUBMAT_FAMILIES:\n",
    "\t\tprint(f\"Expected structure: {FAMILIES_DIR}/{{repId}}/structs/*.pdb\")\n",
    "\t\tprint(f\"Make sure you've run makesubmat.py with --download_structs first\")\n",
    "\telse:\n",
    "\t\tprint(f\"Expected structure: {FAMILIES_DIR}/{{family_name}}/*.pdb\")\n",
    "else:\n",
    "\t# Show first 10 families as sample\n",
    "\tsample_families = list(FAMILIES.items())[:10]\n",
    "\tfor family_name, family_info in sample_families:\n",
    "\t\tprint(f\"  - {family_name}: {family_info['n_structures']} structures\")\n",
    "\t\n",
    "\tif len(FAMILIES) > 10:\n",
    "\t\tprint(f\"  ... and {len(FAMILIES) - 10} more families\")\n",
    "\t\n",
    "\t# Show statistics\n",
    "\tstructure_counts = [info['n_structures'] for info in FAMILIES.values()]\n",
    "\tprint(f\"\\nFamily statistics:\")\n",
    "\tprint(f\"  Total families: {len(FAMILIES)}\")\n",
    "\tprint(f\"  Total structures: {sum(structure_counts)}\")\n",
    "\tprint(f\"  Structures per family (mean ± std): {np.mean(structure_counts):.1f} ± {np.std(structure_counts):.1f}\")\n",
    "\tprint(f\"  Range: {min(structure_counts)} - {max(structure_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41509327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for empty directories in: /mnt/data2/datasets/struct_align\n",
      "Mode: DELETION ENABLED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning directories: 1110161it [4:22:56, 59.40it/s] "
     ]
    }
   ],
   "source": [
    "from directory_tree import DisplayTree\n",
    "\n",
    "# ==================== CLEANUP: REMOVE EMPTY DIRECTORIES ====================\n",
    "\n",
    "def cleanup_empty_directories(base_dir, use_makesubmat=False, dry_run=True):\n",
    "\t\"\"\"\n",
    "\tRemove directories that don't contain any structure files.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tbase_dir: Base directory to scan\n",
    "\t\tuse_makesubmat: If True, look for structs/ subdirectory\n",
    "\t\tdry_run: If True, only report what would be deleted (don't actually delete)\n",
    "\t\n",
    "\tReturns:\n",
    "\t\ttuple: (n_deleted, list_of_deleted_paths)\n",
    "\t\"\"\"\n",
    "\tif not os.path.isdir(base_dir):\n",
    "\t\tprint(f\"ERROR: Directory not found: {base_dir}\")\n",
    "\t\treturn 0, []\n",
    "\t\n",
    "\tdeleted_dirs = []\n",
    "\tprint(f\"Scanning for empty directories in: {base_dir}\")\n",
    "\tprint(f\"Mode: {'DRY RUN (no deletion)' if dry_run else 'DELETION ENABLED'}\")\n",
    "\t\n",
    "\trmcount = 0\n",
    "\twith tqdm.tqdm(desc=\"Scanning directories\") as pbar:\n",
    "\t\tfor entry in os.scandir(base_dir):\n",
    "\t\t\tif not entry.is_dir():\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Determine the actual structures directory\n",
    "\t\t\tif use_makesubmat:\n",
    "\t\t\t\t# Check if structs/ subdirectory exists\n",
    "\t\t\t\tstructs_dir = os.path.join(entry.path, 'structs')\n",
    "\t\t\t\tif not os.path.isdir(structs_dir):\n",
    "\t\t\t\t\t# No structs directory - mark for deletion\n",
    "\t\t\t\t\tcheck_dir = entry.path\n",
    "\t\t\t\t\thas_structures = False\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcheck_dir = structs_dir\n",
    "\t\t\t\t\t# Count PDB files in structs/\n",
    "\t\t\t\t\thas_structures = False\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tfor file_entry in os.scandir(check_dir):\n",
    "\t\t\t\t\t\t\tif file_entry.is_file() and file_entry.name.endswith('.pdb'):\n",
    "\t\t\t\t\t\t\t\thas_structures = True\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\texcept (PermissionError, OSError):\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Check for PDB files directly in directory\n",
    "\t\t\t\tcheck_dir = entry.path\n",
    "\t\t\t\thas_structures = False\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfor file_entry in os.scandir(check_dir):\n",
    "\t\t\t\t\t\tif file_entry.is_file() and file_entry.name.endswith('.pdb'):\n",
    "\t\t\t\t\t\t\thas_structures = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\texcept (PermissionError, OSError):\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\n",
    "\t\t\t# If no structures found, mark for deletion\n",
    "\t\t\tif not has_structures:\n",
    "\t\t\t\tdeleted_dirs.append(entry.path)\n",
    "\t\t\t\tif not dry_run:\n",
    "\t\t\t\t\timport shutil\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tshutil.rmtree(entry.path)\n",
    "\t\t\t\t\t\t#print(f\"  ✓ Deleted: {entry.name}\")\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\tprint(f\"  ✗ Failed to delete {entry.name}: {e}\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"  [DRY RUN] Would delete: {entry.name}\")\n",
    "\t\t\t\t\t#print dir tree strcuture\n",
    "\t\t\t\t\tDisplayTree(entry.path, maxDepth=3, showHidden=True)\n",
    "\n",
    "\t\t\t\trmcount += 1\n",
    "\t\t\t\tif dry_run and rmcount > 30:\n",
    "\t\t\t\t\tprint(\"  ... Dry run limit reached (200). Stopping further checks.\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tpbar.update(1)\n",
    "\t\n",
    "\tprint(f\"\\n{'='*70}\")\n",
    "\tprint(f\"Summary:\")\n",
    "\tprint(f\"  Empty directories found: {len(deleted_dirs)}\")\n",
    "\tif not dry_run:\n",
    "\t\tprint(f\"  Directories deleted: {len(deleted_dirs)}\")\n",
    "\telse:\n",
    "\t\tprint(f\"  (Set dry_run=False to actually delete)\")\n",
    "\tprint(f\"{'='*70}\")\n",
    "\t\n",
    "\treturn len(deleted_dirs), deleted_dirs\n",
    "\n",
    "# Example usage (with dry_run=True by default for safety):\n",
    "# Uncomment to run cleanup\n",
    "\n",
    "n_deleted, deleted_paths = cleanup_empty_directories(\n",
    "\tFAMILIES_DIR,\n",
    "\tuse_makesubmat=USE_MAKESUBMAT_FAMILIES,\n",
    "\tdry_run=False  # Change to False to actually delete\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✓ Cleanup utility loaded\")\n",
    "print(\"  Run cleanup_empty_directories() to scan for empty directories\")\n",
    "print(\"  Set dry_run=False to actually delete them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fce6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA LOADING & ENCODING ====================\n",
    "\n",
    "class ModelBenchmark:\n",
    "\t\"\"\"Container for model encoding results and metadata\"\"\"\n",
    "\tdef __init__(self, model_path: str, device):\n",
    "\t\tself.model_path = model_path\n",
    "\t\tself.model_name = Path(model_path).stem\n",
    "\t\tself.device = device\n",
    "\t\t\n",
    "\t\t# Load encoder\n",
    "\t\tprint(f\"  Loading model: {self.model_name}\")\n",
    "\t\tself.encoder = torch.load(model_path + '.pt', map_location=device, weights_only=False)\n",
    "\t\tself.encoder = self.encoder.to(device)\n",
    "\t\tself.encoder.device = device\n",
    "\t\tself.encoder.eval()\n",
    "\t\t\n",
    "\t\t# Extract model metadata\n",
    "\t\tself.num_embeddings = self.encoder.num_embeddings\n",
    "\t\tself.embedding_dim = self.encoder.out_channels\n",
    "\t\t\n",
    "\t\t# Storage for encoded sequences\n",
    "\t\tself.encoded_fasta = None\n",
    "\t\tself.encoded_df = None\n",
    "\t\tself.alphabet = None\n",
    "\t\tself.char_position_map = None\n",
    "\t\t\n",
    "\t\tprint(f\"    ✓ Loaded: {self.num_embeddings} embeddings, dim={self.embedding_dim}\")\n",
    "\t\n",
    "\tdef encode_structures(self, structures_loader, output_dir, family_name=\"encoded\"):\n",
    "\t\t\"\"\"Encode structures using this model\"\"\"\n",
    "\t\toutput_path = os.path.join(output_dir, f\"{family_name}_{self.model_name}_encoded.fasta\")\n",
    "\t\t\n",
    "\t\tprint(f\"  Encoding structures with {self.model_name}...\")\n",
    "\t\tself.encoder.encode_structures_fasta(\n",
    "\t\t\tstructures_loader, \n",
    "\t\t\toutput_path, \n",
    "\t\t\treplace=True\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.encoded_fasta = output_path\n",
    "\t\tself.encoded_df = ft2.load_encoded_fasta(output_path, alphabet=None, replace=False)\n",
    "\t\tself._build_alphabet()\n",
    "\t\t\n",
    "\t\tprint(f\"    ✓ Encoded {len(self.encoded_df)} sequences\")\n",
    "\t\treturn output_path\n",
    "\t\n",
    "\tdef _build_alphabet(self):\n",
    "\t\t\"\"\"Build alphabet from encoded sequences\"\"\"\n",
    "\t\tchar_set = set()\n",
    "\t\tfor seq in self.encoded_df.seq:\n",
    "\t\t\tchar_set = char_set.union(set(seq))\n",
    "\t\tself.alphabet = sorted(list(char_set))\n",
    "\t\tself.char_position_map = {char: i for i, char in enumerate(self.alphabet)}\n",
    "\t\tprint(f\"    ✓ Alphabet size: {len(self.alphabet)} characters\")\n",
    "\n",
    "def load_structures(structures_dir: str, converter: PDB2PyG, verbose: bool = True):\n",
    "\t\"\"\"Load and convert PDB structures to PyG format\"\"\"\n",
    "\tpdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "\tif verbose:\n",
    "\t\tprint(f\"  Found {len(pdb_files)} PDB files\")\n",
    "\t\n",
    "\tif len(pdb_files) == 0:\n",
    "\t\traise ValueError(f\"No PDB files found in {structures_dir}\")\n",
    "\t\n",
    "\tdef struct_generator():\n",
    "\t\tfor pdb_file in pdb_files:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdata = converter.struct2pyg(pdb_file)\n",
    "\t\t\t\tif data:\n",
    "\t\t\t\t\tyield data\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tprint(f\"    Warning: Failed to convert {Path(pdb_file).name}: {e}\")\n",
    "\t\t\t\tcontinue\n",
    "\t\n",
    "\treturn struct_generator()\n",
    "\n",
    "def extract_aa_sequences(structures_dir: str, output_dir: str, family_name: str = \"sequences\", verbose: bool = True):\n",
    "\t\"\"\"Extract amino acid sequences from PDB files\"\"\"\n",
    "\tparser = PDBParser(QUIET=True)\n",
    "\taa_dict = {\n",
    "\t\t'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "\t\t'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "\t\t'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "\t\t'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "\t}\n",
    "\t\n",
    "\tpdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "\tsequences = {}\n",
    "\t\n",
    "\tif verbose:\n",
    "\t\tprint(f\"  Extracting amino acid sequences from {len(pdb_files)} structures...\")\n",
    "\tfor pdb_file in pdb_files:\n",
    "\t\tstructure_id = Path(pdb_file).stem\n",
    "\t\ttry:\n",
    "\t\t\tstructure = parser.get_structure(structure_id, pdb_file)\n",
    "\t\t\tseq = \"\"\n",
    "\t\t\tfor model in structure:\n",
    "\t\t\t\tfor chain in model:\n",
    "\t\t\t\t\tfor residue in chain:\n",
    "\t\t\t\t\t\tif residue.get_resname() in aa_dict:\n",
    "\t\t\t\t\t\t\tseq += aa_dict[residue.get_resname()]\n",
    "\t\t\t\t\tbreak  # Only first chain\n",
    "\t\t\t\tbreak  # Only first model\n",
    "\t\t\t\n",
    "\t\t\tif seq:\n",
    "\t\t\t\tsequences[structure_id] = seq\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f\"    Warning: Failed to extract sequence from {Path(pdb_file).name}: {e}\")\n",
    "\t\n",
    "\t# Write to FASTA with family-specific naming\n",
    "\toutput_path = os.path.join(output_dir, f\"{family_name}_aa_sequences.fasta\")\n",
    "\twith open(output_path, 'w') as f:\n",
    "\t\tfor struct_id, seq in sequences.items():\n",
    "\t\t\tf.write(f\">{struct_id}\\n{seq}\\n\")\n",
    "\t\n",
    "\tif verbose:\n",
    "\t\tprint(f\"    ✓ Extracted {len(sequences)} AA sequences\")\n",
    "\treturn output_path, sequences\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize converter\n",
    "converter = PDB2PyG(aapropcsv='/home/dmoi/projects/foldtree2/foldtree2/config/aaindex1.csv')\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40233aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RUN ALL BENCHMARKS PER FAMILY ====================\n",
    "\n",
    "# Store results per family\n",
    "all_results = {}\n",
    "\n",
    "for family_name, family_info in FAMILIES.items():\n",
    "\tprint(f\"\\n{'='*70}\")\n",
    "\tprint(f\"FAMILY: {family_name}\")\n",
    "\tprint(f\"{'='*70}\")\n",
    "\tprint(f\"  {family_info['n_structures']} structures in {family_info['path']}\")\n",
    "\t\n",
    "\tfamily_path = family_info['path']\n",
    "\tfamily_results = {\n",
    "\t\t'n_structures': family_info['n_structures'],\n",
    "\t\t'models': [],\n",
    "\t\t'alphabet_sizes': [],\n",
    "\t\t'k_mer_discrimination': {},\n",
    "\t\t'entropy_rates': {},\n",
    "\t\t'per_position_entropy': {},\n",
    "\t\t'cross_representation_mi': {}\n",
    "\t}\n",
    "\t\n",
    "\t# Extract AA sequences for this family\n",
    "\tprint(f\"\\n1. Extracting amino acid sequences for {family_name}...\")\n",
    "\taa_fasta_path, aa_sequences = extract_aa_sequences(\n",
    "\t\tfamily_path, OUTPUT_DIR, family_name, verbose=True\n",
    "\t)\n",
    "\t\n",
    "\t# Load structures once for all models\n",
    "\tprint(f\"\\n2. Loading structures for {family_name}...\")\n",
    "\t\n",
    "\t# Encode structures with each model\n",
    "\tmodel_benchmarks = []\n",
    "\tfor model_path in MODELS:\n",
    "\t\tprint(f\"\\n3. Processing model: {Path(model_path).stem}\")\n",
    "\t\tmb = ModelBenchmark(model_path, DEVICE)\n",
    "\t\t\n",
    "\t\t# Load and encode structures\n",
    "\t\tstructures_loader = load_structures(family_path, converter, verbose=True)\n",
    "\t\tmb.encode_structures(structures_loader, OUTPUT_DIR, family_name)\n",
    "\t\tmodel_benchmarks.append(mb)\n",
    "\t\t\n",
    "\t\tfamily_results['models'].append(mb.model_name)\n",
    "\t\tfamily_results['alphabet_sizes'].append(mb.num_embeddings)\n",
    "\t\n",
    "\tprint(f\"\\n{'─'*70}\")\n",
    "\tprint(f\"BENCHMARKING {family_name}\")\n",
    "\tprint(f\"{'─'*70}\")\n",
    "\t\n",
    "\t# ==================== BENCHMARK 1: K-MER FOLD DISCRIMINATION ====================\n",
    "\tprint(\"\\n4. K-mer Fold Discrimination Test\")\n",
    "\tprint(\"   Testing if k-mers can discriminate folds better than random...\")\n",
    "\t\n",
    "\tfor mb in model_benchmarks:\n",
    "\t\tmodel_results = {}\n",
    "\t\tfor k in [1, 2, 3, 4]:\n",
    "\t\t\t# Count k-mers for each sequence\n",
    "\t\t\tkmer_counts = []\n",
    "\t\t\tfor seq in mb.encoded_df.seq:\n",
    "\t\t\t\tkmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "\t\t\t\tkmer_counter = Counter(kmers)\n",
    "\t\t\t\t# Create feature vector from k-mer counts\n",
    "\t\t\t\tkmer_counts.append(list(kmer_counter.values()))\n",
    "\t\t\t\n",
    "\t\t\t# Pad to same length\n",
    "\t\t\tmax_len = max(len(x) for x in kmer_counts)\n",
    "\t\t\tX = np.array([x + [0]*(max_len-len(x)) for x in kmer_counts])\n",
    "\t\t\t\n",
    "\t\t\t# KMeans clustering\n",
    "\t\t\tkmeans = KMeans(n_clusters=min(3, len(X)), random_state=42)\n",
    "\t\t\tlabels = kmeans.fit_predict(X)\n",
    "\t\t\tscore = silhouette_score(X, labels)\n",
    "\t\t\t\n",
    "\t\t\tmodel_results[f'k={k}'] = score\n",
    "\t\t\n",
    "\t\tfamily_results['k_mer_discrimination'][mb.model_name] = model_results\n",
    "\t\tprint(f\"   {mb.model_name}: k=1: {model_results['k=1']:.3f}, k=2: {model_results['k=2']:.3f}, k=3: {model_results['k=3']:.3f}, k=4: {model_results['k=4']:.3f}\")\n",
    "\t\n",
    "\t# Also run for AA sequences\n",
    "\taa_results = {}\n",
    "\tfor k in [1, 2, 3, 4]:\n",
    "\t\tkmer_counts = []\n",
    "\t\tfor seq in aa_sequences.values():\n",
    "\t\t\tkmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "\t\t\tkmer_counter = Counter(kmers)\n",
    "\t\t\tkmer_counts.append(list(kmer_counter.values()))\n",
    "\t\t\n",
    "\t\tmax_len = max(len(x) for x in kmer_counts)\n",
    "\t\tX = np.array([x + [0]*(max_len-len(x)) for x in kmer_counts])\n",
    "\t\t\n",
    "\t\tkmeans = KMeans(n_clusters=min(3, len(X)), random_state=42)\n",
    "\t\tlabels = kmeans.fit_predict(X)\n",
    "\t\tscore = silhouette_score(X, labels)\n",
    "\t\taa_results[f'k={k}'] = score\n",
    "\t\n",
    "\tfamily_results['k_mer_discrimination']['AA'] = aa_results\n",
    "\tprint(f\"   AA: k=1: {aa_results['k=1']:.3f}, k=2: {aa_results['k=2']:.3f}, k=3: {aa_results['k=3']:.3f}, k=4: {aa_results['k=4']:.3f}\")\n",
    "\t\n",
    "\t# ==================== BENCHMARK 2: ENTROPY RATE ESTIMATION ====================\n",
    "\tprint(\"\\n5. Entropy Rate Estimation\")\n",
    "\tprint(\"   Estimating entropy rate using k-order Markov models...\")\n",
    "\t\n",
    "\tfor mb in model_benchmarks:\n",
    "\t\tmodel_results = {}\n",
    "\t\tsequences = list(mb.encoded_df.seq)\n",
    "\t\t\n",
    "\t\tfor order in [0, 1, 2, 3]:\n",
    "\t\t\t# k-fold cross-validation\n",
    "\t\t\tn_folds = min(5, len(sequences))\n",
    "\t\t\tfold_size = len(sequences) // n_folds\n",
    "\t\t\tentropies = []\n",
    "\t\t\t\n",
    "\t\t\tfor fold in range(n_folds):\n",
    "\t\t\t\t# Split data\n",
    "\t\t\t\ttest_start = fold * fold_size\n",
    "\t\t\t\ttest_end = test_start + fold_size\n",
    "\t\t\t\ttest_seqs = sequences[test_start:test_end]\n",
    "\t\t\t\ttrain_seqs = sequences[:test_start] + sequences[test_end:]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Build k-order Markov model from training data\n",
    "\t\t\t\tif order == 0:\n",
    "\t\t\t\t\t# 0-order: character frequencies\n",
    "\t\t\t\t\tchar_counts = Counter()\n",
    "\t\t\t\t\tfor seq in train_seqs:\n",
    "\t\t\t\t\t\tchar_counts.update(seq)\n",
    "\t\t\t\t\ttotal = sum(char_counts.values())\n",
    "\t\t\t\t\tprobs = {c: (count + 0.001) / (total + 0.001 * len(char_counts)) \n",
    "\t\t\t\t\t\t\tfor c, count in char_counts.items()}\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Calculate entropy on test set\n",
    "\t\t\t\t\ttest_entropy = 0\n",
    "\t\t\t\t\tfor seq in test_seqs:\n",
    "\t\t\t\t\t\tfor c in seq:\n",
    "\t\t\t\t\t\t\tif c in probs:\n",
    "\t\t\t\t\t\t\t\ttest_entropy += -np.log2(probs[c])\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\ttest_entropy += -np.log2(0.001 / (total + 0.001 * len(char_counts)))\n",
    "\t\t\t\t\ttest_entropy /= sum(len(s) for s in test_seqs)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# k-order: k-gram frequencies\n",
    "\t\t\t\t\tkgram_counts = Counter()\n",
    "\t\t\t\t\tcontext_counts = Counter()\n",
    "\t\t\t\t\tfor seq in train_seqs:\n",
    "\t\t\t\t\t\tfor i in range(len(seq) - order):\n",
    "\t\t\t\t\t\t\tcontext = seq[i:i+order]\n",
    "\t\t\t\t\t\t\tnext_char = seq[i+order]\n",
    "\t\t\t\t\t\t\tkgram_counts[(context, next_char)] += 1\n",
    "\t\t\t\t\t\t\tcontext_counts[context] += 1\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Calculate entropy on test set\n",
    "\t\t\t\t\ttest_entropy = 0\n",
    "\t\t\t\t\tfor seq in test_seqs:\n",
    "\t\t\t\t\t\tfor i in range(len(seq) - order):\n",
    "\t\t\t\t\t\t\tcontext = seq[i:i+order]\n",
    "\t\t\t\t\t\t\tnext_char = seq[i+order]\n",
    "\t\t\t\t\t\t\t# Laplace smoothing\n",
    "\t\t\t\t\t\t\tcount = kgram_counts.get((context, next_char), 0) + 0.001\n",
    "\t\t\t\t\t\t\ttotal = context_counts.get(context, 0) + 0.001 * len(mb.alphabet)\n",
    "\t\t\t\t\t\t\tprob = count / total\n",
    "\t\t\t\t\t\t\ttest_entropy += -np.log2(prob)\n",
    "\t\t\t\t\ttest_entropy /= sum(len(s) - order for s in test_seqs)\n",
    "\t\t\t\t\n",
    "\t\t\t\tentropies.append(test_entropy)\n",
    "\t\t\t\n",
    "\t\t\tmodel_results[f'order={order}'] = np.mean(entropies)\n",
    "\t\t\n",
    "\t\tfamily_results['entropy_rates'][mb.model_name] = model_results\n",
    "\t\tprint(f\"   {mb.model_name}: 0-order: {model_results['order=0']:.3f}, 1-order: {model_results['order=1']:.3f}, 2-order: {model_results['order=2']:.3f}, 3-order: {model_results['order=3']:.3f}\")\n",
    "\t\n",
    "\t# Also run for AA sequences\n",
    "\taa_alphabet = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "\taa_results = {}\n",
    "\tsequences = list(aa_sequences.values())\n",
    "\t\n",
    "\tfor order in [0, 1, 2, 3]:\n",
    "\t\tn_folds = min(5, len(sequences))\n",
    "\t\tfold_size = len(sequences) // n_folds\n",
    "\t\tentropies = []\n",
    "\t\t\n",
    "\t\tfor fold in range(n_folds):\n",
    "\t\t\ttest_start = fold * fold_size\n",
    "\t\t\ttest_end = test_start + fold_size\n",
    "\t\t\ttest_seqs = sequences[test_start:test_end]\n",
    "\t\t\ttrain_seqs = sequences[:test_start] + sequences[test_end:]\n",
    "\t\t\t\n",
    "\t\t\tif order == 0:\n",
    "\t\t\t\tchar_counts = Counter()\n",
    "\t\t\t\tfor seq in train_seqs:\n",
    "\t\t\t\t\tchar_counts.update(seq)\n",
    "\t\t\t\ttotal = sum(char_counts.values())\n",
    "\t\t\t\tprobs = {c: (count + 0.001) / (total + 0.001 * len(char_counts)) \n",
    "\t\t\t\t\t\tfor c, count in char_counts.items()}\n",
    "\t\t\t\t\n",
    "\t\t\t\ttest_entropy = 0\n",
    "\t\t\t\tfor seq in test_seqs:\n",
    "\t\t\t\t\tfor c in seq:\n",
    "\t\t\t\t\t\tif c in probs:\n",
    "\t\t\t\t\t\t\ttest_entropy += -np.log2(probs[c])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttest_entropy += -np.log2(0.001 / (total + 0.001 * len(char_counts)))\n",
    "\t\t\t\ttest_entropy /= sum(len(s) for s in test_seqs)\n",
    "\t\t\telse:\n",
    "\t\t\t\tkgram_counts = Counter()\n",
    "\t\t\t\tcontext_counts = Counter()\n",
    "\t\t\t\tfor seq in train_seqs:\n",
    "\t\t\t\t\tfor i in range(len(seq) - order):\n",
    "\t\t\t\t\t\tcontext = seq[i:i+order]\n",
    "\t\t\t\t\t\tnext_char = seq[i+order]\n",
    "\t\t\t\t\t\tkgram_counts[(context, next_char)] += 1\n",
    "\t\t\t\t\t\tcontext_counts[context] += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\ttest_entropy = 0\n",
    "\t\t\t\tfor seq in test_seqs:\n",
    "\t\t\t\t\tfor i in range(len(seq) - order):\n",
    "\t\t\t\t\t\tcontext = seq[i:i+order]\n",
    "\t\t\t\t\t\tnext_char = seq[i+order]\n",
    "\t\t\t\t\t\tcount = kgram_counts.get((context, next_char), 0) + 0.001\n",
    "\t\t\t\t\t\ttotal = context_counts.get(context, 0) + 0.001 * len(aa_alphabet)\n",
    "\t\t\t\t\t\tprob = count / total\n",
    "\t\t\t\t\t\ttest_entropy += -np.log2(prob)\n",
    "\t\t\t\ttest_entropy /= sum(len(s) - order for s in test_seqs)\n",
    "\t\t\t\n",
    "\t\t\tentropies.append(test_entropy)\n",
    "\t\t\n",
    "\t\taa_results[f'order={order}'] = np.mean(entropies)\n",
    "\t\n",
    "\tfamily_results['entropy_rates']['AA'] = aa_results\n",
    "\tprint(f\"   AA: 0-order: {aa_results['order=0']:.3f}, 1-order: {aa_results['order=1']:.3f}, 2-order: {aa_results['order=2']:.3f}, 3-order: {aa_results['order=3']:.3f}\")\n",
    "\t\n",
    "\t# ==================== BENCHMARK 3: PER-POSITION ENTROPY ====================\n",
    "\tprint(\"\\n6. Per-Position Entropy\")\n",
    "\tprint(\"   Calculating position-specific entropy from MSAs...\")\n",
    "\t\n",
    "\tfor mb in model_benchmarks:\n",
    "\t\t# Create MSA\n",
    "\t\tsequences = list(mb.encoded_df.seq)\n",
    "\t\tmax_len = max(len(s) for s in sequences)\n",
    "\t\t\n",
    "\t\t# Pad sequences with gaps\n",
    "\t\tpadded = [s + '-' * (max_len - len(s)) for s in sequences]\n",
    "\t\t\n",
    "\t\t# Calculate Henikoff weights\n",
    "\t\tweights = []\n",
    "\t\tfor i, seq in enumerate(padded):\n",
    "\t\t\tweight = 0\n",
    "\t\t\tfor pos in range(max_len):\n",
    "\t\t\t\tchar = seq[pos]\n",
    "\t\t\t\tn_different = len(set(s[pos] for s in padded))\n",
    "\t\t\t\tn_with_char = sum(1 for s in padded if s[pos] == char)\n",
    "\t\t\t\tweight += 1.0 / (n_different * n_with_char)\n",
    "\t\t\tweights.append(weight)\n",
    "\t\t\n",
    "\t\t# Normalize weights\n",
    "\t\ttotal_weight = sum(weights)\n",
    "\t\tweights = [w / total_weight for w in weights]\n",
    "\t\t\n",
    "\t\t# Calculate per-position entropy\n",
    "\t\tposition_entropies = []\n",
    "\t\tfor pos in range(max_len):\n",
    "\t\t\tchar_weights = {}\n",
    "\t\t\tfor i, seq in enumerate(padded):\n",
    "\t\t\t\tchar = seq[pos]\n",
    "\t\t\t\tchar_weights[char] = char_weights.get(char, 0) + weights[i]\n",
    "\t\t\t\n",
    "\t\t\t# Filter positions with low occupancy\n",
    "\t\t\tgap_weight = char_weights.get('-', 0)\n",
    "\t\t\tif gap_weight > 0.3:  # Skip if >30% gaps\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Calculate entropy\n",
    "\t\t\tentropy = 0\n",
    "\t\t\tfor char, weight in char_weights.items():\n",
    "\t\t\t\tif char != '-' and weight > 0:\n",
    "\t\t\t\t\tentropy += -weight * np.log2(weight)\n",
    "\t\t\t\n",
    "\t\t\tposition_entropies.append(entropy)\n",
    "\t\t\n",
    "\t\tavg_entropy = np.mean(position_entropies) if position_entropies else 0\n",
    "\t\tfamily_results['per_position_entropy'][mb.model_name] = {\n",
    "\t\t\t'mean': avg_entropy,\n",
    "\t\t\t'std': np.std(position_entropies) if position_entropies else 0\n",
    "\t\t}\n",
    "\t\tprint(f\"   {mb.model_name}: mean={avg_entropy:.3f}, std={np.std(position_entropies) if position_entropies else 0:.3f}\")\n",
    "\t\n",
    "\t# Also run for AA sequences\n",
    "\tsequences = list(aa_sequences.values())\n",
    "\tmax_len = max(len(s) for s in sequences)\n",
    "\tpadded = [s + '-' * (max_len - len(s)) for s in sequences]\n",
    "\t\n",
    "\tweights = []\n",
    "\tfor i, seq in enumerate(padded):\n",
    "\t\tweight = 0\n",
    "\t\tfor pos in range(max_len):\n",
    "\t\t\tchar = seq[pos]\n",
    "\t\t\tn_different = len(set(s[pos] for s in padded))\n",
    "\t\t\tn_with_char = sum(1 for s in padded if s[pos] == char)\n",
    "\t\t\tweight += 1.0 / (n_different * n_with_char)\n",
    "\t\tweights.append(weight)\n",
    "\t\n",
    "\ttotal_weight = sum(weights)\n",
    "\tweights = [w / total_weight for w in weights]\n",
    "\t\n",
    "\tposition_entropies = []\n",
    "\tfor pos in range(max_len):\n",
    "\t\tchar_weights = {}\n",
    "\t\tfor i, seq in enumerate(padded):\n",
    "\t\t\tchar = seq[pos]\n",
    "\t\t\tchar_weights[char] = char_weights.get(char, 0) + weights[i]\n",
    "\t\t\n",
    "\t\tgap_weight = char_weights.get('-', 0)\n",
    "\t\tif gap_weight > 0.3:\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tentropy = 0\n",
    "\t\tfor char, weight in char_weights.items():\n",
    "\t\t\tif char != '-' and weight > 0:\n",
    "\t\t\t\tentropy += -weight * np.log2(weight)\n",
    "\t\t\n",
    "\t\tposition_entropies.append(entropy)\n",
    "\t\n",
    "\tavg_entropy = np.mean(position_entropies) if position_entropies else 0\n",
    "\tfamily_results['per_position_entropy']['AA'] = {\n",
    "\t\t'mean': avg_entropy,\n",
    "\t\t'std': np.std(position_entropies) if position_entropies else 0\n",
    "\t}\n",
    "\tprint(f\"   AA: mean={avg_entropy:.3f}, std={np.std(position_entropies) if position_entropies else 0:.3f}\")\n",
    "\t\n",
    "\t# ==================== BENCHMARK 4: CROSS-REPRESENTATION MUTUAL INFORMATION ====================\n",
    "\tprint(\"\\n7. Cross-Representation Mutual Information\")\n",
    "\tprint(\"   Training neural predictors to estimate MI...\")\n",
    "\t\n",
    "\tfor mb in model_benchmarks:\n",
    "\t\t# Prepare data: FoldTree2 sequences -> AA sequences\n",
    "\t\tft2_seqs = mb.encoded_df.seq.values\n",
    "\t\taa_seqs = [aa_sequences[struct_id] for struct_id in mb.encoded_df.protid.values \n",
    "\t\t\t\t   if struct_id in aa_sequences]\n",
    "\t\t\n",
    "\t\t# Create k-mer feature vectors\n",
    "\t\tk = 2\n",
    "\t\tft2_features = []\n",
    "\t\taa_features = []\n",
    "\t\t\n",
    "\t\tfor ft2_seq, aa_seq in zip(ft2_seqs[:len(aa_seqs)], aa_seqs):\n",
    "\t\t\t# FoldTree2 k-mers\n",
    "\t\t\tft2_kmers = [ft2_seq[i:i+k] for i in range(len(ft2_seq)-k+1)]\n",
    "\t\t\tft2_counter = Counter(ft2_kmers)\n",
    "\t\t\tft2_features.append(list(ft2_counter.values()))\n",
    "\t\t\t\n",
    "\t\t\t# AA k-mers\n",
    "\t\t\taa_kmers = [aa_seq[i:i+k] for i in range(len(aa_seq)-k+1)]\n",
    "\t\t\taa_counter = Counter(aa_kmers)\n",
    "\t\t\taa_features.append(list(aa_counter.values()))\n",
    "\t\t\n",
    "\t\t# Pad to same length\n",
    "\t\tmax_ft2 = max(len(x) for x in ft2_features)\n",
    "\t\tmax_aa = max(len(x) for x in aa_features)\n",
    "\t\t\n",
    "\t\tX_ft2 = np.array([x + [0]*(max_ft2-len(x)) for x in ft2_features])\n",
    "\t\tX_aa = np.array([x + [0]*(max_aa-len(x)) for x in aa_features])\n",
    "\t\t\n",
    "\t\t# Train simple linear predictor: FoldTree2 -> AA\n",
    "\t\tfrom sklearn.linear_model import Ridge\n",
    "\t\tmodel = Ridge(alpha=1.0)\n",
    "\t\t\n",
    "\t\t# Cross-validation\n",
    "\t\tfrom sklearn.model_selection import cross_val_score\n",
    "\t\tscores = cross_val_score(model, X_ft2, X_aa, cv=min(5, len(X_ft2)), \n",
    "\t\t\t\t\t\t\t\t scoring='r2')\n",
    "\t\t\n",
    "\t\t# Calculate Spearman correlation as MI proxy\n",
    "\t\tmodel.fit(X_ft2, X_aa)\n",
    "\t\tpredictions = model.predict(X_ft2)\n",
    "\t\t\n",
    "\t\t# Spearman correlation between predicted and actual\n",
    "\t\tfrom scipy.stats import spearmanr\n",
    "\t\tcorrelations = []\n",
    "\t\tfor i in range(X_aa.shape[1]):\n",
    "\t\t\tcorr, _ = spearmanr(predictions[:, i], X_aa[:, i])\n",
    "\t\t\tif not np.isnan(corr):\n",
    "\t\t\t\tcorrelations.append(corr)\n",
    "\t\t\n",
    "\t\tavg_correlation = np.mean(correlations) if correlations else 0\n",
    "\t\tfamily_results['cross_representation_mi'][mb.model_name] = {\n",
    "\t\t\t'r2_score': scores.mean(),\n",
    "\t\t\t'spearman_corr': avg_correlation\n",
    "\t\t}\n",
    "\t\tprint(f\"   {mb.model_name}: R²={scores.mean():.3f}, Spearman={avg_correlation:.3f}\")\n",
    "\t\n",
    "\t# Store family results\n",
    "\tall_results[family_name] = family_results\n",
    "\tprint(f\"\\n✓ Completed benchmarks for {family_name}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ ALL FAMILIES COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed {len(all_results)} families:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "\n",
    "# Save full results to JSON\n",
    "results_json_path = os.path.join(OUTPUT_DIR, 'benchmark_results.json')\n",
    "with open(results_json_path, 'w') as f:\n",
    "\tjson.dump(all_results, f, indent=2)\n",
    "print(f\"\\n✓ Results saved to: {results_json_path}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for model_name in all_results['models']:\n",
    "\trow = {\n",
    "\t\t'model': model_name,\n",
    "\t\t'alphabet_size': all_results['alphabet_sizes'][all_results['models'].index(model_name)]\n",
    "\t}\n",
    "\t\n",
    "\t# K-mer discrimination\n",
    "\tif model_name in all_results['k_mer_discrimination']:\n",
    "\t\tfor k_key, k_data in all_results['k_mer_discrimination'][model_name].items():\n",
    "\t\t\trow[f'kmers_{k_key}_aa_sil'] = k_data['aa_silhouette']\n",
    "\t\t\trow[f'kmers_{k_key}_enc_sil'] = k_data['encoded_silhouette']\n",
    "\t\n",
    "\t# Entropy rates\n",
    "\tif model_name in all_results['entropy_rates']:\n",
    "\t\tfor order_key, order_data in all_results['entropy_rates'][model_name].items():\n",
    "\t\t\trow[f'entropy_{order_key}_aa'] = order_data['aa_entropy_rate']\n",
    "\t\t\trow[f'entropy_{order_key}_enc'] = order_data['encoded_entropy_rate']\n",
    "\t\n",
    "\t# Per-position entropy\n",
    "\tif model_name in all_results['per_position_entropy'] and all_results['per_position_entropy'][model_name]:\n",
    "\t\tpos_data = all_results['per_position_entropy'][model_name]\n",
    "\t\trow['pos_entropy_aa_mean'] = pos_data['aa_mean_entropy']\n",
    "\t\trow['pos_entropy_enc_mean'] = pos_data['encoded_mean_entropy']\n",
    "\t\n",
    "\t# Cross-representation MI\n",
    "\tif model_name in all_results['cross_representation_mi'] and all_results['cross_representation_mi'][model_name]:\n",
    "\t\tmi_data = all_results['cross_representation_mi'][model_name]\n",
    "\t\trow['cross_rep_correlation'] = mi_data.get('proxy_correlation', np.nan)\n",
    "\t\n",
    "\tsummary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save summary CSV\n",
    "summary_csv_path = os.path.join(OUTPUT_DIR, 'benchmark_summary.csv')\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"✓ Summary saved to: {summary_csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706704bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Multi-Model Benchmark Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract alphabet sizes for x-axis\n",
    "alphabet_sizes = summary_df['alphabet_size'].values\n",
    "models = summary_df['model'].values\n",
    "\n",
    "# Color palette\n",
    "colors = sns.color_palette(\"husl\", len(models))\n",
    "\n",
    "# ========== 1. K-mer Discrimination (Silhouette Scores) ==========\n",
    "ax = axes[0, 0]\n",
    "k_cols = [col for col in summary_df.columns if 'kmers_k' in col and '_enc_sil' in col]\n",
    "if k_cols:\n",
    "\tfor i, col in enumerate(k_cols):\n",
    "\t\tk_value = col.split('_')[1]  # Extract k value\n",
    "\t\tax.plot(alphabet_sizes, summary_df[col].values, marker='o', \n",
    "\t\t\t\tlabel=f'{k_value} (encoded)', linewidth=2, markersize=8)\n",
    "\t\n",
    "\tax.set_xlabel('Alphabet Size', fontsize=12)\n",
    "\tax.set_ylabel('Silhouette Score', fontsize=12)\n",
    "\tax.set_title('K-mer Fold Discrimination', fontsize=14, fontweight='bold')\n",
    "\tax.legend(fontsize=10)\n",
    "\tax.grid(True, alpha=0.3)\n",
    "\tax.set_ylim([-0.1, 1.0])\n",
    "\n",
    "# ========== 2. Entropy Rate ==========\n",
    "ax = axes[0, 1]\n",
    "entropy_cols = [col for col in summary_df.columns if 'entropy_order' in col and '_enc' in col]\n",
    "if entropy_cols:\n",
    "\tfor col in entropy_cols:\n",
    "\t\torder_value = col.split('_')[1]  # Extract order\n",
    "\t\tax.plot(alphabet_sizes, summary_df[col].values, marker='s',\n",
    "\t\t\t\tlabel=f'{order_value} (encoded)', linewidth=2, markersize=8)\n",
    "\t\n",
    "\tax.set_xlabel('Alphabet Size', fontsize=12)\n",
    "\tax.set_ylabel('Entropy Rate (bits)', fontsize=12)\n",
    "\tax.set_title('Markov Entropy Rate Estimation', fontsize=14, fontweight='bold')\n",
    "\tax.legend(fontsize=10)\n",
    "\tax.grid(True, alpha=0.3)\n",
    "\n",
    "# ========== 3. Per-Position Entropy ==========\n",
    "ax = axes[1, 0]\n",
    "if 'pos_entropy_enc_mean' in summary_df.columns:\n",
    "\tax.scatter(alphabet_sizes, summary_df['pos_entropy_aa_mean'].values,\n",
    "\t\t\t   s=150, alpha=0.7, label='AA sequences', marker='o', color='blue')\n",
    "\tax.scatter(alphabet_sizes, summary_df['pos_entropy_enc_mean'].values,\n",
    "\t\t\t   s=150, alpha=0.7, label='Encoded sequences', marker='s', color='orange')\n",
    "\t\n",
    "\t# Add error bars if std available\n",
    "\tif 'pos_entropy_enc_std' in summary_df.columns:\n",
    "\t\tax.errorbar(alphabet_sizes, summary_df['pos_entropy_enc_mean'].values,\n",
    "\t\t\t\t   yerr=summary_df['pos_entropy_enc_std'].values, fmt='none', \n",
    "\t\t\t\t   color='orange', alpha=0.5, capsize=5)\n",
    "\t\n",
    "\tax.set_xlabel('Alphabet Size', fontsize=12)\n",
    "\tax.set_ylabel('Mean Positional Entropy (bits)', fontsize=12)\n",
    "\tax.set_title('Per-Position Entropy (MSA)', fontsize=14, fontweight='bold')\n",
    "\tax.legend(fontsize=10)\n",
    "\tax.grid(True, alpha=0.3)\n",
    "\n",
    "# ========== 4. Model Comparison Table ==========\n",
    "ax = axes[1, 1]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create comparison table\n",
    "table_data = []\n",
    "for idx, row in summary_df.iterrows():\n",
    "\ttable_row = [\n",
    "\t\trow['model'][:20],  # Truncate model name\n",
    "\t\tf\"{row['alphabet_size']}\",\n",
    "\t]\n",
    "\t\n",
    "\t# Add best k-mer score\n",
    "\tk_scores = [row[col] for col in summary_df.columns if 'enc_sil' in col and not pd.isna(row[col])]\n",
    "\ttable_row.append(f\"{max(k_scores):.3f}\" if k_scores else \"N/A\")\n",
    "\t\n",
    "\t# Add best entropy rate\n",
    "\tent_scores = [row[col] for col in summary_df.columns if 'entropy_order' in col and '_enc' in col and not pd.isna(row[col])]\n",
    "\ttable_row.append(f\"{np.mean(ent_scores):.3f}\" if ent_scores else \"N/A\")\n",
    "\t\n",
    "\ttable_data.append(table_row)\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "\t\t\t\tcolLabels=['Model', 'Alphabet', 'Best Sil.', 'Avg Entropy'],\n",
    "\t\t\t\tcellLoc='left',\n",
    "\t\t\t\tloc='center',\n",
    "\t\t\t\tcolWidths=[0.4, 0.2, 0.2, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "\ttable[(0, i)].set_facecolor('#4CAF50')\n",
    "\ttable[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(table_data) + 1):\n",
    "\tfor j in range(4):\n",
    "\t\tif i % 2 == 0:\n",
    "\t\t\ttable[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "ax.set_title('Benchmark Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(OUTPUT_DIR, 'benchmark_comparison.png')\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Visualization saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "# Models to benchmark (list of model paths without .pt extension)\n",
    "MODELS = [\n",
    "\t\"/home/dmoi/projects/foldtree2/models/model_10_embeddings_best_encoder\",\n",
    "\t\"/home/dmoi/projects/foldtree2/models/model_20_embeddings_best_encoder\",\n",
    "\t\"/home/dmoi/projects/foldtree2/models/model_30_embeddings_best_encoder\",\n",
    "\t\"/home/dmoi/projects/foldtree2/models/model_40_embeddings_best_encoder\",\n",
    "]\n",
    "\n",
    "# Structure directory (PDB files)\n",
    "STRUCTURES_DIR = \"/home/dmoi/projects/foldtree2/alphafold_benchmark/rhodopsin/structs\"\n",
    "\n",
    "# Output directory for encoded sequences\n",
    "OUTPUT_DIR = \"/home/dmoi/projects/foldtree2/benchmark_results\"\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_PARAMS = {\n",
    "\t'k_mer_sizes': [2, 3, 4, 5],  # k-mer lengths to test\n",
    "\t'markov_orders': [0, 1, 2, 3, 4],  # Markov model orders\n",
    "\t'cv_folds': 5,  # Cross-validation folds\n",
    "\t'alpha_smoothing': 0.1,  # Smoothing parameter\n",
    "\t'occupancy_threshold': 0.7,  # MSA column occupancy threshold\n",
    "\t'reweight_threshold': 0.8,  # Sequence reweighting identity threshold\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  - Models to benchmark: {len(MODELS)}\")\n",
    "print(f\"  - Structure directory: {STRUCTURES_DIR}\")\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA LOADING & ENCODING ====================\n",
    "\n",
    "class ModelBenchmark:\n",
    "\t\"\"\"Container for model encoding results and metadata\"\"\"\n",
    "\tdef __init__(self, model_path: str, device):\n",
    "\t\tself.model_path = model_path\n",
    "\t\tself.model_name = Path(model_path).stem\n",
    "\t\tself.device = device\n",
    "\t\t\n",
    "\t\t# Load encoder\n",
    "\t\tprint(f\"Loading model: {self.model_name}\")\n",
    "\t\tself.encoder = torch.load(model_path + '.pt', map_location=device, weights_only=False)\n",
    "\t\tself.encoder = self.encoder.to(device)\n",
    "\t\tself.encoder.device = device\n",
    "\t\tself.encoder.eval()\n",
    "\t\t\n",
    "\t\t# Extract model metadata\n",
    "\t\tself.num_embeddings = self.encoder.num_embeddings\n",
    "\t\tself.embedding_dim = self.encoder.out_channels\n",
    "\t\t\n",
    "\t\t# Storage for encoded sequences\n",
    "\t\tself.encoded_fasta = None\n",
    "\t\tself.encoded_df = None\n",
    "\t\tself.alphabet = None\n",
    "\t\tself.char_position_map = None\n",
    "\t\t\n",
    "\t\tprint(f\"  ✓ Loaded: {self.num_embeddings} embeddings, dim={self.embedding_dim}\")\n",
    "\t\n",
    "\tdef encode_structures(self, structures_loader, output_dir):\n",
    "\t\t\"\"\"Encode structures using this model\"\"\"\n",
    "\t\toutput_path = os.path.join(output_dir, f\"{self.model_name}_encoded.fasta\")\n",
    "\t\t\n",
    "\t\tprint(f\"  Encoding structures with {self.model_name}...\")\n",
    "\t\tself.encoder.encode_structures_fasta(\n",
    "\t\t\tstructures_loader, \n",
    "\t\t\toutput_path, \n",
    "\t\t\treplace=True\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.encoded_fasta = output_path\n",
    "\t\tself.encoded_df = ft2.load_encoded_fasta(output_path, alphabet=None, replace=False)\n",
    "\t\tself._build_alphabet()\n",
    "\t\t\n",
    "\t\tprint(f\"  ✓ Encoded {len(self.encoded_df)} sequences\")\n",
    "\t\treturn output_path\n",
    "\t\n",
    "\tdef _build_alphabet(self):\n",
    "\t\t\"\"\"Build alphabet from encoded sequences\"\"\"\n",
    "\t\tchar_set = set()\n",
    "\t\tfor seq in self.encoded_df.seq:\n",
    "\t\t\tchar_set = char_set.union(set(seq))\n",
    "\t\tself.alphabet = sorted(list(char_set))\n",
    "\t\tself.char_position_map = {char: i for i, char in enumerate(self.alphabet)}\n",
    "\t\tprint(f\"  ✓ Alphabet size: {len(self.alphabet)} characters\")\n",
    "\n",
    "def load_structures(structures_dir: str, converter: PDB2PyG):\n",
    "\t\"\"\"Load and convert PDB structures to PyG format\"\"\"\n",
    "\tpdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "\tprint(f\"\\nFound {len(pdb_files)} PDB files in {structures_dir}\")\n",
    "\t\n",
    "\tif len(pdb_files) == 0:\n",
    "\t\traise ValueError(f\"No PDB files found in {structures_dir}\")\n",
    "\t\n",
    "\tdef struct_generator():\n",
    "\t\tfor pdb_file in pdb_files:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdata = converter.struct2pyg(pdb_file)\n",
    "\t\t\t\tif data:\n",
    "\t\t\t\t\tyield data\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"  Warning: Failed to convert {pdb_file}: {e}\")\n",
    "\t\t\t\tcontinue\n",
    "\t\n",
    "\treturn struct_generator()\n",
    "\n",
    "def extract_aa_sequences(structures_dir: str, output_dir: str):\n",
    "\t\"\"\"Extract amino acid sequences from PDB files\"\"\"\n",
    "\tparser = PDBParser(QUIET=True)\n",
    "\taa_dict = {\n",
    "\t\t'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "\t\t'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "\t\t'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "\t\t'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "\t}\n",
    "\t\n",
    "\tpdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "\tsequences = {}\n",
    "\t\n",
    "\tprint(f\"\\nExtracting amino acid sequences from {len(pdb_files)} structures...\")\n",
    "\tfor pdb_file in pdb_files:\n",
    "\t\tstructure_id = Path(pdb_file).stem\n",
    "\t\ttry:\n",
    "\t\t\tstructure = parser.get_structure(structure_id, pdb_file)\n",
    "\t\t\tseq = \"\"\n",
    "\t\t\tfor model in structure:\n",
    "\t\t\t\tfor chain in model:\n",
    "\t\t\t\t\tfor residue in chain:\n",
    "\t\t\t\t\t\tif residue.get_resname() in aa_dict:\n",
    "\t\t\t\t\t\t\tseq += aa_dict[residue.get_resname()]\n",
    "\t\t\t\t\tbreak  # Only first chain\n",
    "\t\t\t\tbreak  # Only first model\n",
    "\t\t\t\n",
    "\t\t\tif seq:\n",
    "\t\t\t\tsequences[structure_id] = seq\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"  Warning: Failed to extract sequence from {pdb_file}: {e}\")\n",
    "\t\n",
    "\t# Write to FASTA\n",
    "\toutput_path = os.path.join(output_dir, \"aa_sequences.fasta\")\n",
    "\twith open(output_path, 'w') as f:\n",
    "\t\tfor struct_id, seq in sequences.items():\n",
    "\t\t\tf.write(f\">{struct_id}\\n{seq}\\n\")\n",
    "\t\n",
    "\tprint(f\"  ✓ Extracted {len(sequences)} AA sequences\")\n",
    "\treturn output_path, sequences\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize converter\n",
    "converter = PDB2PyG(aapropcsv='/home/dmoi/projects/foldtree2/foldtree2/config/aaindex1.csv')\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note. try both alignment using foldmason, ft2 and regular mafft...\n",
    "# the aligner can also be an argument for a particular character model\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bcbf5",
   "metadata": {},
   "source": [
    "## K-mer Frequency Analysis for Fold Discrimination\n",
    "\n",
    "This experiment analyzes the discriminative power of FoldTree2 (DSR) versus amino acid representations by examining k-mer frequency distributions. The goal is to determine which alphabet better distinguishes between different protein folds.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- **K-mer extraction**: Compute frequency distributions of subsequences of length k for both AA and DSR sequences\n",
    "- **Within-family distances**: Calculate Jensen-Shannon divergences between k-mer distributions of sequences within the same fold family\n",
    "- **Between-family distances**: Measure k-mer distribution differences across distinct fold families\n",
    "- **Discrimination analysis**: Compare the separation between within-family (similar folds) and between-family (different folds) distance distributions\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. **Fold specificity**: Does the FoldTree2 alphabet capture fold-specific sequence patterns more effectively than amino acid sequences?\n",
    "2. **Optimal k-mer length**: What subsequence length provides the best discrimination for each representation?\n",
    "3. **Distribution separation**: Which alphabet shows clearer separation between intra-fold similarity and inter-fold dissimilarity?\n",
    "\n",
    "The analysis will reveal whether structural alphabets provide enhanced discriminative power for protein fold classification compared to traditional sequence-based representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def kmer_freqs(seq, k, alpha_map, A):\n",
    "\tidx = [alpha_map[c] for c in seq if c in alpha_map]\n",
    "\tif len(idx) < k: return np.ones(A**k)/(A**k)\n",
    "\tcounts = np.zeros(A**k)\n",
    "\tbase = A**np.arange(k)[::-1]\n",
    "\tfor t in range(len(idx)-k+1):\n",
    "\t\tcode = 0\n",
    "\t\tfor j in range(k):\n",
    "\t\t\tcode = code*A + idx[t+j]\n",
    "\t\tcounts[code]+=1\n",
    "\tp = counts + 1e-9\n",
    "\tp /= p.sum()\n",
    "\treturn p\n",
    "\n",
    "def jsd(p, q):\n",
    "\tm = 0.5*(p+q)\n",
    "\tdef KL(a,b): \n",
    "\t\tmask = (a>0)\n",
    "\t\treturn (a[mask]*np.log2(a[mask]/b[mask])).sum()\n",
    "\treturn 0.5*(KL(p,m)+KL(q,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afd397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# ---------------------------- k-mer utils ------------------------------\n",
    "\n",
    "def build_alpha_index(alphabet: str) -> Dict[str, int]:\n",
    "\treturn {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "def kmers_counts(seq: str, k: int, alpha_idx: Dict[str,int], A: int) -> np.ndarray:\n",
    "\t\"\"\"Return counts vector of length A^k for overlapping k-mers in seq (skip k-mer if any unseen char).\"\"\"\n",
    "\tL = len(seq)\n",
    "\tif L < k or k == 0:\n",
    "\t\treturn np.zeros(A**max(k,1), dtype=np.float64)\n",
    "\tv = np.zeros(A**k, dtype=np.float64)\n",
    "\tcode = -1\n",
    "\tfor i, ch in enumerate(seq):\n",
    "\t\tif ch not in alpha_idx:\n",
    "\t\t\tcode = -1\n",
    "\t\telse:\n",
    "\t\t\tx = alpha_idx[ch]\n",
    "\t\t\tif code == -1:\n",
    "\t\t\t\tif i >= k-1:\n",
    "\t\t\t\t\tok = True\n",
    "\t\t\t\t\tcode_tmp = 0\n",
    "\t\t\t\t\tfor j in range(i-k+1, i+1):\n",
    "\t\t\t\t\t\tc2 = seq[j]\n",
    "\t\t\t\t\t\tif c2 not in alpha_idx:\n",
    "\t\t\t\t\t\t\tok = False; break\n",
    "\t\t\t\t\t\tcode_tmp = code_tmp * A + alpha_idx[c2]\n",
    "\t\t\t\t\tif ok:\n",
    "\t\t\t\t\t\tcode = code_tmp\n",
    "\t\t\t\t\t\tv[code] += 1.0\n",
    "\t\t\telse:\n",
    "\t\t\t\tcode = (code % (A**(k-1))) * A + x\n",
    "\t\t\t\tv[code] += 1.0\n",
    "\treturn v\n",
    "\n",
    "def kmer_prob(seq: str, k: int, alphabet: str, pseudocount: float = 1e-9) -> np.ndarray:\n",
    "\tA = len(alphabet)\n",
    "\tidx = build_alpha_index(alphabet)\n",
    "\tc = kmers_counts(seq, k, idx, A)\n",
    "\ttotal = c.sum()\n",
    "\tif total == 0:\n",
    "\t\t# no valid k-mers: return uniform tiny distribution\n",
    "\t\tp = np.ones(A**k, dtype=np.float64)\n",
    "\t\tp /= p.sum()\n",
    "\t\treturn p\n",
    "\tp = (c + pseudocount) / (total + pseudocount * c.shape[0])\n",
    "\treturn p\n",
    "\n",
    "def build_feature_matrix(fasta: List[Tuple[str,str]], alphabet: str, k: int, pseudocount: float):\n",
    "\tids = [name for name,_ in fasta]\n",
    "\tseqs = [seq.upper() for _,seq in fasta]\n",
    "\tP = np.vstack([kmer_prob(s, k, alphabet, pseudocount=pseudocount) for s in seqs])\n",
    "\treturn ids, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- KMeans + eval -----------------------------\n",
    "\n",
    "def kmeans_cluster(P: np.ndarray, K: int, n_init: int = 20, max_iter: int = 300, random_state: int = 0):\n",
    "\t# KMeans on probability vectors (Euclidean). For probability geometry, you can also sqrt-transform (Hellinger).\n",
    "\tmodel = KMeans(n_clusters=K, n_init=n_init, max_iter=max_iter, random_state=random_state)\n",
    "\tlabels = model.fit_predict(P)\n",
    "\treturn labels, model\n",
    "\n",
    "def run_rep(\n",
    "\tfasta_path: str, labels_map: Dict[str,str],\n",
    "\talphabet: str, k: int, pseudocount: float,\n",
    "\ttarget_clusters: int, random_state: int, n_init: int, max_iter: int\n",
    "):\n",
    "\tfasta = read_fasta(fasta_path)\n",
    "\tids, P = build_feature_matrix(fasta, alphabet, k, pseudocount)\n",
    "\n",
    "\t# align labels and filter\n",
    "\ty = []\n",
    "\tkeep = []\n",
    "\tfor i, sid in enumerate(ids):\n",
    "\t\tif sid in labels_map:\n",
    "\t\t\ty.append(labels_map[sid])\n",
    "\t\t\tkeep.append(i)\n",
    "\tif not keep:\n",
    "\t\traise ValueError(\"No IDs from FASTA matched labels.tsv\")\n",
    "\tids = [ids[i] for i in keep]\n",
    "\tP = P[keep]\n",
    "\ty = np.array(y)\n",
    "\n",
    "\tuniq = {lab:i for i,lab in enumerate(sorted(set(y)))}\n",
    "\ty_int = np.array([uniq[lab] for lab in y], dtype=int)\n",
    "\tK = target_clusters or len(uniq)\n",
    "\n",
    "\t# KMeans\n",
    "\tlabels_pred, model = kmeans_cluster(\n",
    "\t\tP, K=K, n_init=n_init, max_iter=max_iter, random_state=random_state\n",
    "\t)\n",
    "\n",
    "\tari = adjusted_rand_score(y_int, labels_pred)\n",
    "\tnmi = normalized_mutual_info_score(y_int, labels_pred)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"ids\": ids,\n",
    "\t\t\"P\": P,\n",
    "\t\t\"y_int\": y_int,\n",
    "\t\t\"y_str\": y,\n",
    "\t\t\"labels_pred\": labels_pred,\n",
    "\t\t\"K\": K,\n",
    "\t\t\"ari\": ari,\n",
    "\t\t\"nmi\": nmi,\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957a560",
   "metadata": {},
   "source": [
    "## Entropy Rate Estimation with k-order Markov Models\n",
    "\n",
    "This experiment estimates the global entropy rate using k-order Markov models across multiple protein families. The analysis compares two different sequence representations:\n",
    "\n",
    "- **AA sequences**: Traditional amino acid sequences using the 20-letter alphabet\n",
    "- **DSR sequences**: Discrete structure representation using a K-token alphabet\n",
    "\n",
    "### Methodology\n",
    "\n",
    "The experiment uses a **backoff smoothing approach** with cross-validation to estimate entropy rates:\n",
    "\n",
    "1. **k-order Markov modeling**: Models conditional probabilities P(x|context) where context length = k\n",
    "2. **Backoff smoothing**: Handles sparse data by interpolating between different order models (k-gram → (k-1)-gram → ... → unigram)\n",
    "3. **5-fold cross-validation**: Splits sequences by family to avoid overfitting\n",
    "4. **Additive smoothing**: Regularizes maximum likelihood estimates with parameter α\n",
    "\n",
    "### Aggregation Strategy\n",
    "\n",
    "Results are aggregated at two levels:\n",
    "- **Macro-averaging**: Equal weight per family (family-centric view)\n",
    "- **Micro-averaging**: Weight by total tokens (sequence-centric view)\n",
    "\n",
    "This allows comparison of structural vs. sequence-based entropy rates across different context lengths (k=0,1,2,3,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare your data as dict: family -> list of sequences (strings)\n",
    "families_AA  = {\"PF00001\": [\"MKT...\", \"MSS...\"], \"PF00002\": [...], ...}\n",
    "families_DSR = {\"PF00001\": [\"QAB...\", \"QAA...\"], \"PF00002\": [...], ...}\n",
    "\n",
    "# 2) Define alphabets\n",
    "alphabet_AA  = list(\"ACDEFGHIKLMNPQRSTVWY\")       # or include 'X' if you keep it\n",
    "alphabet_DSR = [chr(i) for i in range(65, 65+K)]   # e.g., 'A'.. for K tokens, or your actual token set\n",
    "\n",
    "# 3) Run\n",
    "ks = (0,1,2,3,4)\n",
    "aa_res, aa_agg   = run_entropy_over_families(families_AA,  alphabet_AA,  k_values=ks, alpha=0.1, delta=None, folds=5)\n",
    "dsr_res, dsr_agg = run_entropy_over_families(families_DSR, alphabet_DSR, k_values=ks, alpha=0.1, delta=None, folds=5)\n",
    "\n",
    "# 4) Compare and plot:\n",
    "#   - aa_agg[k]['macro'] vs dsr_agg[k]['macro']\n",
    "#   - per-family deltas: {fam: dsr_res[k][fam]-aa_res[k][fam]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random, math\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ---------- utils\n",
    "def tokenize(seq: str, alpha_map: Dict[str,int]) -> List[int]:\n",
    "\treturn [alpha_map[c] for c in seq if c in alpha_map]\n",
    "\n",
    "def k_context(stream: List[int], k: int):\n",
    "\t# yields (context_tuple, symbol) skipping first k\n",
    "\tif k == 0:\n",
    "\t\tfor x in stream: yield (), x\n",
    "\telse:\n",
    "\t\tctx = []\n",
    "\t\tfor x in stream:\n",
    "\t\t\tctx.append(x)\n",
    "\t\t\tif len(ctx) > k:\n",
    "\t\t\t\tyield tuple(ctx[-k-1:-1]), x\n",
    "\n",
    "def add_counts(counts, stream: List[int], k: int):\n",
    "\tfor ctx, x in k_context(stream, k):\n",
    "\t\tcounts[ctx][x] += 1\n",
    "\n",
    "def build_counts(seqs: List[List[int]], k: int, A: int):\n",
    "\tcounts = defaultdict(lambda: Counter())\n",
    "\ttotal_tokens = 0\n",
    "\tfor s in seqs:\n",
    "\t\ttotal_tokens += max(0, len(s)-k)\n",
    "\t\tadd_counts(counts, s, k)\n",
    "\treturn counts, total_tokens\n",
    "\n",
    "# ---------- smoothed conditional with simple backoff\n",
    "class BackoffKModel:\n",
    "\tdef __init__(self, counts_k_list, A: int, alpha=0.1, delta=None):\n",
    "\t\t\"\"\"\n",
    "\t\tcounts_k_list: list where idx j holds counts for order j (0..k)\n",
    "\t\tA: alphabet size\n",
    "\t\talpha: additive smoothing for MLE\n",
    "\t\tdelta: backoff strength; if None, set to A (alphabet size)\n",
    "\t\t\"\"\"\n",
    "\t\tself.counts = counts_k_list\n",
    "\t\tself.A = A\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.delta = delta if delta is not None else A\n",
    "\n",
    "\tdef p_cond(self, ctx: Tuple[int,...], x: int) -> float:\n",
    "\t\tk = len(ctx)\n",
    "\t\treturn self._p_k(k, ctx, x)\n",
    "\n",
    "\tdef _p_k(self, k: int, ctx: Tuple[int,...], x: int) -> float:\n",
    "\t\t# base: unigram (order 0)\n",
    "\t\tif k == 0:\n",
    "\t\t\tcnts0 = self.counts[0][()]\n",
    "\t\t\tnum = cnts0.get(x, 0) + self.alpha\n",
    "\t\t\tden = sum(cnts0.values()) + self.alpha * self.A\n",
    "\t\t\treturn num / den\n",
    "\n",
    "\t\t# order-k MLE with smoothing\n",
    "\t\tcnts_k = self.counts[k][ctx]\n",
    "\t\tnum = cnts_k.get(x, 0) + self.alpha\n",
    "\t\tden = sum(cnts_k.values()) + self.alpha * self.A\n",
    "\t\tp_mle = num / den\n",
    "\n",
    "\t\t# backoff weight\n",
    "\t\tgamma = self.delta / (self.delta + sum(cnts_k.values()))\n",
    "\t\t# suffix context\n",
    "\t\tsuffix = ctx[1:]\n",
    "\t\treturn (1 - gamma) * p_mle + gamma * self._p_k(k-1, suffix, x)\n",
    "\n",
    "# ---------- cross-entropy (held-out)\n",
    "def cross_entropy_bits(model: BackoffKModel, seqs: List[List[int]], k: int) -> float:\n",
    "\ttot_logloss = 0.0\n",
    "\ttot_tokens = 0\n",
    "\tfor s in seqs:\n",
    "\t\t# iterate tokens with contexts; boundaries reset by per-seq processing\n",
    "\t\tfor ctx, x in k_context(s, k):\n",
    "\t\t\tp = model.p_cond(ctx, x)\n",
    "\t\t\ttot_logloss += -math.log2(max(p, 1e-300))\n",
    "\t\t\ttot_tokens += 1\n",
    "\treturn tot_logloss / max(1, tot_tokens)\n",
    "\n",
    "# ---------- 5-fold CV by sequence\n",
    "def entropy_rate_cv(seqs: List[List[int]], A: int, k: int, alpha=0.1, delta=None, folds=5, seed=0):\n",
    "\trandom.Random(seed).shuffle(seqs)\n",
    "\tif len(seqs) < folds: folds = max(2, len(seqs))\n",
    "\tfold_size = math.ceil(len(seqs)/folds)\n",
    "\tlosses = []\n",
    "\tfor f in range(folds):\n",
    "\t\ttest = seqs[f*fold_size:(f+1)*fold_size]\n",
    "\t\ttrain = seqs[:f*fold_size] + seqs[(f+1)*fold_size:]\n",
    "\t\t# build counts for orders 0..k\n",
    "\t\tcounts_k_list = []\n",
    "\t\tfor j in range(k+1):\n",
    "\t\t\tcounts_j, _ = build_counts(train, j, A)\n",
    "\t\t\tcounts_k_list.append(counts_j)\n",
    "\t\tmodel = BackoffKModel(counts_k_list, A, alpha=alpha, delta=delta)\n",
    "\t\tH = cross_entropy_bits(model, test, k)\n",
    "\t\tlosses.append((H, sum(max(0, len(s)-k) for s in test)))\n",
    "\t# micro-average over folds\n",
    "\tnum = sum(H*n for H, n in losses)\n",
    "\tden = sum(n for _, n in losses) or 1\n",
    "\treturn num/den\n",
    "\n",
    "# ---------- run over families and ks\n",
    "def run_entropy_over_families(\n",
    "\tfamilies: Dict[str, List[str]],\n",
    "\talphabet: List[str],\n",
    "\tk_values=(0,1,2,3,4),\n",
    "\talpha=0.1, delta=None, folds=5, seed=0\n",
    "):\n",
    "\talpha_map = {c:i for i,c in enumerate(alphabet)}\n",
    "\tA = len(alphabet)\n",
    "\n",
    "\t# tokenize\n",
    "\tfam_tok = {\n",
    "\t\tfam: [tokenize(s, alpha_map) for s in seqs if len(tokenize(s, alpha_map))>0]\n",
    "\t\tfor fam, seqs in families.items()\n",
    "\t}\n",
    "\n",
    "\tresults = {k:{} for k in k_values}\n",
    "\tsizes   = {fam: sum(max(0,len(s)-max(k_values)) for s in seqs) for fam, seqs in fam_tok.items()}\n",
    "\n",
    "\tfor k in k_values:\n",
    "\t\tfam_H = {}\n",
    "\t\tfor fam, seqs in fam_tok.items():\n",
    "\t\t\tif len(seqs)==0: continue\n",
    "\t\t\tHk = entropy_rate_cv(seqs, A, k, alpha=alpha, delta=delta, folds=folds, seed=seed)\n",
    "\t\t\tfam_H[fam] = Hk\n",
    "\t\tresults[k] = fam_H\n",
    "\n",
    "\t# aggregates\n",
    "\taggregates = {}\n",
    "\tfor k in k_values:\n",
    "\t\tfam_H = results[k]\n",
    "\t\tfam_list = list(fam_H.items())\n",
    "\t\tif not fam_list:\n",
    "\t\t\taggregates[k] = dict(macro=None, micro=None, n_families=0)\n",
    "\t\t\tcontinue\n",
    "\t\tmacro = sum(h for _,h in fam_list)/len(fam_list)\n",
    "\t\t# micro weight by total tokens (approximate using lengths at this k)\n",
    "\t\tweights = {fam: sum(max(0, len(s)-k) for s in fam_tok[fam]) for fam,_ in fam_list}\n",
    "\t\tnum = sum(fam_H[fam]*weights[fam] for fam,_ in fam_list)\n",
    "\t\tden = sum(weights.values()) or 1\n",
    "\t\tmicro = num/den\n",
    "\t\taggregates[k] = dict(macro=macro, micro=micro, n_families=len(fam_list))\n",
    "\treturn results, aggregates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de83d6",
   "metadata": {},
   "source": [
    "## Cross-Representation Information Analysis\n",
    "\n",
    "This experiment investigates the **mutual information** between amino acid (AA) and discrete structure representation (DSR) sequences by training probabilistic mappings in both directions using local sequence windows.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- **Windowed prediction**: Train neural networks to predict target tokens from source context windows (e.g., predict AA from 7-token DSR window)\n",
    "- **Bidirectional mapping**: Learn both DSR→AA and AA→DSR predictors to estimate cross-entropies\n",
    "- **Information bounds**: Derive mutual information lower bounds using H(target) - H(target|source_window)\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. **Complementarity**: How much structural information is captured in DSR that's not present in AA sequences?\n",
    "2. **Redundancy**: What fraction of sequence information is already encoded in structural representations?\n",
    "3. **Context dependence**: How does prediction accuracy vary with window size and MSA position?\n",
    "\n",
    "The analysis will reveal whether the two representations contain overlapping or complementary information about protein structure and function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Cross-representation information (Strategy 3)\n",
    "- Learn small probabilistic mappers DSR→AA and AA→DSR using windowed tokens\n",
    "- Estimate per-position and global H(AA), H(DSR), and conditional cross-entropies\n",
    "- Derive MI lower bounds: I_hat = H - H_hat(cond)\n",
    "\"\"\"\n",
    "\n",
    "import argparse, math, random\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------- FASTA utils -----------------------------\n",
    "\n",
    "def read_fasta(path: str) -> List[Tuple[str,str]]:\n",
    "\tseqs = []\n",
    "\tname = None\n",
    "\tbuf = []\n",
    "\twith open(path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif not line: \n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif line.startswith('>'):\n",
    "\t\t\t\tif name is not None:\n",
    "\t\t\t\t\tseqs.append((name, ''.join(buf)))\n",
    "\t\t\t\tname = line[1:].strip()\n",
    "\t\t\t\tbuf = []\n",
    "\t\t\telse:\n",
    "\t\t\t\tbuf.append(line)\n",
    "\tif name is not None:\n",
    "\t\tseqs.append((name, ''.join(buf)))\n",
    "\treturn seqs\n",
    "\n",
    "def ensure_same_shape(msa1: List[str], msa2: List[str]):\n",
    "\tassert len(msa1) == len(msa2), \"MSA row count mismatch between AA and DSR.\"\n",
    "\tL1 = len(msa1[0])\n",
    "\tfor s in msa1:\n",
    "\t\tassert len(s) == L1, \"AA MSA rows must have equal length.\"\n",
    "\tfor s in msa2:\n",
    "\t\tassert len(s) == L1, \"DSR MSA must have same number of columns as AA MSA.\"\n",
    "\n",
    "# ------------------------- Sequence reweighting ------------------------\n",
    "\n",
    "def seq_identity(a: str, b: str) -> float:\n",
    "\tmatches = 0\n",
    "\tcomps = 0\n",
    "\tfor x, y in zip(a, b):\n",
    "\t\tif x == '-' or y == '-':\n",
    "\t\t\tcontinue\n",
    "\t\tcomps += 1\n",
    "\t\tif x == y:\n",
    "\t\t\tmatches += 1\n",
    "\tif comps == 0: return 0.0\n",
    "\treturn matches / comps\n",
    "\n",
    "def reweight_sequences(msa: List[str], thresh: float = 0.8) -> np.ndarray:\n",
    "\tn = len(msa)\n",
    "\tw = np.zeros(n, dtype=float)\n",
    "\tfor i in range(n):\n",
    "\t\tc = 0\n",
    "\t\tfor j in range(n):\n",
    "\t\t\tif seq_identity(msa[i], msa[j]) >= thresh:\n",
    "\t\t\t\tc += 1\n",
    "\t\tw[i] = 1.0 / max(1, c)\n",
    "\tif w.sum() > 0:\n",
    "\t\tw *= (n / w.sum())\n",
    "\treturn w\n",
    "\n",
    "# ----------------------- Entropy (empirical) ---------------------------\n",
    "\n",
    "def shannon_entropy_from_counts(counts: np.ndarray, pseudocount: float = 0.0) -> float:\n",
    "\tA = counts.shape[0]\n",
    "\ttotal = counts.sum()\n",
    "\tp = (counts + pseudocount) / (total + pseudocount * A)\n",
    "\tp = p[p > 0]\n",
    "\treturn float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def per_position_entropy(msa: List[str], alphabet: List[str], weights: np.ndarray,\n",
    "\t\t\t\t\t\t occupancy_threshold: float = 0.7,\n",
    "\t\t\t\t\t\t pseudocount: float = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\tA = len(alphabet)\n",
    "\tif pseudocount is None:\n",
    "\t\tpseudocount = 1.0 / A\n",
    "\talpha_index = {c:i for i,c in enumerate(alphabet)}\n",
    "\tL = len(msa[0])\n",
    "\tent = np.full(L, np.nan, dtype=float)\n",
    "\tmask = np.zeros(L, dtype=bool)\n",
    "\n",
    "\tocc = np.zeros(L, dtype=float)\n",
    "\tfor i, seq in enumerate(msa):\n",
    "\t\tg = np.fromiter((c != '-' for c in seq), dtype=bool, count=L)\n",
    "\t\tocc += weights[i] * g.astype(float)\n",
    "\tocc /= max(1e-9, weights.sum())\n",
    "\n",
    "\tfor t in range(L):\n",
    "\t\tif occ[t] < occupancy_threshold: continue\n",
    "\t\tcounts = np.zeros(A, dtype=float)\n",
    "\t\tfor i, seq in enumerate(msa):\n",
    "\t\t\tch = seq[t]\n",
    "\t\t\tif ch == '-' or ch not in alpha_index: continue\n",
    "\t\t\tcounts[alpha_index[ch]] += weights[i]\n",
    "\t\tif counts.sum() <= 0: continue\n",
    "\t\tent[t] = shannon_entropy_from_counts(counts, pseudocount=pseudocount)\n",
    "\t\tmask[t] = True\n",
    "\treturn ent, mask\n",
    "\n",
    "# ------------------------ Windowed samples -----------------------------\n",
    "\n",
    "def build_window_samples(\n",
    "\tsrc_msa: List[str], tgt_msa: List[str], weights: np.ndarray,\n",
    "\tsrc_alpha: List[str], tgt_alpha: List[str],\n",
    "\tpos_mask: np.ndarray, win: int\n",
    "):\n",
    "\t\"\"\"\n",
    "\tBuild samples to predict target token at t from a src window around t.\n",
    "\t- Requires no gaps in the src window or target at t.\n",
    "\t- pos_mask picks columns to consider (e.g., occupancy intersection).\n",
    "\tReturns X (one-hot per-window), y (int labels), w (sample weights), pos_idx (column indices).\n",
    "\t\"\"\"\n",
    "\tassert win % 2 == 1, \"Window must be odd size.\"\n",
    "\thalf = win // 2\n",
    "\tA_src, A_tgt = len(src_alpha), len(tgt_alpha)\n",
    "\tsrc_idx = {c:i for i,c in enumerate(src_alpha)}\n",
    "\ttgt_idx = {c:i for i,c in enumerate(tgt_alpha)}\n",
    "\n",
    "\tL = len(src_msa[0])\n",
    "\tn = len(src_msa)\n",
    "\n",
    "\tfeats = []\n",
    "\tlabels = []\n",
    "\tsw = []\n",
    "\tpos_idx = []\n",
    "\n",
    "\t# precompute valid (non-gap and in alphabet) masks\n",
    "\tsrc_valid = np.array([[ (c!='-' and c in src_idx) for c in row] for row in src_msa], dtype=bool)\n",
    "\ttgt_valid = np.array([[ (c!='-' and c in tgt_idx) for c in row] for row in tgt_msa], dtype=bool)\n",
    "\n",
    "\tfor i in range(n):\n",
    "\t\tw_i = weights[i]\n",
    "\t\tsrc_row = src_msa[i]\n",
    "\t\ttgt_row = tgt_msa[i]\n",
    "\t\tfor t in range(half, L-half):\n",
    "\t\t\tif not pos_mask[t]: continue\n",
    "\t\t\tif not tgt_valid[i, t]: continue\n",
    "\t\t\t# window must be fully valid in src\n",
    "\t\t\tif not src_valid[i, t-half:t+half+1].all(): continue\n",
    "\n",
    "\t\t\t# one-hot encode window: concat per-position one-hots\n",
    "\t\t\tv = np.zeros((win, A_src), dtype=np.float32)\n",
    "\t\t\tfor j, col in enumerate(range(t-half, t+half+1)):\n",
    "\t\t\t\ts = src_row[col]\n",
    "\t\t\t\tv[j, src_idx[s]] = 1.0\n",
    "\t\t\tfeats.append(v.reshape(-1))  # (win*A_src,)\n",
    "\t\t\tlabels.append(tgt_idx[tgt_row[t]])\n",
    "\t\t\tsw.append(w_i)\n",
    "\t\t\tpos_idx.append(t)\n",
    "\n",
    "\tif len(feats) == 0:\n",
    "\t\treturn (np.zeros((0, win*A_src), dtype=np.float32),\n",
    "\t\t\t\tnp.zeros((0,), dtype=np.int64),\n",
    "\t\t\t\tnp.zeros((0,), dtype=np.float32),\n",
    "\t\t\t\tnp.zeros((0,), dtype=np.int32))\n",
    "\tX = np.stack(feats, axis=0)\n",
    "\ty = np.array(labels, dtype=np.int64)\n",
    "\tw = np.array(sw, dtype=np.float32)\n",
    "\tpos_idx = np.array(pos_idx, dtype=np.int32)\n",
    "\treturn X, y, w, pos_idx\n",
    "\n",
    "# --------------------------- Torch model --------------------------------\n",
    "\n",
    "class SoftmaxLinear(nn.Module):\n",
    "\tdef __init__(self, D_in: int, C_out: int, bias=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.lin = nn.Linear(D_in, C_out, bias=bias)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.lin(x)  # logits\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "\tdef __init__(self, X, y, w):\n",
    "\t\tself.X = torch.from_numpy(X)\n",
    "\t\tself.y = torch.from_numpy(y)\n",
    "\t\tself.w = torch.from_numpy(w)\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.X.shape[0]\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "def split_by_sequence_indices(n_seq: int, seed=0, ratios=(0.7, 0.15, 0.15)):\n",
    "\tidx = list(range(n_seq))\n",
    "\trandom.Random(seed).shuffle(idx)\n",
    "\tn_train = int(ratios[0]*n_seq)\n",
    "\tn_val   = int(ratios[1]*n_seq)\n",
    "\ttrain_ids = set(idx[:n_train])\n",
    "\tval_ids   = set(idx[n_train:n_train+n_val])\n",
    "\ttest_ids  = set(idx[n_train+n_val:])\n",
    "\treturn train_ids, val_ids, test_ids\n",
    "\n",
    "def mask_samples_by_seqpos(seq_pos_of_sample: List[int], seq_ids_set: set):\n",
    "\treturn np.array([sp in seq_ids_set for sp in seq_pos_of_sample], dtype=bool)\n",
    "\n",
    "def train_softmax(\n",
    "\tX_train, y_train, w_train,\n",
    "\tX_val,   y_val,   w_val,\n",
    "\tD_in, C_out, lr=1e-2, epochs=20, bs=4096, weight_decay=1e-4, device=\"cpu\"\n",
    "):\n",
    "\tmodel = SoftmaxLinear(D_in, C_out).to(device)\n",
    "\topt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\tds_tr = NumpyDataset(X_train, y_train, w_train)\n",
    "\tdl_tr = DataLoader(ds_tr, batch_size=bs, shuffle=True, drop_last=False)\n",
    "\n",
    "\tdef eval_ce(X, y, w):\n",
    "\t\tif X.shape[0] == 0: return float('nan')\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tX_t = torch.from_numpy(X).to(device)\n",
    "\t\t\ty_t = torch.from_numpy(y).to(device)\n",
    "\t\t\tw_t = torch.from_numpy(w).to(device)\n",
    "\t\t\tlogits = model(X_t)\n",
    "\t\t\tce = nn.functional.cross_entropy(logits, y_t, reduction='none')\n",
    "\t\t\treturn float((ce * w_t).sum().item() / max(1e-9, w_t.sum().item()))\n",
    "\n",
    "\tbest_val = float('inf')\n",
    "\tbest_state = None\n",
    "\tpatience, bad = 4, 0\n",
    "\n",
    "\tfor ep in range(epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor xb, yb, wb in dl_tr:\n",
    "\t\t\txb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "\t\t\tlogits = model(xb)\n",
    "\t\t\tce = nn.functional.cross_entropy(logits, yb, reduction='none')\n",
    "\t\t\tloss = (ce * wb).sum() / (wb.sum() + 1e-9)\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\n",
    "\t\tval_ce = eval_ce(X_val, y_val, w_val)\n",
    "\t\tif val_ce < best_val - 1e-5:\n",
    "\t\t\tbest_val = val_ce\n",
    "\t\t\tbest_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\t\t\tbad = 0\n",
    "\t\telse:\n",
    "\t\t\tbad += 1\n",
    "\t\t\tif bad >= patience:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tif best_state is not None:\n",
    "\t\tmodel.load_state_dict(best_state)\n",
    "\treturn model\n",
    "\n",
    "def predict_log_probs(model: nn.Module, X: np.ndarray, bs=8192, device=\"cpu\") -> np.ndarray:\n",
    "\tif X.shape[0] == 0: return np.zeros((0, model.lin.out_features), dtype=np.float32)\n",
    "\tmodel.eval()\n",
    "\tout = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i in range(0, X.shape[0], bs):\n",
    "\t\t\txb = torch.from_numpy(X[i:i+bs]).to(device)\n",
    "\t\t\tlogits = model(xb)\n",
    "\t\t\tlogp = nn.functional.log_softmax(logits, dim=-1)\n",
    "\t\t\tout.append(logp.detach().cpu().numpy())\n",
    "\treturn np.vstack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5949766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CROSS-FAMILY RESULTS SUMMARY ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-FAMILY BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display results for each family\n",
    "for family_name in all_results.keys():\n",
    "\tprint(f\"\\n  - {family_name}: {all_results[family_name]['n_structures']} structures\")\n",
    "\n",
    "# Save comprehensive results to JSON\n",
    "import json\n",
    "results_path = os.path.join(OUTPUT_DIR, 'all_families_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "\t# Convert numpy types to native Python types for JSON serialization\n",
    "\tdef convert_to_serializable(obj):\n",
    "\t\tif isinstance(obj, np.integer):\n",
    "\t\t\treturn int(obj)\n",
    "\t\telif isinstance(obj, np.floating):\n",
    "\t\t\treturn float(obj)\n",
    "\t\telif isinstance(obj, np.ndarray):\n",
    "\t\t\treturn obj.tolist()\n",
    "\t\telif isinstance(obj, dict):\n",
    "\t\t\treturn {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "\t\telif isinstance(obj, list):\n",
    "\t\t\treturn [convert_to_serializable(item) for item in obj]\n",
    "\t\telse:\n",
    "\t\t\treturn obj\n",
    "\t\n",
    "\tserializable_results = convert_to_serializable(all_results)\n",
    "\tjson.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {results_path}\")\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "\n",
    "# Collect all results into structured format\n",
    "summary_data = []\n",
    "\n",
    "for family_name, family_results in all_results.items():\n",
    "\tfor model_name in family_results['models'] + ['AA']:\n",
    "\t\trow = {\n",
    "\t\t\t'Family': family_name,\n",
    "\t\t\t'Model': model_name,\n",
    "\t\t\t'N_Structures': family_results['n_structures']\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# K-mer discrimination scores\n",
    "\t\tif model_name in family_results['k_mer_discrimination']:\n",
    "\t\t\tkmer_results = family_results['k_mer_discrimination'][model_name]\n",
    "\t\t\tfor k_val in ['k=1', 'k=2', 'k=3', 'k=4']:\n",
    "\t\t\t\tif k_val in kmer_results:\n",
    "\t\t\t\t\trow[f'KMer_{k_val}'] = kmer_results[k_val]\n",
    "\t\t\n",
    "\t\t# Entropy rates\n",
    "\t\tif model_name in family_results['entropy_rates']:\n",
    "\t\t\tentropy_results = family_results['entropy_rates'][model_name]\n",
    "\t\t\tfor order in ['order=0', 'order=1', 'order=2', 'order=3']:\n",
    "\t\t\t\tif order in entropy_results:\n",
    "\t\t\t\t\trow[f'Entropy_{order}'] = entropy_results[order]\n",
    "\t\t\n",
    "\t\t# Per-position entropy\n",
    "\t\tif model_name in family_results['per_position_entropy']:\n",
    "\t\t\tpp_entropy = family_results['per_position_entropy'][model_name]\n",
    "\t\t\trow['PerPos_Entropy_Mean'] = pp_entropy['mean']\n",
    "\t\t\trow['PerPos_Entropy_Std'] = pp_entropy['std']\n",
    "\t\t\n",
    "\t\t# Cross-representation MI\n",
    "\t\tif model_name in family_results['cross_representation_mi']:\n",
    "\t\t\tmi_results = family_results['cross_representation_mi'][model_name]\n",
    "\t\t\trow['CrossMI_R2'] = mi_results['r2_score']\n",
    "\t\t\trow['CrossMI_Spearman'] = mi_results['spearman_corr']\n",
    "\t\t\n",
    "\t\tsummary_data.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'all_families_results.csv')\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ CSV results saved to: {csv_path}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"─\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"─\"*70)\n",
    "\n",
    "# Group by model and show average performance across families\n",
    "print(\"\\n1. K-MER DISCRIMINATION (k=2) - Average Silhouette Score:\")\n",
    "kmer_cols = [c for c in results_df.columns if 'KMer_k=2' in c]\n",
    "if kmer_cols:\n",
    "\tkmer_summary = results_df.groupby('Model')['KMer_k=2'].mean().sort_values(ascending=False)\n",
    "\tfor model, score in kmer_summary.items():\n",
    "\t\tprint(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n2. ENTROPY RATE (1st-order) - Average Entropy:\")\n",
    "entropy_cols = [c for c in results_df.columns if 'Entropy_order=1' in c]\n",
    "if entropy_cols:\n",
    "\tentropy_summary = results_df.groupby('Model')['Entropy_order=1'].mean().sort_values()\n",
    "\tfor model, score in entropy_summary.items():\n",
    "\t\tprint(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n3. PER-POSITION ENTROPY - Average Mean Entropy:\")\n",
    "if 'PerPos_Entropy_Mean' in results_df.columns:\n",
    "\tpp_summary = results_df.groupby('Model')['PerPos_Entropy_Mean'].mean().sort_values()\n",
    "\tfor model, score in pp_summary.items():\n",
    "\t\tprint(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n4. CROSS-REPRESENTATION MI - Average R² Score:\")\n",
    "if 'CrossMI_R2' in results_df.columns:\n",
    "\tmi_summary = results_df.groupby('Model')['CrossMI_R2'].mean().sort_values(ascending=False)\n",
    "\tfor model, score in mi_summary.items():\n",
    "\t\tprint(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ BENCHMARK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256102f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MULTI-FAMILY VISUALIZATION ====================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Multi-Family Multi-Model Benchmark Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Define colors for models\n",
    "unique_models = results_df['Model'].unique()\n",
    "model_colors = {model: color for model, color in zip(unique_models, sns.color_palette(\"husl\", len(unique_models)))}\n",
    "\n",
    "# ========== 1. K-MER DISCRIMINATION ACROSS FAMILIES ==========\n",
    "ax = axes[0, 0]\n",
    "\n",
    "for model in unique_models:\n",
    "\tmodel_data = results_df[results_df['Model'] == model]\n",
    "\tif 'KMer_k=2' in model_data.columns:\n",
    "\t\tfamilies = model_data['Family'].values\n",
    "\t\tscores = model_data['KMer_k=2'].values\n",
    "\t\tax.plot(families, scores, marker='o', label=model, \n",
    "\t\t\t\tcolor=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Silhouette Score (k=2)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('K-mer Fold Discrimination', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ========== 2. ENTROPY RATES ACROSS FAMILIES ==========\n",
    "ax = axes[0, 1]\n",
    "\n",
    "for model in unique_models:\n",
    "\tmodel_data = results_df[results_df['Model'] == model]\n",
    "\tif 'Entropy_order=1' in model_data.columns:\n",
    "\t\tfamilies = model_data['Family'].values\n",
    "\t\tentropies = model_data['Entropy_order=1'].values\n",
    "\t\tax.plot(families, entropies, marker='s', label=model,\n",
    "\t\t\t\tcolor=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Entropy Rate (1st-order)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Markov Entropy Estimation', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ========== 3. PER-POSITION ENTROPY ACROSS FAMILIES ==========\n",
    "ax = axes[1, 0]\n",
    "\n",
    "if 'PerPos_Entropy_Mean' in results_df.columns:\n",
    "\tfor model in unique_models:\n",
    "\t\tmodel_data = results_df[results_df['Model'] == model]\n",
    "\t\tfamilies = model_data['Family'].values\n",
    "\t\tentropies = model_data['PerPos_Entropy_Mean'].values\n",
    "\t\tax.plot(families, entropies, marker='^', label=model,\n",
    "\t\t\t\tcolor=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "\tax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "\tax.set_ylabel('Mean Entropy', fontsize=12, fontweight='bold')\n",
    "\tax.set_title('Per-Position Entropy', fontsize=14, fontweight='bold')\n",
    "\tax.legend(loc='best', fontsize=10)\n",
    "\tax.grid(True, alpha=0.3)\n",
    "\tax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ========== 4. CROSS-REPRESENTATION MI ACROSS FAMILIES ==========\n",
    "ax = axes[1, 1]\n",
    "\n",
    "if 'CrossMI_R2' in results_df.columns:\n",
    "\tfor model in unique_models:\n",
    "\t\tif model != 'AA':  # Skip AA baseline for cross-representation\n",
    "\t\t\tmodel_data = results_df[results_df['Model'] == model]\n",
    "\t\t\tfamilies = model_data['Family'].values\n",
    "\t\t\tr2_scores = model_data['CrossMI_R2'].values\n",
    "\t\t\tax.plot(families, r2_scores, marker='D', label=model,\n",
    "\t\t\t\t\tcolor=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "\tax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "\tax.set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
    "\tax.set_title('Cross-Representation MI (FoldTree2→AA)', fontsize=14, fontweight='bold')\n",
    "\tax.legend(loc='best', fontsize=10)\n",
    "\tax.grid(True, alpha=0.3)\n",
    "\tax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(OUTPUT_DIR, 'multi_family_benchmark_comparison.png')\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Visualization saved to: {fig_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ==================== HEATMAP: MODEL PERFORMANCE BY FAMILY ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HEATMAP: MODEL PERFORMANCE BY FAMILY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "if 'KMer_k=2' in results_df.columns:\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\tfig.suptitle('Model Performance Heatmaps by Family', fontsize=16, fontweight='bold')\n",
    "\t\n",
    "\t# K-mer discrimination heatmap\n",
    "\tax = axes[0, 0]\n",
    "\tpivot_kmer = results_df.pivot(index='Model', columns='Family', values='KMer_k=2')\n",
    "\tsns.heatmap(pivot_kmer, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax, cbar_kws={'label': 'Score'})\n",
    "\tax.set_title('K-mer Discrimination (k=2)', fontsize=14, fontweight='bold')\n",
    "\tax.set_xlabel('Family', fontsize=12)\n",
    "\tax.set_ylabel('Model', fontsize=12)\n",
    "\t\n",
    "\t# Entropy rate heatmap\n",
    "\tif 'Entropy_order=1' in results_df.columns:\n",
    "\t\tax = axes[0, 1]\n",
    "\t\tpivot_entropy = results_df.pivot(index='Model', columns='Family', values='Entropy_order=1')\n",
    "\t\tsns.heatmap(pivot_entropy, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Entropy'})\n",
    "\t\tax.set_title('Entropy Rate (1st-order)', fontsize=14, fontweight='bold')\n",
    "\t\tax.set_xlabel('Family', fontsize=12)\n",
    "\t\tax.set_ylabel('Model', fontsize=12)\n",
    "\t\n",
    "\t# Per-position entropy heatmap\n",
    "\tif 'PerPos_Entropy_Mean' in results_df.columns:\n",
    "\t\tax = axes[1, 0]\n",
    "\t\tpivot_pp = results_df.pivot(index='Model', columns='Family', values='PerPos_Entropy_Mean')\n",
    "\t\tsns.heatmap(pivot_pp, annot=True, fmt='.3f', cmap='Purples', ax=ax, cbar_kws={'label': 'Entropy'})\n",
    "\t\tax.set_title('Per-Position Entropy', fontsize=14, fontweight='bold')\n",
    "\t\tax.set_xlabel('Family', fontsize=12)\n",
    "\t\tax.set_ylabel('Model', fontsize=12)\n",
    "\t\n",
    "\t# Cross-MI heatmap\n",
    "\tif 'CrossMI_R2' in results_df.columns:\n",
    "\t\tax = axes[1, 1]\n",
    "\t\t# Filter out AA for cross-representation\n",
    "\t\tmi_data = results_df[results_df['Model'] != 'AA']\n",
    "\t\tpivot_mi = mi_data.pivot(index='Model', columns='Family', values='CrossMI_R2')\n",
    "\t\tsns.heatmap(pivot_mi, annot=True, fmt='.3f', cmap='Greens', ax=ax, cbar_kws={'label': 'R²'})\n",
    "\t\tax.set_title('Cross-Representation MI', fontsize=14, fontweight='bold')\n",
    "\t\tax.set_xlabel('Family', fontsize=12)\n",
    "\t\tax.set_ylabel('Model', fontsize=12)\n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\theatmap_path = os.path.join(OUTPUT_DIR, 'model_family_heatmaps.png')\n",
    "\tplt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "\tprint(f\"✓ Heatmaps saved to: {heatmap_path}\")\n",
    "\tplt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL VISUALIZATIONS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foldtree2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
