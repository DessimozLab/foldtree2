{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c660247e",
   "metadata": {},
   "source": [
    "# Multi-Family Multi-Model Information Content Benchmark\n",
    "\n",
    "This notebook performs comprehensive information-theoretic benchmarks on **multiple protein families** using **multiple FoldTree2 models** with different alphabet sizes, comparing them against standard amino acid representations.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Evaluate how well discrete structural alphabets (DSR - Discrete Structural Representations) capture biologically relevant information compared to amino acid sequences, specifically testing:\n",
    "\n",
    "1. **Fold Discrimination**: Can k-mers distinguish protein folds better?\n",
    "2. **Entropy Rate**: How compressible are the representations?\n",
    "3. **Position-Specific Conservation**: Does entropy reflect functional constraints?\n",
    "4. **Cross-Representation Information**: How much structural information predicts sequence?\n",
    "\n",
    "## Family Data Sources\n",
    "\n",
    "This notebook supports **two family data sources**:\n",
    "\n",
    "### Option 1: Custom Families (USE_MAKESUBMAT_FAMILIES = False)\n",
    "Manual curation of protein families in separate folders:\n",
    "```\n",
    "FAMILIES_DIR/\n",
    "  ├── rhodopsin/\n",
    "  │   ├── structure1.pdb\n",
    "  │   └── structure2.pdb\n",
    "  ├── kinase/\n",
    "  │   ├── structure3.pdb\n",
    "  │   └── structure4.pdb\n",
    "  └── ...\n",
    "```\n",
    "\n",
    "### Option 2: AFDB Cluster Families (USE_MAKESUBMAT_FAMILIES = True) ⭐ RECOMMENDED\n",
    "**Uses the same ~200+ families as `makesubmat.py`** from AlphaFold Database clusters:\n",
    "```\n",
    "struct_align/              # Created by makesubmat.py\n",
    "  ├── AF-A0A024RBG1-F1/   # AFDB cluster representative ID\n",
    "  │   └── structs/\n",
    "  │       ├── AF-A0A024RBG1-F1-model_v4.pdb\n",
    "  │       ├── AF-P12345-F1-model_v4.pdb\n",
    "  │       └── ...\n",
    "  ├── AF-A0A075B6H9-F1/\n",
    "  │   └── structs/\n",
    "  │       └── ...\n",
    "  └── ... (200+ families)\n",
    "```\n",
    "\n",
    "**Prerequisites for Option 2**:\n",
    "1. Run `makesubmat.py --download_structs` to fetch AFDB cluster structures\n",
    "2. Point `MAKESUBMAT_BASE_DIR` to your datasets directory\n",
    "3. Set `USE_MAKESUBMAT_FAMILIES = True` in Cell #3\n",
    "\n",
    "**Advantages of Option 2**:\n",
    "- **Consistency**: Benchmark on same data used for substitution matrices\n",
    "- **Scale**: ~200+ diverse protein families automatically\n",
    "- **Reproducibility**: Standard dataset from AlphaFold Database clusters\n",
    "- **No manual curation**: Families pre-defined by structural clustering\n",
    "\n",
    "## Benchmarks Performed (Per Family, Per Model)\n",
    "\n",
    "### 1. **K-mer Fold Discrimination**\n",
    "- Compute k-mer frequency distributions (k=1,2,3,4)\n",
    "- Perform KMeans clustering\n",
    "- Calculate silhouette scores as discrimination metric\n",
    "- **Higher scores** = better fold separation\n",
    "\n",
    "### 2. **Entropy Rate Estimation**  \n",
    "- Build k-order Markov models (orders 0-3)\n",
    "- Cross-validation for entropy estimation\n",
    "- **Lower entropy** = more predictable/compressible representation\n",
    "\n",
    "### 3. **Per-Position Entropy**\n",
    "- Create Multiple Sequence Alignments (MSAs)\n",
    "- Apply Henikoff sequence reweighting\n",
    "- Calculate position-specific entropy\n",
    "- **Entropy profile** reveals conserved vs variable positions\n",
    "\n",
    "### 4. **Cross-Representation Mutual Information**\n",
    "- Train Ridge regression: FoldTree2 features → AA features\n",
    "- Cross-validation R² scores\n",
    "- Spearman correlation between representations\n",
    "- **Higher scores** = more shared information\n",
    "\n",
    "## Outputs\n",
    "\n",
    "1. **JSON**: `all_families_results.json` - Complete results\n",
    "2. **CSV**: `all_families_results.csv` - Flattened table\n",
    "3. **Plots**:\n",
    "   - `multi_family_benchmark_comparison.png` - Line plots across families\n",
    "   - `model_family_heatmaps.png` - Performance heatmaps\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Configure** (Cell #3):\n",
    "   - **Choose data source**: Set `USE_MAKESUBMAT_FAMILIES = True` (AFDB) or `False` (custom)\n",
    "   - **Set paths**: \n",
    "     - If using AFDB: `MAKESUBMAT_BASE_DIR` (datasets directory)\n",
    "     - If custom: `FAMILIES_DIR` (your families directory)\n",
    "   - **Optional**: Set `MAX_FAMILIES` to limit number (useful for testing)\n",
    "   - Set `MODELS` to list of FoldTree2 model paths\n",
    "   - Adjust `BENCHMARK_PARAMS` if needed\n",
    "\n",
    "2. **Run cells sequentially** (Cells 1-9)\n",
    "\n",
    "3. **View results**:\n",
    "   - Terminal output shows progress and summary statistics\n",
    "   - Plots display comparative performance\n",
    "   - CSV/JSON files for detailed analysis\n",
    "\n",
    "## Quick Start with AFDB Families\n",
    "\n",
    "```python\n",
    "# In Cell #3:\n",
    "USE_MAKESUBMAT_FAMILIES = True\n",
    "MAKESUBMAT_BASE_DIR = '/path/to/your/datasets'  # Contains struct_align/\n",
    "MAX_FAMILIES = 50  # Start with 50 families for quick test (or None for all)\n",
    "\n",
    "# Then run all cells\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import FoldTree2 modules\n",
    "sys.path.insert(0, '/home/dmoi/projects/foldtree2')\n",
    "from foldtree2.src import encoder as ft2\n",
    "from foldtree2.src.pdbgraph import PDB2PyG, StructureDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2490a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Model paths (list of models to benchmark)\n",
    "MODELS = [\n",
    "    '/home/dmoi/projects/foldtree2/models/model_10_embeddings',\n",
    "    '/home/dmoi/projects/foldtree2/models/model_20_embeddings',\n",
    "    '/home/dmoi/projects/foldtree2/models/model_30_embeddings',\n",
    "    '/home/dmoi/projects/foldtree2/models/model_40_embeddings',\n",
    "]\n",
    "\n",
    "# ==================== FAMILY DATA SOURCE CONFIGURATION ====================\n",
    "# Choose one of two modes:\n",
    "#   1. Custom families in separate folders (USE_MAKESUBMAT_FAMILIES = False)\n",
    "#   2. AFDB cluster families from makesubmat (USE_MAKESUBMAT_FAMILIES = True)\n",
    "\n",
    "USE_MAKESUBMAT_FAMILIES = True  # Set to True to use makesubmat AFDB families\n",
    "\n",
    "if USE_MAKESUBMAT_FAMILIES:\n",
    "    # Path to the struct_align directory created by makesubmat.py\n",
    "    # This contains one subfolder per AFDB cluster representative\n",
    "    # Structure: MAKESUBMAT_BASE_DIR/struct_align/{repId}/structs/*.pdb\n",
    "    MAKESUBMAT_BASE_DIR = '/home/dmoi/projects/foldtree2/../../datasets'\n",
    "    FAMILIES_DIR = os.path.join(MAKESUBMAT_BASE_DIR, 'struct_align')\n",
    "    \n",
    "    # Optional: Limit number of families (useful for quick tests)\n",
    "    # Set to None to use all available families\n",
    "    MAX_FAMILIES = None  # or e.g., 50 for first 50 families\n",
    "    \n",
    "    print(f\"Using makesubmat AFDB cluster families from: {FAMILIES_DIR}\")\n",
    "else:\n",
    "    # Custom families directory containing subfolders for each protein family\n",
    "    # Each subfolder should contain PDB files for that family\n",
    "    # Example structure:\n",
    "    #   FAMILIES_DIR/\n",
    "    #     rhodopsin/\n",
    "    #       structure1.pdb\n",
    "    #       structure2.pdb\n",
    "    #     kinase/\n",
    "    #       structure3.pdb\n",
    "    #       structure4.pdb\n",
    "    FAMILIES_DIR = '/home/dmoi/projects/foldtree2/families/examples'\n",
    "    MAX_FAMILIES = None\n",
    "    \n",
    "    print(f\"Using custom families from: {FAMILIES_DIR}\")\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = '/home/dmoi/projects/foldtree2/benchmark_output'\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_PARAMS = {\n",
    "    'k_mer_sizes': [1, 2, 3, 4],\n",
    "    'markov_orders': [0, 1, 2, 3],\n",
    "    'cv_folds': 5,\n",
    "    'alpha_smoothing': 0.001,\n",
    "    'occupancy_threshold': 0.7,\n",
    "    'reweight_threshold': 0.8,\n",
    "}\n",
    "\n",
    "# Standard amino acid alphabet\n",
    "AA_ALPHABET = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "print(f\"\\nConfiguration loaded:\")\n",
    "print(f\"  Models: {len(MODELS)}\")\n",
    "print(f\"  Families directory: {FAMILIES_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "\n",
    "# ==================== FAMILY DISCOVERY ====================\n",
    "\n",
    "def discover_families(families_dir, use_makesubmat=False, max_families=None):\n",
    "    \"\"\"\n",
    "    Discover protein families from directory structure.\n",
    "    \n",
    "    Supports two modes:\n",
    "    1. Custom families: Each subfolder is a family\n",
    "    2. Makesubmat AFDB: Each subfolder contains a 'structs' subdirectory\n",
    "    \n",
    "    Args:\n",
    "        families_dir: Base directory containing families\n",
    "        use_makesubmat: If True, expect makesubmat structure (repId/structs/)\n",
    "        max_families: Maximum number of families to include (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        dict: {family_name: {'path': str, 'n_structures': int}}\n",
    "    \"\"\"\n",
    "    families = {}\n",
    "    \n",
    "    if not os.path.isdir(families_dir):\n",
    "        print(f\"ERROR: Families directory not found: {families_dir}\")\n",
    "        return families\n",
    "    \n",
    "    items = sorted(os.listdir(families_dir))\n",
    "    \n",
    "    # Limit number of families if specified\n",
    "    if max_families is not None:\n",
    "        items = items[:max_families]\n",
    "    \n",
    "    for item in items:\n",
    "        family_base_path = os.path.join(families_dir, item)\n",
    "        \n",
    "        if not os.path.isdir(family_base_path):\n",
    "            continue\n",
    "        \n",
    "        # Determine the actual structures directory\n",
    "        if use_makesubmat:\n",
    "            # Makesubmat structure: {repId}/structs/*.pdb\n",
    "            structs_dir = os.path.join(family_base_path, 'structs')\n",
    "            if not os.path.isdir(structs_dir):\n",
    "                continue\n",
    "            family_path = structs_dir\n",
    "        else:\n",
    "            # Custom structure: {family_name}/*.pdb\n",
    "            family_path = family_base_path\n",
    "        \n",
    "        # Count PDB files\n",
    "        pdb_files = glob.glob(os.path.join(family_path, '*.pdb'))\n",
    "        \n",
    "        # Only include if there are structures\n",
    "        if len(pdb_files) > 0:\n",
    "            families[item] = {\n",
    "                'path': family_path,\n",
    "                'n_structures': len(pdb_files)\n",
    "            }\n",
    "    \n",
    "    return families\n",
    "\n",
    "# Discover families\n",
    "FAMILIES = discover_families(\n",
    "    FAMILIES_DIR, \n",
    "    use_makesubmat=USE_MAKESUBMAT_FAMILIES,\n",
    "    max_families=MAX_FAMILIES\n",
    ")\n",
    "\n",
    "print(f\"\\nDiscovered {len(FAMILIES)} protein families:\")\n",
    "if len(FAMILIES) == 0:\n",
    "    print(f\"ERROR: No families found!\")\n",
    "    if USE_MAKESUBMAT_FAMILIES:\n",
    "        print(f\"Expected structure: {FAMILIES_DIR}/{{repId}}/structs/*.pdb\")\n",
    "        print(f\"Make sure you've run makesubmat.py with --download_structs first\")\n",
    "    else:\n",
    "        print(f\"Expected structure: {FAMILIES_DIR}/{{family_name}}/*.pdb\")\n",
    "else:\n",
    "    # Show first 10 families as sample\n",
    "    sample_families = list(FAMILIES.items())[:10]\n",
    "    for family_name, family_info in sample_families:\n",
    "        print(f\"  - {family_name}: {family_info['n_structures']} structures\")\n",
    "    \n",
    "    if len(FAMILIES) > 10:\n",
    "        print(f\"  ... and {len(FAMILIES) - 10} more families\")\n",
    "    \n",
    "    # Show statistics\n",
    "    structure_counts = [info['n_structures'] for info in FAMILIES.values()]\n",
    "    print(f\"\\nFamily statistics:\")\n",
    "    print(f\"  Total families: {len(FAMILIES)}\")\n",
    "    print(f\"  Total structures: {sum(structure_counts)}\")\n",
    "    print(f\"  Structures per family (mean ± std): {np.mean(structure_counts):.1f} ± {np.std(structure_counts):.1f}\")\n",
    "    print(f\"  Range: {min(structure_counts)} - {max(structure_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fce6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA LOADING & ENCODING ====================\n",
    "\n",
    "class ModelBenchmark:\n",
    "    \"\"\"Container for model encoding results and metadata\"\"\"\n",
    "    def __init__(self, model_path: str, device):\n",
    "        self.model_path = model_path\n",
    "        self.model_name = Path(model_path).stem\n",
    "        self.device = device\n",
    "        \n",
    "        # Load encoder\n",
    "        print(f\"  Loading model: {self.model_name}\")\n",
    "        self.encoder = torch.load(model_path + '.pt', map_location=device, weights_only=False)\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.encoder.device = device\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        # Extract model metadata\n",
    "        self.num_embeddings = self.encoder.num_embeddings\n",
    "        self.embedding_dim = self.encoder.out_channels\n",
    "        \n",
    "        # Storage for encoded sequences\n",
    "        self.encoded_fasta = None\n",
    "        self.encoded_df = None\n",
    "        self.alphabet = None\n",
    "        self.char_position_map = None\n",
    "        \n",
    "        print(f\"    ✓ Loaded: {self.num_embeddings} embeddings, dim={self.embedding_dim}\")\n",
    "    \n",
    "    def encode_structures(self, structures_loader, output_dir, family_name=\"encoded\"):\n",
    "        \"\"\"Encode structures using this model\"\"\"\n",
    "        output_path = os.path.join(output_dir, f\"{family_name}_{self.model_name}_encoded.fasta\")\n",
    "        \n",
    "        print(f\"  Encoding structures with {self.model_name}...\")\n",
    "        self.encoder.encode_structures_fasta(\n",
    "            structures_loader, \n",
    "            output_path, \n",
    "            replace=True\n",
    "        )\n",
    "        \n",
    "        self.encoded_fasta = output_path\n",
    "        self.encoded_df = ft2.load_encoded_fasta(output_path, alphabet=None, replace=False)\n",
    "        self._build_alphabet()\n",
    "        \n",
    "        print(f\"    ✓ Encoded {len(self.encoded_df)} sequences\")\n",
    "        return output_path\n",
    "    \n",
    "    def _build_alphabet(self):\n",
    "        \"\"\"Build alphabet from encoded sequences\"\"\"\n",
    "        char_set = set()\n",
    "        for seq in self.encoded_df.seq:\n",
    "            char_set = char_set.union(set(seq))\n",
    "        self.alphabet = sorted(list(char_set))\n",
    "        self.char_position_map = {char: i for i, char in enumerate(self.alphabet)}\n",
    "        print(f\"    ✓ Alphabet size: {len(self.alphabet)} characters\")\n",
    "\n",
    "def load_structures(structures_dir: str, converter: PDB2PyG, verbose: bool = True):\n",
    "    \"\"\"Load and convert PDB structures to PyG format\"\"\"\n",
    "    pdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "    if verbose:\n",
    "        print(f\"  Found {len(pdb_files)} PDB files\")\n",
    "    \n",
    "    if len(pdb_files) == 0:\n",
    "        raise ValueError(f\"No PDB files found in {structures_dir}\")\n",
    "    \n",
    "    def struct_generator():\n",
    "        for pdb_file in pdb_files:\n",
    "            try:\n",
    "                data = converter.struct2pyg(pdb_file)\n",
    "                if data:\n",
    "                    yield data\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"    Warning: Failed to convert {Path(pdb_file).name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return struct_generator()\n",
    "\n",
    "def extract_aa_sequences(structures_dir: str, output_dir: str, family_name: str = \"sequences\", verbose: bool = True):\n",
    "    \"\"\"Extract amino acid sequences from PDB files\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    aa_dict = {\n",
    "        'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "        'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "        'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "        'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "    }\n",
    "    \n",
    "    pdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "    sequences = {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Extracting amino acid sequences from {len(pdb_files)} structures...\")\n",
    "    for pdb_file in pdb_files:\n",
    "        structure_id = Path(pdb_file).stem\n",
    "        try:\n",
    "            structure = parser.get_structure(structure_id, pdb_file)\n",
    "            seq = \"\"\n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    for residue in chain:\n",
    "                        if residue.get_resname() in aa_dict:\n",
    "                            seq += aa_dict[residue.get_resname()]\n",
    "                    break  # Only first chain\n",
    "                break  # Only first model\n",
    "            \n",
    "            if seq:\n",
    "                sequences[structure_id] = seq\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"    Warning: Failed to extract sequence from {Path(pdb_file).name}: {e}\")\n",
    "    \n",
    "    # Write to FASTA with family-specific naming\n",
    "    output_path = os.path.join(output_dir, f\"{family_name}_aa_sequences.fasta\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for struct_id, seq in sequences.items():\n",
    "            f.write(f\">{struct_id}\\n{seq}\\n\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"    ✓ Extracted {len(sequences)} AA sequences\")\n",
    "    return output_path, sequences\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize converter\n",
    "converter = PDB2PyG(aapropcsv='/home/dmoi/projects/foldtree2/foldtree2/config/aaindex1.csv')\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40233aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RUN ALL BENCHMARKS PER FAMILY ====================\n",
    "\n",
    "# Store results per family\n",
    "all_results = {}\n",
    "\n",
    "for family_name, family_info in FAMILIES.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FAMILY: {family_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  {family_info['n_structures']} structures in {family_info['path']}\")\n",
    "    \n",
    "    family_path = family_info['path']\n",
    "    family_results = {\n",
    "        'n_structures': family_info['n_structures'],\n",
    "        'models': [],\n",
    "        'alphabet_sizes': [],\n",
    "        'k_mer_discrimination': {},\n",
    "        'entropy_rates': {},\n",
    "        'per_position_entropy': {},\n",
    "        'cross_representation_mi': {}\n",
    "    }\n",
    "    \n",
    "    # Extract AA sequences for this family\n",
    "    print(f\"\\n1. Extracting amino acid sequences for {family_name}...\")\n",
    "    aa_fasta_path, aa_sequences = extract_aa_sequences(\n",
    "        family_path, OUTPUT_DIR, family_name, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Load structures once for all models\n",
    "    print(f\"\\n2. Loading structures for {family_name}...\")\n",
    "    \n",
    "    # Encode structures with each model\n",
    "    model_benchmarks = []\n",
    "    for model_path in MODELS:\n",
    "        print(f\"\\n3. Processing model: {Path(model_path).stem}\")\n",
    "        mb = ModelBenchmark(model_path, DEVICE)\n",
    "        \n",
    "        # Load and encode structures\n",
    "        structures_loader = load_structures(family_path, converter, verbose=True)\n",
    "        mb.encode_structures(structures_loader, OUTPUT_DIR, family_name)\n",
    "        model_benchmarks.append(mb)\n",
    "        \n",
    "        family_results['models'].append(mb.model_name)\n",
    "        family_results['alphabet_sizes'].append(mb.num_embeddings)\n",
    "    \n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"BENCHMARKING {family_name}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # ==================== BENCHMARK 1: K-MER FOLD DISCRIMINATION ====================\n",
    "    print(\"\\n4. K-mer Fold Discrimination Test\")\n",
    "    print(\"   Testing if k-mers can discriminate folds better than random...\")\n",
    "    \n",
    "    for mb in model_benchmarks:\n",
    "        model_results = {}\n",
    "        for k in [1, 2, 3, 4]:\n",
    "            # Count k-mers for each sequence\n",
    "            kmer_counts = []\n",
    "            for seq in mb.encoded_df.seq:\n",
    "                kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "                kmer_counter = Counter(kmers)\n",
    "                # Create feature vector from k-mer counts\n",
    "                kmer_counts.append(list(kmer_counter.values()))\n",
    "            \n",
    "            # Pad to same length\n",
    "            max_len = max(len(x) for x in kmer_counts)\n",
    "            X = np.array([x + [0]*(max_len-len(x)) for x in kmer_counts])\n",
    "            \n",
    "            # KMeans clustering\n",
    "            kmeans = KMeans(n_clusters=min(3, len(X)), random_state=42)\n",
    "            labels = kmeans.fit_predict(X)\n",
    "            score = silhouette_score(X, labels)\n",
    "            \n",
    "            model_results[f'k={k}'] = score\n",
    "        \n",
    "        family_results['k_mer_discrimination'][mb.model_name] = model_results\n",
    "        print(f\"   {mb.model_name}: k=1: {model_results['k=1']:.3f}, k=2: {model_results['k=2']:.3f}, k=3: {model_results['k=3']:.3f}, k=4: {model_results['k=4']:.3f}\")\n",
    "    \n",
    "    # Also run for AA sequences\n",
    "    aa_results = {}\n",
    "    for k in [1, 2, 3, 4]:\n",
    "        kmer_counts = []\n",
    "        for seq in aa_sequences.values():\n",
    "            kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "            kmer_counter = Counter(kmers)\n",
    "            kmer_counts.append(list(kmer_counter.values()))\n",
    "        \n",
    "        max_len = max(len(x) for x in kmer_counts)\n",
    "        X = np.array([x + [0]*(max_len-len(x)) for x in kmer_counts])\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=min(3, len(X)), random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        aa_results[f'k={k}'] = score\n",
    "    \n",
    "    family_results['k_mer_discrimination']['AA'] = aa_results\n",
    "    print(f\"   AA: k=1: {aa_results['k=1']:.3f}, k=2: {aa_results['k=2']:.3f}, k=3: {aa_results['k=3']:.3f}, k=4: {aa_results['k=4']:.3f}\")\n",
    "    \n",
    "    # ==================== BENCHMARK 2: ENTROPY RATE ESTIMATION ====================\n",
    "    print(\"\\n5. Entropy Rate Estimation\")\n",
    "    print(\"   Estimating entropy rate using k-order Markov models...\")\n",
    "    \n",
    "    for mb in model_benchmarks:\n",
    "        model_results = {}\n",
    "        sequences = list(mb.encoded_df.seq)\n",
    "        \n",
    "        for order in [0, 1, 2, 3]:\n",
    "            # k-fold cross-validation\n",
    "            n_folds = min(5, len(sequences))\n",
    "            fold_size = len(sequences) // n_folds\n",
    "            entropies = []\n",
    "            \n",
    "            for fold in range(n_folds):\n",
    "                # Split data\n",
    "                test_start = fold * fold_size\n",
    "                test_end = test_start + fold_size\n",
    "                test_seqs = sequences[test_start:test_end]\n",
    "                train_seqs = sequences[:test_start] + sequences[test_end:]\n",
    "                \n",
    "                # Build k-order Markov model from training data\n",
    "                if order == 0:\n",
    "                    # 0-order: character frequencies\n",
    "                    char_counts = Counter()\n",
    "                    for seq in train_seqs:\n",
    "                        char_counts.update(seq)\n",
    "                    total = sum(char_counts.values())\n",
    "                    probs = {c: (count + 0.001) / (total + 0.001 * len(char_counts)) \n",
    "                            for c, count in char_counts.items()}\n",
    "                    \n",
    "                    # Calculate entropy on test set\n",
    "                    test_entropy = 0\n",
    "                    for seq in test_seqs:\n",
    "                        for c in seq:\n",
    "                            if c in probs:\n",
    "                                test_entropy += -np.log2(probs[c])\n",
    "                            else:\n",
    "                                test_entropy += -np.log2(0.001 / (total + 0.001 * len(char_counts)))\n",
    "                    test_entropy /= sum(len(s) for s in test_seqs)\n",
    "                else:\n",
    "                    # k-order: k-gram frequencies\n",
    "                    kgram_counts = Counter()\n",
    "                    context_counts = Counter()\n",
    "                    for seq in train_seqs:\n",
    "                        for i in range(len(seq) - order):\n",
    "                            context = seq[i:i+order]\n",
    "                            next_char = seq[i+order]\n",
    "                            kgram_counts[(context, next_char)] += 1\n",
    "                            context_counts[context] += 1\n",
    "                    \n",
    "                    # Calculate entropy on test set\n",
    "                    test_entropy = 0\n",
    "                    for seq in test_seqs:\n",
    "                        for i in range(len(seq) - order):\n",
    "                            context = seq[i:i+order]\n",
    "                            next_char = seq[i+order]\n",
    "                            # Laplace smoothing\n",
    "                            count = kgram_counts.get((context, next_char), 0) + 0.001\n",
    "                            total = context_counts.get(context, 0) + 0.001 * len(mb.alphabet)\n",
    "                            prob = count / total\n",
    "                            test_entropy += -np.log2(prob)\n",
    "                    test_entropy /= sum(len(s) - order for s in test_seqs)\n",
    "                \n",
    "                entropies.append(test_entropy)\n",
    "            \n",
    "            model_results[f'order={order}'] = np.mean(entropies)\n",
    "        \n",
    "        family_results['entropy_rates'][mb.model_name] = model_results\n",
    "        print(f\"   {mb.model_name}: 0-order: {model_results['order=0']:.3f}, 1-order: {model_results['order=1']:.3f}, 2-order: {model_results['order=2']:.3f}, 3-order: {model_results['order=3']:.3f}\")\n",
    "    \n",
    "    # Also run for AA sequences\n",
    "    aa_alphabet = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "    aa_results = {}\n",
    "    sequences = list(aa_sequences.values())\n",
    "    \n",
    "    for order in [0, 1, 2, 3]:\n",
    "        n_folds = min(5, len(sequences))\n",
    "        fold_size = len(sequences) // n_folds\n",
    "        entropies = []\n",
    "        \n",
    "        for fold in range(n_folds):\n",
    "            test_start = fold * fold_size\n",
    "            test_end = test_start + fold_size\n",
    "            test_seqs = sequences[test_start:test_end]\n",
    "            train_seqs = sequences[:test_start] + sequences[test_end:]\n",
    "            \n",
    "            if order == 0:\n",
    "                char_counts = Counter()\n",
    "                for seq in train_seqs:\n",
    "                    char_counts.update(seq)\n",
    "                total = sum(char_counts.values())\n",
    "                probs = {c: (count + 0.001) / (total + 0.001 * len(char_counts)) \n",
    "                        for c, count in char_counts.items()}\n",
    "                \n",
    "                test_entropy = 0\n",
    "                for seq in test_seqs:\n",
    "                    for c in seq:\n",
    "                        if c in probs:\n",
    "                            test_entropy += -np.log2(probs[c])\n",
    "                        else:\n",
    "                            test_entropy += -np.log2(0.001 / (total + 0.001 * len(char_counts)))\n",
    "                test_entropy /= sum(len(s) for s in test_seqs)\n",
    "            else:\n",
    "                kgram_counts = Counter()\n",
    "                context_counts = Counter()\n",
    "                for seq in train_seqs:\n",
    "                    for i in range(len(seq) - order):\n",
    "                        context = seq[i:i+order]\n",
    "                        next_char = seq[i+order]\n",
    "                        kgram_counts[(context, next_char)] += 1\n",
    "                        context_counts[context] += 1\n",
    "                \n",
    "                test_entropy = 0\n",
    "                for seq in test_seqs:\n",
    "                    for i in range(len(seq) - order):\n",
    "                        context = seq[i:i+order]\n",
    "                        next_char = seq[i+order]\n",
    "                        count = kgram_counts.get((context, next_char), 0) + 0.001\n",
    "                        total = context_counts.get(context, 0) + 0.001 * len(aa_alphabet)\n",
    "                        prob = count / total\n",
    "                        test_entropy += -np.log2(prob)\n",
    "                test_entropy /= sum(len(s) - order for s in test_seqs)\n",
    "            \n",
    "            entropies.append(test_entropy)\n",
    "        \n",
    "        aa_results[f'order={order}'] = np.mean(entropies)\n",
    "    \n",
    "    family_results['entropy_rates']['AA'] = aa_results\n",
    "    print(f\"   AA: 0-order: {aa_results['order=0']:.3f}, 1-order: {aa_results['order=1']:.3f}, 2-order: {aa_results['order=2']:.3f}, 3-order: {aa_results['order=3']:.3f}\")\n",
    "    \n",
    "    # ==================== BENCHMARK 3: PER-POSITION ENTROPY ====================\n",
    "    print(\"\\n6. Per-Position Entropy\")\n",
    "    print(\"   Calculating position-specific entropy from MSAs...\")\n",
    "    \n",
    "    for mb in model_benchmarks:\n",
    "        # Create MSA\n",
    "        sequences = list(mb.encoded_df.seq)\n",
    "        max_len = max(len(s) for s in sequences)\n",
    "        \n",
    "        # Pad sequences with gaps\n",
    "        padded = [s + '-' * (max_len - len(s)) for s in sequences]\n",
    "        \n",
    "        # Calculate Henikoff weights\n",
    "        weights = []\n",
    "        for i, seq in enumerate(padded):\n",
    "            weight = 0\n",
    "            for pos in range(max_len):\n",
    "                char = seq[pos]\n",
    "                n_different = len(set(s[pos] for s in padded))\n",
    "                n_with_char = sum(1 for s in padded if s[pos] == char)\n",
    "                weight += 1.0 / (n_different * n_with_char)\n",
    "            weights.append(weight)\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = sum(weights)\n",
    "        weights = [w / total_weight for w in weights]\n",
    "        \n",
    "        # Calculate per-position entropy\n",
    "        position_entropies = []\n",
    "        for pos in range(max_len):\n",
    "            char_weights = {}\n",
    "            for i, seq in enumerate(padded):\n",
    "                char = seq[pos]\n",
    "                char_weights[char] = char_weights.get(char, 0) + weights[i]\n",
    "            \n",
    "            # Filter positions with low occupancy\n",
    "            gap_weight = char_weights.get('-', 0)\n",
    "            if gap_weight > 0.3:  # Skip if >30% gaps\n",
    "                continue\n",
    "            \n",
    "            # Calculate entropy\n",
    "            entropy = 0\n",
    "            for char, weight in char_weights.items():\n",
    "                if char != '-' and weight > 0:\n",
    "                    entropy += -weight * np.log2(weight)\n",
    "            \n",
    "            position_entropies.append(entropy)\n",
    "        \n",
    "        avg_entropy = np.mean(position_entropies) if position_entropies else 0\n",
    "        family_results['per_position_entropy'][mb.model_name] = {\n",
    "            'mean': avg_entropy,\n",
    "            'std': np.std(position_entropies) if position_entropies else 0\n",
    "        }\n",
    "        print(f\"   {mb.model_name}: mean={avg_entropy:.3f}, std={np.std(position_entropies) if position_entropies else 0:.3f}\")\n",
    "    \n",
    "    # Also run for AA sequences\n",
    "    sequences = list(aa_sequences.values())\n",
    "    max_len = max(len(s) for s in sequences)\n",
    "    padded = [s + '-' * (max_len - len(s)) for s in sequences]\n",
    "    \n",
    "    weights = []\n",
    "    for i, seq in enumerate(padded):\n",
    "        weight = 0\n",
    "        for pos in range(max_len):\n",
    "            char = seq[pos]\n",
    "            n_different = len(set(s[pos] for s in padded))\n",
    "            n_with_char = sum(1 for s in padded if s[pos] == char)\n",
    "            weight += 1.0 / (n_different * n_with_char)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    total_weight = sum(weights)\n",
    "    weights = [w / total_weight for w in weights]\n",
    "    \n",
    "    position_entropies = []\n",
    "    for pos in range(max_len):\n",
    "        char_weights = {}\n",
    "        for i, seq in enumerate(padded):\n",
    "            char = seq[pos]\n",
    "            char_weights[char] = char_weights.get(char, 0) + weights[i]\n",
    "        \n",
    "        gap_weight = char_weights.get('-', 0)\n",
    "        if gap_weight > 0.3:\n",
    "            continue\n",
    "        \n",
    "        entropy = 0\n",
    "        for char, weight in char_weights.items():\n",
    "            if char != '-' and weight > 0:\n",
    "                entropy += -weight * np.log2(weight)\n",
    "        \n",
    "        position_entropies.append(entropy)\n",
    "    \n",
    "    avg_entropy = np.mean(position_entropies) if position_entropies else 0\n",
    "    family_results['per_position_entropy']['AA'] = {\n",
    "        'mean': avg_entropy,\n",
    "        'std': np.std(position_entropies) if position_entropies else 0\n",
    "    }\n",
    "    print(f\"   AA: mean={avg_entropy:.3f}, std={np.std(position_entropies) if position_entropies else 0:.3f}\")\n",
    "    \n",
    "    # ==================== BENCHMARK 4: CROSS-REPRESENTATION MUTUAL INFORMATION ====================\n",
    "    print(\"\\n7. Cross-Representation Mutual Information\")\n",
    "    print(\"   Training neural predictors to estimate MI...\")\n",
    "    \n",
    "    for mb in model_benchmarks:\n",
    "        # Prepare data: FoldTree2 sequences -> AA sequences\n",
    "        ft2_seqs = mb.encoded_df.seq.values\n",
    "        aa_seqs = [aa_sequences[struct_id] for struct_id in mb.encoded_df.protid.values \n",
    "                   if struct_id in aa_sequences]\n",
    "        \n",
    "        # Create k-mer feature vectors\n",
    "        k = 2\n",
    "        ft2_features = []\n",
    "        aa_features = []\n",
    "        \n",
    "        for ft2_seq, aa_seq in zip(ft2_seqs[:len(aa_seqs)], aa_seqs):\n",
    "            # FoldTree2 k-mers\n",
    "            ft2_kmers = [ft2_seq[i:i+k] for i in range(len(ft2_seq)-k+1)]\n",
    "            ft2_counter = Counter(ft2_kmers)\n",
    "            ft2_features.append(list(ft2_counter.values()))\n",
    "            \n",
    "            # AA k-mers\n",
    "            aa_kmers = [aa_seq[i:i+k] for i in range(len(aa_seq)-k+1)]\n",
    "            aa_counter = Counter(aa_kmers)\n",
    "            aa_features.append(list(aa_counter.values()))\n",
    "        \n",
    "        # Pad to same length\n",
    "        max_ft2 = max(len(x) for x in ft2_features)\n",
    "        max_aa = max(len(x) for x in aa_features)\n",
    "        \n",
    "        X_ft2 = np.array([x + [0]*(max_ft2-len(x)) for x in ft2_features])\n",
    "        X_aa = np.array([x + [0]*(max_aa-len(x)) for x in aa_features])\n",
    "        \n",
    "        # Train simple linear predictor: FoldTree2 -> AA\n",
    "        from sklearn.linear_model import Ridge\n",
    "        model = Ridge(alpha=1.0)\n",
    "        \n",
    "        # Cross-validation\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(model, X_ft2, X_aa, cv=min(5, len(X_ft2)), \n",
    "                                 scoring='r2')\n",
    "        \n",
    "        # Calculate Spearman correlation as MI proxy\n",
    "        model.fit(X_ft2, X_aa)\n",
    "        predictions = model.predict(X_ft2)\n",
    "        \n",
    "        # Spearman correlation between predicted and actual\n",
    "        from scipy.stats import spearmanr\n",
    "        correlations = []\n",
    "        for i in range(X_aa.shape[1]):\n",
    "            corr, _ = spearmanr(predictions[:, i], X_aa[:, i])\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        avg_correlation = np.mean(correlations) if correlations else 0\n",
    "        family_results['cross_representation_mi'][mb.model_name] = {\n",
    "            'r2_score': scores.mean(),\n",
    "            'spearman_corr': avg_correlation\n",
    "        }\n",
    "        print(f\"   {mb.model_name}: R²={scores.mean():.3f}, Spearman={avg_correlation:.3f}\")\n",
    "    \n",
    "    # Store family results\n",
    "    all_results[family_name] = family_results\n",
    "    print(f\"\\n✓ Completed benchmarks for {family_name}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ ALL FAMILIES COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed {len(all_results)} families:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "\n",
    "# Save full results to JSON\n",
    "results_json_path = os.path.join(OUTPUT_DIR, 'benchmark_results.json')\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"\\n✓ Results saved to: {results_json_path}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for model_name in all_results['models']:\n",
    "    row = {\n",
    "        'model': model_name,\n",
    "        'alphabet_size': all_results['alphabet_sizes'][all_results['models'].index(model_name)]\n",
    "    }\n",
    "    \n",
    "    # K-mer discrimination\n",
    "    if model_name in all_results['k_mer_discrimination']:\n",
    "        for k_key, k_data in all_results['k_mer_discrimination'][model_name].items():\n",
    "            row[f'kmers_{k_key}_aa_sil'] = k_data['aa_silhouette']\n",
    "            row[f'kmers_{k_key}_enc_sil'] = k_data['encoded_silhouette']\n",
    "    \n",
    "    # Entropy rates\n",
    "    if model_name in all_results['entropy_rates']:\n",
    "        for order_key, order_data in all_results['entropy_rates'][model_name].items():\n",
    "            row[f'entropy_{order_key}_aa'] = order_data['aa_entropy_rate']\n",
    "            row[f'entropy_{order_key}_enc'] = order_data['encoded_entropy_rate']\n",
    "    \n",
    "    # Per-position entropy\n",
    "    if model_name in all_results['per_position_entropy'] and all_results['per_position_entropy'][model_name]:\n",
    "        pos_data = all_results['per_position_entropy'][model_name]\n",
    "        row['pos_entropy_aa_mean'] = pos_data['aa_mean_entropy']\n",
    "        row['pos_entropy_enc_mean'] = pos_data['encoded_mean_entropy']\n",
    "    \n",
    "    # Cross-representation MI\n",
    "    if model_name in all_results['cross_representation_mi'] and all_results['cross_representation_mi'][model_name]:\n",
    "        mi_data = all_results['cross_representation_mi'][model_name]\n",
    "        row['cross_rep_correlation'] = mi_data.get('proxy_correlation', np.nan)\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save summary CSV\n",
    "summary_csv_path = os.path.join(OUTPUT_DIR, 'benchmark_summary.csv')\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"✓ Summary saved to: {summary_csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706704bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Multi-Model Benchmark Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract alphabet sizes for x-axis\n",
    "alphabet_sizes = summary_df['alphabet_size'].values\n",
    "models = summary_df['model'].values\n",
    "\n",
    "# Color palette\n",
    "colors = sns.color_palette(\"husl\", len(models))\n",
    "\n",
    "# ========== 1. K-mer Discrimination (Silhouette Scores) ==========\n",
    "ax = axes[0, 0]\n",
    "k_cols = [col for col in summary_df.columns if 'kmers_k' in col and '_enc_sil' in col]\n",
    "if k_cols:\n",
    "    for i, col in enumerate(k_cols):\n",
    "        k_value = col.split('_')[1]  # Extract k value\n",
    "        ax.plot(alphabet_sizes, summary_df[col].values, marker='o', \n",
    "                label=f'{k_value} (encoded)', linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Alphabet Size', fontsize=12)\n",
    "    ax.set_ylabel('Silhouette Score', fontsize=12)\n",
    "    ax.set_title('K-mer Fold Discrimination', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([-0.1, 1.0])\n",
    "\n",
    "# ========== 2. Entropy Rate ==========\n",
    "ax = axes[0, 1]\n",
    "entropy_cols = [col for col in summary_df.columns if 'entropy_order' in col and '_enc' in col]\n",
    "if entropy_cols:\n",
    "    for col in entropy_cols:\n",
    "        order_value = col.split('_')[1]  # Extract order\n",
    "        ax.plot(alphabet_sizes, summary_df[col].values, marker='s',\n",
    "                label=f'{order_value} (encoded)', linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Alphabet Size', fontsize=12)\n",
    "    ax.set_ylabel('Entropy Rate (bits)', fontsize=12)\n",
    "    ax.set_title('Markov Entropy Rate Estimation', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ========== 3. Per-Position Entropy ==========\n",
    "ax = axes[1, 0]\n",
    "if 'pos_entropy_enc_mean' in summary_df.columns:\n",
    "    ax.scatter(alphabet_sizes, summary_df['pos_entropy_aa_mean'].values,\n",
    "               s=150, alpha=0.7, label='AA sequences', marker='o', color='blue')\n",
    "    ax.scatter(alphabet_sizes, summary_df['pos_entropy_enc_mean'].values,\n",
    "               s=150, alpha=0.7, label='Encoded sequences', marker='s', color='orange')\n",
    "    \n",
    "    # Add error bars if std available\n",
    "    if 'pos_entropy_enc_std' in summary_df.columns:\n",
    "        ax.errorbar(alphabet_sizes, summary_df['pos_entropy_enc_mean'].values,\n",
    "                   yerr=summary_df['pos_entropy_enc_std'].values, fmt='none', \n",
    "                   color='orange', alpha=0.5, capsize=5)\n",
    "    \n",
    "    ax.set_xlabel('Alphabet Size', fontsize=12)\n",
    "    ax.set_ylabel('Mean Positional Entropy (bits)', fontsize=12)\n",
    "    ax.set_title('Per-Position Entropy (MSA)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ========== 4. Model Comparison Table ==========\n",
    "ax = axes[1, 1]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create comparison table\n",
    "table_data = []\n",
    "for idx, row in summary_df.iterrows():\n",
    "    table_row = [\n",
    "        row['model'][:20],  # Truncate model name\n",
    "        f\"{row['alphabet_size']}\",\n",
    "    ]\n",
    "    \n",
    "    # Add best k-mer score\n",
    "    k_scores = [row[col] for col in summary_df.columns if 'enc_sil' in col and not pd.isna(row[col])]\n",
    "    table_row.append(f\"{max(k_scores):.3f}\" if k_scores else \"N/A\")\n",
    "    \n",
    "    # Add best entropy rate\n",
    "    ent_scores = [row[col] for col in summary_df.columns if 'entropy_order' in col and '_enc' in col and not pd.isna(row[col])]\n",
    "    table_row.append(f\"{np.mean(ent_scores):.3f}\" if ent_scores else \"N/A\")\n",
    "    \n",
    "    table_data.append(table_row)\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Model', 'Alphabet', 'Best Sil.', 'Avg Entropy'],\n",
    "                cellLoc='left',\n",
    "                loc='center',\n",
    "                colWidths=[0.4, 0.2, 0.2, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    for j in range(4):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "ax.set_title('Benchmark Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(OUTPUT_DIR, 'benchmark_comparison.png')\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Visualization saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "# Models to benchmark (list of model paths without .pt extension)\n",
    "MODELS = [\n",
    "    \"/home/dmoi/projects/foldtree2/models/model_10_embeddings_best_encoder\",\n",
    "    \"/home/dmoi/projects/foldtree2/models/model_20_embeddings_best_encoder\",\n",
    "    \"/home/dmoi/projects/foldtree2/models/model_30_embeddings_best_encoder\",\n",
    "    \"/home/dmoi/projects/foldtree2/models/model_40_embeddings_best_encoder\",\n",
    "]\n",
    "\n",
    "# Structure directory (PDB files)\n",
    "STRUCTURES_DIR = \"/home/dmoi/projects/foldtree2/alphafold_benchmark/rhodopsin/structs\"\n",
    "\n",
    "# Output directory for encoded sequences\n",
    "OUTPUT_DIR = \"/home/dmoi/projects/foldtree2/benchmark_results\"\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_PARAMS = {\n",
    "    'k_mer_sizes': [2, 3, 4, 5],  # k-mer lengths to test\n",
    "    'markov_orders': [0, 1, 2, 3, 4],  # Markov model orders\n",
    "    'cv_folds': 5,  # Cross-validation folds\n",
    "    'alpha_smoothing': 0.1,  # Smoothing parameter\n",
    "    'occupancy_threshold': 0.7,  # MSA column occupancy threshold\n",
    "    'reweight_threshold': 0.8,  # Sequence reweighting identity threshold\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  - Models to benchmark: {len(MODELS)}\")\n",
    "print(f\"  - Structure directory: {STRUCTURES_DIR}\")\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA LOADING & ENCODING ====================\n",
    "\n",
    "class ModelBenchmark:\n",
    "    \"\"\"Container for model encoding results and metadata\"\"\"\n",
    "    def __init__(self, model_path: str, device):\n",
    "        self.model_path = model_path\n",
    "        self.model_name = Path(model_path).stem\n",
    "        self.device = device\n",
    "        \n",
    "        # Load encoder\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        self.encoder = torch.load(model_path + '.pt', map_location=device, weights_only=False)\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.encoder.device = device\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        # Extract model metadata\n",
    "        self.num_embeddings = self.encoder.num_embeddings\n",
    "        self.embedding_dim = self.encoder.out_channels\n",
    "        \n",
    "        # Storage for encoded sequences\n",
    "        self.encoded_fasta = None\n",
    "        self.encoded_df = None\n",
    "        self.alphabet = None\n",
    "        self.char_position_map = None\n",
    "        \n",
    "        print(f\"  ✓ Loaded: {self.num_embeddings} embeddings, dim={self.embedding_dim}\")\n",
    "    \n",
    "    def encode_structures(self, structures_loader, output_dir):\n",
    "        \"\"\"Encode structures using this model\"\"\"\n",
    "        output_path = os.path.join(output_dir, f\"{self.model_name}_encoded.fasta\")\n",
    "        \n",
    "        print(f\"  Encoding structures with {self.model_name}...\")\n",
    "        self.encoder.encode_structures_fasta(\n",
    "            structures_loader, \n",
    "            output_path, \n",
    "            replace=True\n",
    "        )\n",
    "        \n",
    "        self.encoded_fasta = output_path\n",
    "        self.encoded_df = ft2.load_encoded_fasta(output_path, alphabet=None, replace=False)\n",
    "        self._build_alphabet()\n",
    "        \n",
    "        print(f\"  ✓ Encoded {len(self.encoded_df)} sequences\")\n",
    "        return output_path\n",
    "    \n",
    "    def _build_alphabet(self):\n",
    "        \"\"\"Build alphabet from encoded sequences\"\"\"\n",
    "        char_set = set()\n",
    "        for seq in self.encoded_df.seq:\n",
    "            char_set = char_set.union(set(seq))\n",
    "        self.alphabet = sorted(list(char_set))\n",
    "        self.char_position_map = {char: i for i, char in enumerate(self.alphabet)}\n",
    "        print(f\"  ✓ Alphabet size: {len(self.alphabet)} characters\")\n",
    "\n",
    "def load_structures(structures_dir: str, converter: PDB2PyG):\n",
    "    \"\"\"Load and convert PDB structures to PyG format\"\"\"\n",
    "    pdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "    print(f\"\\nFound {len(pdb_files)} PDB files in {structures_dir}\")\n",
    "    \n",
    "    if len(pdb_files) == 0:\n",
    "        raise ValueError(f\"No PDB files found in {structures_dir}\")\n",
    "    \n",
    "    def struct_generator():\n",
    "        for pdb_file in pdb_files:\n",
    "            try:\n",
    "                data = converter.struct2pyg(pdb_file)\n",
    "                if data:\n",
    "                    yield data\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Failed to convert {pdb_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return struct_generator()\n",
    "\n",
    "def extract_aa_sequences(structures_dir: str, output_dir: str):\n",
    "    \"\"\"Extract amino acid sequences from PDB files\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    aa_dict = {\n",
    "        'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "        'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "        'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "        'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "    }\n",
    "    \n",
    "    pdb_files = glob.glob(os.path.join(structures_dir, \"*.pdb\"))\n",
    "    sequences = {}\n",
    "    \n",
    "    print(f\"\\nExtracting amino acid sequences from {len(pdb_files)} structures...\")\n",
    "    for pdb_file in pdb_files:\n",
    "        structure_id = Path(pdb_file).stem\n",
    "        try:\n",
    "            structure = parser.get_structure(structure_id, pdb_file)\n",
    "            seq = \"\"\n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    for residue in chain:\n",
    "                        if residue.get_resname() in aa_dict:\n",
    "                            seq += aa_dict[residue.get_resname()]\n",
    "                    break  # Only first chain\n",
    "                break  # Only first model\n",
    "            \n",
    "            if seq:\n",
    "                sequences[structure_id] = seq\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to extract sequence from {pdb_file}: {e}\")\n",
    "    \n",
    "    # Write to FASTA\n",
    "    output_path = os.path.join(output_dir, \"aa_sequences.fasta\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for struct_id, seq in sequences.items():\n",
    "            f.write(f\">{struct_id}\\n{seq}\\n\")\n",
    "    \n",
    "    print(f\"  ✓ Extracted {len(sequences)} AA sequences\")\n",
    "    return output_path, sequences\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize converter\n",
    "converter = PDB2PyG(aapropcsv='/home/dmoi/projects/foldtree2/foldtree2/config/aaindex1.csv')\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note. try both alignment using foldmason, ft2 and regular mafft...\n",
    "# the aligner can also be an argument for a particular character model\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bcbf5",
   "metadata": {},
   "source": [
    "## K-mer Frequency Analysis for Fold Discrimination\n",
    "\n",
    "This experiment analyzes the discriminative power of FoldTree2 (DSR) versus amino acid representations by examining k-mer frequency distributions. The goal is to determine which alphabet better distinguishes between different protein folds.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- **K-mer extraction**: Compute frequency distributions of subsequences of length k for both AA and DSR sequences\n",
    "- **Within-family distances**: Calculate Jensen-Shannon divergences between k-mer distributions of sequences within the same fold family\n",
    "- **Between-family distances**: Measure k-mer distribution differences across distinct fold families\n",
    "- **Discrimination analysis**: Compare the separation between within-family (similar folds) and between-family (different folds) distance distributions\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. **Fold specificity**: Does the FoldTree2 alphabet capture fold-specific sequence patterns more effectively than amino acid sequences?\n",
    "2. **Optimal k-mer length**: What subsequence length provides the best discrimination for each representation?\n",
    "3. **Distribution separation**: Which alphabet shows clearer separation between intra-fold similarity and inter-fold dissimilarity?\n",
    "\n",
    "The analysis will reveal whether structural alphabets provide enhanced discriminative power for protein fold classification compared to traditional sequence-based representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def kmer_freqs(seq, k, alpha_map, A):\n",
    "    idx = [alpha_map[c] for c in seq if c in alpha_map]\n",
    "    if len(idx) < k: return np.ones(A**k)/(A**k)\n",
    "    counts = np.zeros(A**k)\n",
    "    base = A**np.arange(k)[::-1]\n",
    "    for t in range(len(idx)-k+1):\n",
    "        code = 0\n",
    "        for j in range(k):\n",
    "            code = code*A + idx[t+j]\n",
    "        counts[code]+=1\n",
    "    p = counts + 1e-9\n",
    "    p /= p.sum()\n",
    "    return p\n",
    "\n",
    "def jsd(p, q):\n",
    "    m = 0.5*(p+q)\n",
    "    def KL(a,b): \n",
    "        mask = (a>0)\n",
    "        return (a[mask]*np.log2(a[mask]/b[mask])).sum()\n",
    "    return 0.5*(KL(p,m)+KL(q,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afd397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# ---------------------------- k-mer utils ------------------------------\n",
    "\n",
    "def build_alpha_index(alphabet: str) -> Dict[str, int]:\n",
    "    return {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "def kmers_counts(seq: str, k: int, alpha_idx: Dict[str,int], A: int) -> np.ndarray:\n",
    "    \"\"\"Return counts vector of length A^k for overlapping k-mers in seq (skip k-mer if any unseen char).\"\"\"\n",
    "    L = len(seq)\n",
    "    if L < k or k == 0:\n",
    "        return np.zeros(A**max(k,1), dtype=np.float64)\n",
    "    v = np.zeros(A**k, dtype=np.float64)\n",
    "    code = -1\n",
    "    for i, ch in enumerate(seq):\n",
    "        if ch not in alpha_idx:\n",
    "            code = -1\n",
    "        else:\n",
    "            x = alpha_idx[ch]\n",
    "            if code == -1:\n",
    "                if i >= k-1:\n",
    "                    ok = True\n",
    "                    code_tmp = 0\n",
    "                    for j in range(i-k+1, i+1):\n",
    "                        c2 = seq[j]\n",
    "                        if c2 not in alpha_idx:\n",
    "                            ok = False; break\n",
    "                        code_tmp = code_tmp * A + alpha_idx[c2]\n",
    "                    if ok:\n",
    "                        code = code_tmp\n",
    "                        v[code] += 1.0\n",
    "            else:\n",
    "                code = (code % (A**(k-1))) * A + x\n",
    "                v[code] += 1.0\n",
    "    return v\n",
    "\n",
    "def kmer_prob(seq: str, k: int, alphabet: str, pseudocount: float = 1e-9) -> np.ndarray:\n",
    "    A = len(alphabet)\n",
    "    idx = build_alpha_index(alphabet)\n",
    "    c = kmers_counts(seq, k, idx, A)\n",
    "    total = c.sum()\n",
    "    if total == 0:\n",
    "        # no valid k-mers: return uniform tiny distribution\n",
    "        p = np.ones(A**k, dtype=np.float64)\n",
    "        p /= p.sum()\n",
    "        return p\n",
    "    p = (c + pseudocount) / (total + pseudocount * c.shape[0])\n",
    "    return p\n",
    "\n",
    "def build_feature_matrix(fasta: List[Tuple[str,str]], alphabet: str, k: int, pseudocount: float):\n",
    "    ids = [name for name,_ in fasta]\n",
    "    seqs = [seq.upper() for _,seq in fasta]\n",
    "    P = np.vstack([kmer_prob(s, k, alphabet, pseudocount=pseudocount) for s in seqs])\n",
    "    return ids, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- KMeans + eval -----------------------------\n",
    "\n",
    "def kmeans_cluster(P: np.ndarray, K: int, n_init: int = 20, max_iter: int = 300, random_state: int = 0):\n",
    "    # KMeans on probability vectors (Euclidean). For probability geometry, you can also sqrt-transform (Hellinger).\n",
    "    model = KMeans(n_clusters=K, n_init=n_init, max_iter=max_iter, random_state=random_state)\n",
    "    labels = model.fit_predict(P)\n",
    "    return labels, model\n",
    "\n",
    "def run_rep(\n",
    "    fasta_path: str, labels_map: Dict[str,str],\n",
    "    alphabet: str, k: int, pseudocount: float,\n",
    "    target_clusters: int, random_state: int, n_init: int, max_iter: int\n",
    "):\n",
    "    fasta = read_fasta(fasta_path)\n",
    "    ids, P = build_feature_matrix(fasta, alphabet, k, pseudocount)\n",
    "\n",
    "    # align labels and filter\n",
    "    y = []\n",
    "    keep = []\n",
    "    for i, sid in enumerate(ids):\n",
    "        if sid in labels_map:\n",
    "            y.append(labels_map[sid])\n",
    "            keep.append(i)\n",
    "    if not keep:\n",
    "        raise ValueError(\"No IDs from FASTA matched labels.tsv\")\n",
    "    ids = [ids[i] for i in keep]\n",
    "    P = P[keep]\n",
    "    y = np.array(y)\n",
    "\n",
    "    uniq = {lab:i for i,lab in enumerate(sorted(set(y)))}\n",
    "    y_int = np.array([uniq[lab] for lab in y], dtype=int)\n",
    "    K = target_clusters or len(uniq)\n",
    "\n",
    "    # KMeans\n",
    "    labels_pred, model = kmeans_cluster(\n",
    "        P, K=K, n_init=n_init, max_iter=max_iter, random_state=random_state\n",
    "    )\n",
    "\n",
    "    ari = adjusted_rand_score(y_int, labels_pred)\n",
    "    nmi = normalized_mutual_info_score(y_int, labels_pred)\n",
    "\n",
    "    return {\n",
    "        \"ids\": ids,\n",
    "        \"P\": P,\n",
    "        \"y_int\": y_int,\n",
    "        \"y_str\": y,\n",
    "        \"labels_pred\": labels_pred,\n",
    "        \"K\": K,\n",
    "        \"ari\": ari,\n",
    "        \"nmi\": nmi,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957a560",
   "metadata": {},
   "source": [
    "## Entropy Rate Estimation with k-order Markov Models\n",
    "\n",
    "This experiment estimates the global entropy rate using k-order Markov models across multiple protein families. The analysis compares two different sequence representations:\n",
    "\n",
    "- **AA sequences**: Traditional amino acid sequences using the 20-letter alphabet\n",
    "- **DSR sequences**: Discrete structure representation using a K-token alphabet\n",
    "\n",
    "### Methodology\n",
    "\n",
    "The experiment uses a **backoff smoothing approach** with cross-validation to estimate entropy rates:\n",
    "\n",
    "1. **k-order Markov modeling**: Models conditional probabilities P(x|context) where context length = k\n",
    "2. **Backoff smoothing**: Handles sparse data by interpolating between different order models (k-gram → (k-1)-gram → ... → unigram)\n",
    "3. **5-fold cross-validation**: Splits sequences by family to avoid overfitting\n",
    "4. **Additive smoothing**: Regularizes maximum likelihood estimates with parameter α\n",
    "\n",
    "### Aggregation Strategy\n",
    "\n",
    "Results are aggregated at two levels:\n",
    "- **Macro-averaging**: Equal weight per family (family-centric view)\n",
    "- **Micro-averaging**: Weight by total tokens (sequence-centric view)\n",
    "\n",
    "This allows comparison of structural vs. sequence-based entropy rates across different context lengths (k=0,1,2,3,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare your data as dict: family -> list of sequences (strings)\n",
    "families_AA  = {\"PF00001\": [\"MKT...\", \"MSS...\"], \"PF00002\": [...], ...}\n",
    "families_DSR = {\"PF00001\": [\"QAB...\", \"QAA...\"], \"PF00002\": [...], ...}\n",
    "\n",
    "# 2) Define alphabets\n",
    "alphabet_AA  = list(\"ACDEFGHIKLMNPQRSTVWY\")       # or include 'X' if you keep it\n",
    "alphabet_DSR = [chr(i) for i in range(65, 65+K)]   # e.g., 'A'.. for K tokens, or your actual token set\n",
    "\n",
    "# 3) Run\n",
    "ks = (0,1,2,3,4)\n",
    "aa_res, aa_agg   = run_entropy_over_families(families_AA,  alphabet_AA,  k_values=ks, alpha=0.1, delta=None, folds=5)\n",
    "dsr_res, dsr_agg = run_entropy_over_families(families_DSR, alphabet_DSR, k_values=ks, alpha=0.1, delta=None, folds=5)\n",
    "\n",
    "# 4) Compare and plot:\n",
    "#   - aa_agg[k]['macro'] vs dsr_agg[k]['macro']\n",
    "#   - per-family deltas: {fam: dsr_res[k][fam]-aa_res[k][fam]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random, math\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ---------- utils\n",
    "def tokenize(seq: str, alpha_map: Dict[str,int]) -> List[int]:\n",
    "    return [alpha_map[c] for c in seq if c in alpha_map]\n",
    "\n",
    "def k_context(stream: List[int], k: int):\n",
    "    # yields (context_tuple, symbol) skipping first k\n",
    "    if k == 0:\n",
    "        for x in stream: yield (), x\n",
    "    else:\n",
    "        ctx = []\n",
    "        for x in stream:\n",
    "            ctx.append(x)\n",
    "            if len(ctx) > k:\n",
    "                yield tuple(ctx[-k-1:-1]), x\n",
    "\n",
    "def add_counts(counts, stream: List[int], k: int):\n",
    "    for ctx, x in k_context(stream, k):\n",
    "        counts[ctx][x] += 1\n",
    "\n",
    "def build_counts(seqs: List[List[int]], k: int, A: int):\n",
    "    counts = defaultdict(lambda: Counter())\n",
    "    total_tokens = 0\n",
    "    for s in seqs:\n",
    "        total_tokens += max(0, len(s)-k)\n",
    "        add_counts(counts, s, k)\n",
    "    return counts, total_tokens\n",
    "\n",
    "# ---------- smoothed conditional with simple backoff\n",
    "class BackoffKModel:\n",
    "    def __init__(self, counts_k_list, A: int, alpha=0.1, delta=None):\n",
    "        \"\"\"\n",
    "        counts_k_list: list where idx j holds counts for order j (0..k)\n",
    "        A: alphabet size\n",
    "        alpha: additive smoothing for MLE\n",
    "        delta: backoff strength; if None, set to A (alphabet size)\n",
    "        \"\"\"\n",
    "        self.counts = counts_k_list\n",
    "        self.A = A\n",
    "        self.alpha = alpha\n",
    "        self.delta = delta if delta is not None else A\n",
    "\n",
    "    def p_cond(self, ctx: Tuple[int,...], x: int) -> float:\n",
    "        k = len(ctx)\n",
    "        return self._p_k(k, ctx, x)\n",
    "\n",
    "    def _p_k(self, k: int, ctx: Tuple[int,...], x: int) -> float:\n",
    "        # base: unigram (order 0)\n",
    "        if k == 0:\n",
    "            cnts0 = self.counts[0][()]\n",
    "            num = cnts0.get(x, 0) + self.alpha\n",
    "            den = sum(cnts0.values()) + self.alpha * self.A\n",
    "            return num / den\n",
    "\n",
    "        # order-k MLE with smoothing\n",
    "        cnts_k = self.counts[k][ctx]\n",
    "        num = cnts_k.get(x, 0) + self.alpha\n",
    "        den = sum(cnts_k.values()) + self.alpha * self.A\n",
    "        p_mle = num / den\n",
    "\n",
    "        # backoff weight\n",
    "        gamma = self.delta / (self.delta + sum(cnts_k.values()))\n",
    "        # suffix context\n",
    "        suffix = ctx[1:]\n",
    "        return (1 - gamma) * p_mle + gamma * self._p_k(k-1, suffix, x)\n",
    "\n",
    "# ---------- cross-entropy (held-out)\n",
    "def cross_entropy_bits(model: BackoffKModel, seqs: List[List[int]], k: int) -> float:\n",
    "    tot_logloss = 0.0\n",
    "    tot_tokens = 0\n",
    "    for s in seqs:\n",
    "        # iterate tokens with contexts; boundaries reset by per-seq processing\n",
    "        for ctx, x in k_context(s, k):\n",
    "            p = model.p_cond(ctx, x)\n",
    "            tot_logloss += -math.log2(max(p, 1e-300))\n",
    "            tot_tokens += 1\n",
    "    return tot_logloss / max(1, tot_tokens)\n",
    "\n",
    "# ---------- 5-fold CV by sequence\n",
    "def entropy_rate_cv(seqs: List[List[int]], A: int, k: int, alpha=0.1, delta=None, folds=5, seed=0):\n",
    "    random.Random(seed).shuffle(seqs)\n",
    "    if len(seqs) < folds: folds = max(2, len(seqs))\n",
    "    fold_size = math.ceil(len(seqs)/folds)\n",
    "    losses = []\n",
    "    for f in range(folds):\n",
    "        test = seqs[f*fold_size:(f+1)*fold_size]\n",
    "        train = seqs[:f*fold_size] + seqs[(f+1)*fold_size:]\n",
    "        # build counts for orders 0..k\n",
    "        counts_k_list = []\n",
    "        for j in range(k+1):\n",
    "            counts_j, _ = build_counts(train, j, A)\n",
    "            counts_k_list.append(counts_j)\n",
    "        model = BackoffKModel(counts_k_list, A, alpha=alpha, delta=delta)\n",
    "        H = cross_entropy_bits(model, test, k)\n",
    "        losses.append((H, sum(max(0, len(s)-k) for s in test)))\n",
    "    # micro-average over folds\n",
    "    num = sum(H*n for H, n in losses)\n",
    "    den = sum(n for _, n in losses) or 1\n",
    "    return num/den\n",
    "\n",
    "# ---------- run over families and ks\n",
    "def run_entropy_over_families(\n",
    "    families: Dict[str, List[str]],\n",
    "    alphabet: List[str],\n",
    "    k_values=(0,1,2,3,4),\n",
    "    alpha=0.1, delta=None, folds=5, seed=0\n",
    "):\n",
    "    alpha_map = {c:i for i,c in enumerate(alphabet)}\n",
    "    A = len(alphabet)\n",
    "\n",
    "    # tokenize\n",
    "    fam_tok = {\n",
    "        fam: [tokenize(s, alpha_map) for s in seqs if len(tokenize(s, alpha_map))>0]\n",
    "        for fam, seqs in families.items()\n",
    "    }\n",
    "\n",
    "    results = {k:{} for k in k_values}\n",
    "    sizes   = {fam: sum(max(0,len(s)-max(k_values)) for s in seqs) for fam, seqs in fam_tok.items()}\n",
    "\n",
    "    for k in k_values:\n",
    "        fam_H = {}\n",
    "        for fam, seqs in fam_tok.items():\n",
    "            if len(seqs)==0: continue\n",
    "            Hk = entropy_rate_cv(seqs, A, k, alpha=alpha, delta=delta, folds=folds, seed=seed)\n",
    "            fam_H[fam] = Hk\n",
    "        results[k] = fam_H\n",
    "\n",
    "    # aggregates\n",
    "    aggregates = {}\n",
    "    for k in k_values:\n",
    "        fam_H = results[k]\n",
    "        fam_list = list(fam_H.items())\n",
    "        if not fam_list:\n",
    "            aggregates[k] = dict(macro=None, micro=None, n_families=0)\n",
    "            continue\n",
    "        macro = sum(h for _,h in fam_list)/len(fam_list)\n",
    "        # micro weight by total tokens (approximate using lengths at this k)\n",
    "        weights = {fam: sum(max(0, len(s)-k) for s in fam_tok[fam]) for fam,_ in fam_list}\n",
    "        num = sum(fam_H[fam]*weights[fam] for fam,_ in fam_list)\n",
    "        den = sum(weights.values()) or 1\n",
    "        micro = num/den\n",
    "        aggregates[k] = dict(macro=macro, micro=micro, n_families=len(fam_list))\n",
    "    return results, aggregates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de83d6",
   "metadata": {},
   "source": [
    "## Cross-Representation Information Analysis\n",
    "\n",
    "This experiment investigates the **mutual information** between amino acid (AA) and discrete structure representation (DSR) sequences by training probabilistic mappings in both directions using local sequence windows.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- **Windowed prediction**: Train neural networks to predict target tokens from source context windows (e.g., predict AA from 7-token DSR window)\n",
    "- **Bidirectional mapping**: Learn both DSR→AA and AA→DSR predictors to estimate cross-entropies\n",
    "- **Information bounds**: Derive mutual information lower bounds using H(target) - H(target|source_window)\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. **Complementarity**: How much structural information is captured in DSR that's not present in AA sequences?\n",
    "2. **Redundancy**: What fraction of sequence information is already encoded in structural representations?\n",
    "3. **Context dependence**: How does prediction accuracy vary with window size and MSA position?\n",
    "\n",
    "The analysis will reveal whether the two representations contain overlapping or complementary information about protein structure and function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Cross-representation information (Strategy 3)\n",
    "- Learn small probabilistic mappers DSR→AA and AA→DSR using windowed tokens\n",
    "- Estimate per-position and global H(AA), H(DSR), and conditional cross-entropies\n",
    "- Derive MI lower bounds: I_hat = H - H_hat(cond)\n",
    "\"\"\"\n",
    "\n",
    "import argparse, math, random\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------- FASTA utils -----------------------------\n",
    "\n",
    "def read_fasta(path: str) -> List[Tuple[str,str]]:\n",
    "    seqs = []\n",
    "    name = None\n",
    "    buf = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            if line.startswith('>'):\n",
    "                if name is not None:\n",
    "                    seqs.append((name, ''.join(buf)))\n",
    "                name = line[1:].strip()\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(line)\n",
    "    if name is not None:\n",
    "        seqs.append((name, ''.join(buf)))\n",
    "    return seqs\n",
    "\n",
    "def ensure_same_shape(msa1: List[str], msa2: List[str]):\n",
    "    assert len(msa1) == len(msa2), \"MSA row count mismatch between AA and DSR.\"\n",
    "    L1 = len(msa1[0])\n",
    "    for s in msa1:\n",
    "        assert len(s) == L1, \"AA MSA rows must have equal length.\"\n",
    "    for s in msa2:\n",
    "        assert len(s) == L1, \"DSR MSA must have same number of columns as AA MSA.\"\n",
    "\n",
    "# ------------------------- Sequence reweighting ------------------------\n",
    "\n",
    "def seq_identity(a: str, b: str) -> float:\n",
    "    matches = 0\n",
    "    comps = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == '-' or y == '-':\n",
    "            continue\n",
    "        comps += 1\n",
    "        if x == y:\n",
    "            matches += 1\n",
    "    if comps == 0: return 0.0\n",
    "    return matches / comps\n",
    "\n",
    "def reweight_sequences(msa: List[str], thresh: float = 0.8) -> np.ndarray:\n",
    "    n = len(msa)\n",
    "    w = np.zeros(n, dtype=float)\n",
    "    for i in range(n):\n",
    "        c = 0\n",
    "        for j in range(n):\n",
    "            if seq_identity(msa[i], msa[j]) >= thresh:\n",
    "                c += 1\n",
    "        w[i] = 1.0 / max(1, c)\n",
    "    if w.sum() > 0:\n",
    "        w *= (n / w.sum())\n",
    "    return w\n",
    "\n",
    "# ----------------------- Entropy (empirical) ---------------------------\n",
    "\n",
    "def shannon_entropy_from_counts(counts: np.ndarray, pseudocount: float = 0.0) -> float:\n",
    "    A = counts.shape[0]\n",
    "    total = counts.sum()\n",
    "    p = (counts + pseudocount) / (total + pseudocount * A)\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def per_position_entropy(msa: List[str], alphabet: List[str], weights: np.ndarray,\n",
    "                         occupancy_threshold: float = 0.7,\n",
    "                         pseudocount: float = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    A = len(alphabet)\n",
    "    if pseudocount is None:\n",
    "        pseudocount = 1.0 / A\n",
    "    alpha_index = {c:i for i,c in enumerate(alphabet)}\n",
    "    L = len(msa[0])\n",
    "    ent = np.full(L, np.nan, dtype=float)\n",
    "    mask = np.zeros(L, dtype=bool)\n",
    "\n",
    "    occ = np.zeros(L, dtype=float)\n",
    "    for i, seq in enumerate(msa):\n",
    "        g = np.fromiter((c != '-' for c in seq), dtype=bool, count=L)\n",
    "        occ += weights[i] * g.astype(float)\n",
    "    occ /= max(1e-9, weights.sum())\n",
    "\n",
    "    for t in range(L):\n",
    "        if occ[t] < occupancy_threshold: continue\n",
    "        counts = np.zeros(A, dtype=float)\n",
    "        for i, seq in enumerate(msa):\n",
    "            ch = seq[t]\n",
    "            if ch == '-' or ch not in alpha_index: continue\n",
    "            counts[alpha_index[ch]] += weights[i]\n",
    "        if counts.sum() <= 0: continue\n",
    "        ent[t] = shannon_entropy_from_counts(counts, pseudocount=pseudocount)\n",
    "        mask[t] = True\n",
    "    return ent, mask\n",
    "\n",
    "# ------------------------ Windowed samples -----------------------------\n",
    "\n",
    "def build_window_samples(\n",
    "    src_msa: List[str], tgt_msa: List[str], weights: np.ndarray,\n",
    "    src_alpha: List[str], tgt_alpha: List[str],\n",
    "    pos_mask: np.ndarray, win: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Build samples to predict target token at t from a src window around t.\n",
    "    - Requires no gaps in the src window or target at t.\n",
    "    - pos_mask picks columns to consider (e.g., occupancy intersection).\n",
    "    Returns X (one-hot per-window), y (int labels), w (sample weights), pos_idx (column indices).\n",
    "    \"\"\"\n",
    "    assert win % 2 == 1, \"Window must be odd size.\"\n",
    "    half = win // 2\n",
    "    A_src, A_tgt = len(src_alpha), len(tgt_alpha)\n",
    "    src_idx = {c:i for i,c in enumerate(src_alpha)}\n",
    "    tgt_idx = {c:i for i,c in enumerate(tgt_alpha)}\n",
    "\n",
    "    L = len(src_msa[0])\n",
    "    n = len(src_msa)\n",
    "\n",
    "    feats = []\n",
    "    labels = []\n",
    "    sw = []\n",
    "    pos_idx = []\n",
    "\n",
    "    # precompute valid (non-gap and in alphabet) masks\n",
    "    src_valid = np.array([[ (c!='-' and c in src_idx) for c in row] for row in src_msa], dtype=bool)\n",
    "    tgt_valid = np.array([[ (c!='-' and c in tgt_idx) for c in row] for row in tgt_msa], dtype=bool)\n",
    "\n",
    "    for i in range(n):\n",
    "        w_i = weights[i]\n",
    "        src_row = src_msa[i]\n",
    "        tgt_row = tgt_msa[i]\n",
    "        for t in range(half, L-half):\n",
    "            if not pos_mask[t]: continue\n",
    "            if not tgt_valid[i, t]: continue\n",
    "            # window must be fully valid in src\n",
    "            if not src_valid[i, t-half:t+half+1].all(): continue\n",
    "\n",
    "            # one-hot encode window: concat per-position one-hots\n",
    "            v = np.zeros((win, A_src), dtype=np.float32)\n",
    "            for j, col in enumerate(range(t-half, t+half+1)):\n",
    "                s = src_row[col]\n",
    "                v[j, src_idx[s]] = 1.0\n",
    "            feats.append(v.reshape(-1))  # (win*A_src,)\n",
    "            labels.append(tgt_idx[tgt_row[t]])\n",
    "            sw.append(w_i)\n",
    "            pos_idx.append(t)\n",
    "\n",
    "    if len(feats) == 0:\n",
    "        return (np.zeros((0, win*A_src), dtype=np.float32),\n",
    "                np.zeros((0,), dtype=np.int64),\n",
    "                np.zeros((0,), dtype=np.float32),\n",
    "                np.zeros((0,), dtype=np.int32))\n",
    "    X = np.stack(feats, axis=0)\n",
    "    y = np.array(labels, dtype=np.int64)\n",
    "    w = np.array(sw, dtype=np.float32)\n",
    "    pos_idx = np.array(pos_idx, dtype=np.int32)\n",
    "    return X, y, w, pos_idx\n",
    "\n",
    "# --------------------------- Torch model --------------------------------\n",
    "\n",
    "class SoftmaxLinear(nn.Module):\n",
    "    def __init__(self, D_in: int, C_out: int, bias=True):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(D_in, C_out, bias=bias)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)  # logits\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        self.w = torch.from_numpy(w)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "def split_by_sequence_indices(n_seq: int, seed=0, ratios=(0.7, 0.15, 0.15)):\n",
    "    idx = list(range(n_seq))\n",
    "    random.Random(seed).shuffle(idx)\n",
    "    n_train = int(ratios[0]*n_seq)\n",
    "    n_val   = int(ratios[1]*n_seq)\n",
    "    train_ids = set(idx[:n_train])\n",
    "    val_ids   = set(idx[n_train:n_train+n_val])\n",
    "    test_ids  = set(idx[n_train+n_val:])\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "def mask_samples_by_seqpos(seq_pos_of_sample: List[int], seq_ids_set: set):\n",
    "    return np.array([sp in seq_ids_set for sp in seq_pos_of_sample], dtype=bool)\n",
    "\n",
    "def train_softmax(\n",
    "    X_train, y_train, w_train,\n",
    "    X_val,   y_val,   w_val,\n",
    "    D_in, C_out, lr=1e-2, epochs=20, bs=4096, weight_decay=1e-4, device=\"cpu\"\n",
    "):\n",
    "    model = SoftmaxLinear(D_in, C_out).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    ds_tr = NumpyDataset(X_train, y_train, w_train)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=bs, shuffle=True, drop_last=False)\n",
    "\n",
    "    def eval_ce(X, y, w):\n",
    "        if X.shape[0] == 0: return float('nan')\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.from_numpy(X).to(device)\n",
    "            y_t = torch.from_numpy(y).to(device)\n",
    "            w_t = torch.from_numpy(w).to(device)\n",
    "            logits = model(X_t)\n",
    "            ce = nn.functional.cross_entropy(logits, y_t, reduction='none')\n",
    "            return float((ce * w_t).sum().item() / max(1e-9, w_t.sum().item()))\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    patience, bad = 4, 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb, wb in dl_tr:\n",
    "            xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "            logits = model(xb)\n",
    "            ce = nn.functional.cross_entropy(logits, yb, reduction='none')\n",
    "            loss = (ce * wb).sum() / (wb.sum() + 1e-9)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        val_ce = eval_ce(X_val, y_val, w_val)\n",
    "        if val_ce < best_val - 1e-5:\n",
    "            best_val = val_ce\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def predict_log_probs(model: nn.Module, X: np.ndarray, bs=8192, device=\"cpu\") -> np.ndarray:\n",
    "    if X.shape[0] == 0: return np.zeros((0, model.lin.out_features), dtype=np.float32)\n",
    "    model.eval()\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X.shape[0], bs):\n",
    "            xb = torch.from_numpy(X[i:i+bs]).to(device)\n",
    "            logits = model(xb)\n",
    "            logp = nn.functional.log_softmax(logits, dim=-1)\n",
    "            out.append(logp.detach().cpu().numpy())\n",
    "    return np.vstack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5949766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CROSS-FAMILY RESULTS SUMMARY ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-FAMILY BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display results for each family\n",
    "for family_name in all_results.keys():\n",
    "    print(f\"\\n  - {family_name}: {all_results[family_name]['n_structures']} structures\")\n",
    "\n",
    "# Save comprehensive results to JSON\n",
    "import json\n",
    "results_path = os.path.join(OUTPUT_DIR, 'all_families_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    serializable_results = convert_to_serializable(all_results)\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {results_path}\")\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "\n",
    "# Collect all results into structured format\n",
    "summary_data = []\n",
    "\n",
    "for family_name, family_results in all_results.items():\n",
    "    for model_name in family_results['models'] + ['AA']:\n",
    "        row = {\n",
    "            'Family': family_name,\n",
    "            'Model': model_name,\n",
    "            'N_Structures': family_results['n_structures']\n",
    "        }\n",
    "        \n",
    "        # K-mer discrimination scores\n",
    "        if model_name in family_results['k_mer_discrimination']:\n",
    "            kmer_results = family_results['k_mer_discrimination'][model_name]\n",
    "            for k_val in ['k=1', 'k=2', 'k=3', 'k=4']:\n",
    "                if k_val in kmer_results:\n",
    "                    row[f'KMer_{k_val}'] = kmer_results[k_val]\n",
    "        \n",
    "        # Entropy rates\n",
    "        if model_name in family_results['entropy_rates']:\n",
    "            entropy_results = family_results['entropy_rates'][model_name]\n",
    "            for order in ['order=0', 'order=1', 'order=2', 'order=3']:\n",
    "                if order in entropy_results:\n",
    "                    row[f'Entropy_{order}'] = entropy_results[order]\n",
    "        \n",
    "        # Per-position entropy\n",
    "        if model_name in family_results['per_position_entropy']:\n",
    "            pp_entropy = family_results['per_position_entropy'][model_name]\n",
    "            row['PerPos_Entropy_Mean'] = pp_entropy['mean']\n",
    "            row['PerPos_Entropy_Std'] = pp_entropy['std']\n",
    "        \n",
    "        # Cross-representation MI\n",
    "        if model_name in family_results['cross_representation_mi']:\n",
    "            mi_results = family_results['cross_representation_mi'][model_name]\n",
    "            row['CrossMI_R2'] = mi_results['r2_score']\n",
    "            row['CrossMI_Spearman'] = mi_results['spearman_corr']\n",
    "        \n",
    "        summary_data.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'all_families_results.csv')\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ CSV results saved to: {csv_path}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"─\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"─\"*70)\n",
    "\n",
    "# Group by model and show average performance across families\n",
    "print(\"\\n1. K-MER DISCRIMINATION (k=2) - Average Silhouette Score:\")\n",
    "kmer_cols = [c for c in results_df.columns if 'KMer_k=2' in c]\n",
    "if kmer_cols:\n",
    "    kmer_summary = results_df.groupby('Model')['KMer_k=2'].mean().sort_values(ascending=False)\n",
    "    for model, score in kmer_summary.items():\n",
    "        print(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n2. ENTROPY RATE (1st-order) - Average Entropy:\")\n",
    "entropy_cols = [c for c in results_df.columns if 'Entropy_order=1' in c]\n",
    "if entropy_cols:\n",
    "    entropy_summary = results_df.groupby('Model')['Entropy_order=1'].mean().sort_values()\n",
    "    for model, score in entropy_summary.items():\n",
    "        print(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n3. PER-POSITION ENTROPY - Average Mean Entropy:\")\n",
    "if 'PerPos_Entropy_Mean' in results_df.columns:\n",
    "    pp_summary = results_df.groupby('Model')['PerPos_Entropy_Mean'].mean().sort_values()\n",
    "    for model, score in pp_summary.items():\n",
    "        print(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n4. CROSS-REPRESENTATION MI - Average R² Score:\")\n",
    "if 'CrossMI_R2' in results_df.columns:\n",
    "    mi_summary = results_df.groupby('Model')['CrossMI_R2'].mean().sort_values(ascending=False)\n",
    "    for model, score in mi_summary.items():\n",
    "        print(f\"   {model:20s}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ BENCHMARK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256102f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MULTI-FAMILY VISUALIZATION ====================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Multi-Family Multi-Model Benchmark Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Define colors for models\n",
    "unique_models = results_df['Model'].unique()\n",
    "model_colors = {model: color for model, color in zip(unique_models, sns.color_palette(\"husl\", len(unique_models)))}\n",
    "\n",
    "# ========== 1. K-MER DISCRIMINATION ACROSS FAMILIES ==========\n",
    "ax = axes[0, 0]\n",
    "\n",
    "for model in unique_models:\n",
    "    model_data = results_df[results_df['Model'] == model]\n",
    "    if 'KMer_k=2' in model_data.columns:\n",
    "        families = model_data['Family'].values\n",
    "        scores = model_data['KMer_k=2'].values\n",
    "        ax.plot(families, scores, marker='o', label=model, \n",
    "                color=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Silhouette Score (k=2)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('K-mer Fold Discrimination', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ========== 2. ENTROPY RATES ACROSS FAMILIES ==========\n",
    "ax = axes[0, 1]\n",
    "\n",
    "for model in unique_models:\n",
    "    model_data = results_df[results_df['Model'] == model]\n",
    "    if 'Entropy_order=1' in model_data.columns:\n",
    "        families = model_data['Family'].values\n",
    "        entropies = model_data['Entropy_order=1'].values\n",
    "        ax.plot(families, entropies, marker='s', label=model,\n",
    "                color=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Entropy Rate (1st-order)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Markov Entropy Estimation', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ========== 3. PER-POSITION ENTROPY ACROSS FAMILIES ==========\n",
    "ax = axes[1, 0]\n",
    "\n",
    "if 'PerPos_Entropy_Mean' in results_df.columns:\n",
    "    for model in unique_models:\n",
    "        model_data = results_df[results_df['Model'] == model]\n",
    "        families = model_data['Family'].values\n",
    "        entropies = model_data['PerPos_Entropy_Mean'].values\n",
    "        ax.plot(families, entropies, marker='^', label=model,\n",
    "                color=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Mean Entropy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Per-Position Entropy', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ========== 4. CROSS-REPRESENTATION MI ACROSS FAMILIES ==========\n",
    "ax = axes[1, 1]\n",
    "\n",
    "if 'CrossMI_R2' in results_df.columns:\n",
    "    for model in unique_models:\n",
    "        if model != 'AA':  # Skip AA baseline for cross-representation\n",
    "            model_data = results_df[results_df['Model'] == model]\n",
    "            families = model_data['Family'].values\n",
    "            r2_scores = model_data['CrossMI_R2'].values\n",
    "            ax.plot(families, r2_scores, marker='D', label=model,\n",
    "                    color=model_colors[model], linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('Family', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Cross-Representation MI (FoldTree2→AA)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(OUTPUT_DIR, 'multi_family_benchmark_comparison.png')\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Visualization saved to: {fig_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ==================== HEATMAP: MODEL PERFORMANCE BY FAMILY ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HEATMAP: MODEL PERFORMANCE BY FAMILY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "if 'KMer_k=2' in results_df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Performance Heatmaps by Family', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # K-mer discrimination heatmap\n",
    "    ax = axes[0, 0]\n",
    "    pivot_kmer = results_df.pivot(index='Model', columns='Family', values='KMer_k=2')\n",
    "    sns.heatmap(pivot_kmer, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax, cbar_kws={'label': 'Score'})\n",
    "    ax.set_title('K-mer Discrimination (k=2)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Family', fontsize=12)\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    # Entropy rate heatmap\n",
    "    if 'Entropy_order=1' in results_df.columns:\n",
    "        ax = axes[0, 1]\n",
    "        pivot_entropy = results_df.pivot(index='Model', columns='Family', values='Entropy_order=1')\n",
    "        sns.heatmap(pivot_entropy, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Entropy'})\n",
    "        ax.set_title('Entropy Rate (1st-order)', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Family', fontsize=12)\n",
    "        ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    # Per-position entropy heatmap\n",
    "    if 'PerPos_Entropy_Mean' in results_df.columns:\n",
    "        ax = axes[1, 0]\n",
    "        pivot_pp = results_df.pivot(index='Model', columns='Family', values='PerPos_Entropy_Mean')\n",
    "        sns.heatmap(pivot_pp, annot=True, fmt='.3f', cmap='Purples', ax=ax, cbar_kws={'label': 'Entropy'})\n",
    "        ax.set_title('Per-Position Entropy', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Family', fontsize=12)\n",
    "        ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    # Cross-MI heatmap\n",
    "    if 'CrossMI_R2' in results_df.columns:\n",
    "        ax = axes[1, 1]\n",
    "        # Filter out AA for cross-representation\n",
    "        mi_data = results_df[results_df['Model'] != 'AA']\n",
    "        pivot_mi = mi_data.pivot(index='Model', columns='Family', values='CrossMI_R2')\n",
    "        sns.heatmap(pivot_mi, annot=True, fmt='.3f', cmap='Greens', ax=ax, cbar_kws={'label': 'R²'})\n",
    "        ax.set_title('Cross-Representation MI', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Family', fontsize=12)\n",
    "        ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    heatmap_path = os.path.join(OUTPUT_DIR, 'model_family_heatmaps.png')\n",
    "    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Heatmaps saved to: {heatmap_path}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL VISUALIZATIONS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
