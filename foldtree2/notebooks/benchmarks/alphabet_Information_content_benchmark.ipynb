{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c660247e",
   "metadata": {},
   "source": [
    "# Information-Theoretic Analysis of Protein Sequence Representations\n",
    "\n",
    "This notebook compares the information content between traditional amino acid sequences and FoldTree2 discrete structure representations. The goal is to quantify whether our discrete structural alphabet successfully captures and preserves structural information that complements or enhances sequence-based representations.\n",
    "\n",
    "## Key Analyses\n",
    "\n",
    "- **Sequence-level**: Direct comparison of entropy rates between AA and DSR sequences\n",
    "- **K-mer analysis**: Information content in local sequence contexts using k-mer frequency distributions\n",
    "- **Family-level MSAs**: Positional entropy analysis and mutual information between aligned positions\n",
    "- **Cross-validation**: Robust entropy rate estimation using backoff smoothing and k-order Markov models\n",
    "\n",
    "The analyses will reveal whether the FoldTree2 representation provides a more compressed encoding of structural information compared to amino acid sequences alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p): \n",
    "    p = p[p>0]\n",
    "    return -(p*np.log2(p)).sum()\n",
    "\n",
    "def empirical_probs(tokens, alphabet):\n",
    "    counts = np.bincount(tokens, minlength=len(alphabet)).astype(float)\n",
    "    return (counts + 1.0/len(alphabet)) / (counts.sum() + 1.0)\n",
    "\n",
    "def mutual_info(x, y, ax, ay):\n",
    "    # x,y are integer-encoded; ax,ay are alphabet sizes\n",
    "    px = empirical_probs(x, np.arange(ax))\n",
    "    py = empirical_probs(y, np.arange(ay))\n",
    "    joint = np.zeros((ax, ay))\n",
    "    for xi, yi in zip(x, y):\n",
    "        joint[xi, yi] += 1\n",
    "    joint = (joint + 1.0/(ax*ay)) / (len(x) + 1.0)\n",
    "    mi = 0.0\n",
    "    for i in range(ax):\n",
    "        for j in range(ay):\n",
    "            if joint[i,j] > 0:\n",
    "                mi += joint[i,j]*np.log2(joint[i,j]/(px[i]*py[j]))\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_same_shape(msa1: List[str], msa2: List[str]):\n",
    "    assert len(msa1) == len(msa2), \"MSA row count mismatch between AA and DSR.\"\n",
    "    L1 = len(msa1[0])\n",
    "    for s in msa1:\n",
    "        assert len(s) == L1, \"AA MSA rows must have equal length.\"\n",
    "    for s in msa2:\n",
    "        assert len(s) == L1, \"DSR MSA must have same number of columns as AA MSA.\"\n",
    "\n",
    "# ------------------------- Sequence reweighting ------------------------\n",
    "\n",
    "def seq_identity(a: str, b: str) -> float:\n",
    "    # pairwise identity over non-gap positions shared by both\n",
    "    matches = 0\n",
    "    comps = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == '-' or y == '-':\n",
    "            continue\n",
    "        comps += 1\n",
    "        if x == y:\n",
    "            matches += 1\n",
    "    if comps == 0:\n",
    "        return 0.0\n",
    "    return matches / comps\n",
    "\n",
    "def reweight_sequences(msa: List[str], thresh: float = 0.8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Henikoff-like simple sequence reweighting by identity threshold.\n",
    "    w_i = 1 / |{j: pid(i,j) >= thresh}|\n",
    "    \"\"\"\n",
    "    n = len(msa)\n",
    "    w = np.zeros(n, dtype=float)\n",
    "    for i in range(n):\n",
    "        c = 0\n",
    "        for j in range(n):\n",
    "            if seq_identity(msa[i], msa[j]) >= thresh:\n",
    "                c += 1\n",
    "        w[i] = 1.0 / max(1, c)\n",
    "    # normalize so sum weights ~ n (convention)\n",
    "    w *= (n / w.sum()) if w.sum() > 0 else 1.0\n",
    "    return w\n",
    "\n",
    "# ----------------------- Per-position entropy -------------------------\n",
    "\n",
    "def shannon_entropy_from_counts(counts: np.ndarray, pseudocount: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    counts: array of size A (weighted counts over alphabet)\n",
    "    returns entropy in bits\n",
    "    \"\"\"\n",
    "    A = counts.shape[0]\n",
    "    total = counts.sum()\n",
    "    p = (counts + pseudocount) / (total + pseudocount * A)\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def per_position_entropy(\n",
    "    msa: List[str],\n",
    "    alphabet: List[str],\n",
    "    weights: np.ndarray,\n",
    "    occupancy_threshold: float = 0.7,\n",
    "    pseudocount: float = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns (entropy_per_position, valid_mask) arrays of length L\n",
    "    \"\"\"\n",
    "    A = len(alphabet)\n",
    "    alpha_index = {c: i for i, c in enumerate(alphabet)}\n",
    "    L = len(msa[0])\n",
    "    n = len(msa)\n",
    "\n",
    "    if pseudocount is None:\n",
    "        pseudocount = 1.0 / A\n",
    "\n",
    "    ent = np.full(L, np.nan, dtype=float)\n",
    "    mask = np.zeros(L, dtype=bool)\n",
    "\n",
    "    # occupancy per column (weighted fraction non-gap)\n",
    "    occ = np.zeros(L, dtype=float)\n",
    "    for i, seq in enumerate(msa):\n",
    "        g = np.fromiter((c != '-' for c in seq), dtype=bool, count=L)\n",
    "        occ += weights[i] * g.astype(float)\n",
    "    occ /= weights.sum()\n",
    "\n",
    "    for t in range(L):\n",
    "        if occ[t] < occupancy_threshold:\n",
    "            continue\n",
    "        counts = np.zeros(A, dtype=float)\n",
    "        for i, seq in enumerate(msa):\n",
    "            ch = seq[t]\n",
    "            if ch == '-':\n",
    "                continue\n",
    "            if ch not in alpha_index:\n",
    "                # skip unknowns\n",
    "                continue\n",
    "            counts[alpha_index[ch]] += weights[i]\n",
    "        if counts.sum() <= 0:\n",
    "            continue\n",
    "        ent[t] = shannon_entropy_from_counts(counts, pseudocount=pseudocount)\n",
    "        mask[t] = True\n",
    "\n",
    "    return ent, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note. try both alignment using foldmason, ft2 and regular mafft...\n",
    "# the aligner can also be an argument for a particular character model\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bcbf5",
   "metadata": {},
   "source": [
    "## K-mer Frequency Analysis for Fold Discrimination\n",
    "\n",
    "This experiment analyzes the discriminative power of FoldTree2 (DSR) versus amino acid representations by examining k-mer frequency distributions. The goal is to determine which alphabet better distinguishes between different protein folds.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- **K-mer extraction**: Compute frequency distributions of subsequences of length k for both AA and DSR sequences\n",
    "- **Within-family distances**: Calculate Jensen-Shannon divergences between k-mer distributions of sequences within the same fold family\n",
    "- **Between-family distances**: Measure k-mer distribution differences across distinct fold families\n",
    "- **Discrimination analysis**: Compare the separation between within-family (similar folds) and between-family (different folds) distance distributions\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. **Fold specificity**: Does the FoldTree2 alphabet capture fold-specific sequence patterns more effectively than amino acid sequences?\n",
    "2. **Optimal k-mer length**: What subsequence length provides the best discrimination for each representation?\n",
    "3. **Distribution separation**: Which alphabet shows clearer separation between intra-fold similarity and inter-fold dissimilarity?\n",
    "\n",
    "The analysis will reveal whether structural alphabets provide enhanced discriminative power for protein fold classification compared to traditional sequence-based representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def kmer_freqs(seq, k, alpha_map, A):\n",
    "    idx = [alpha_map[c] for c in seq if c in alpha_map]\n",
    "    if len(idx) < k: return np.ones(A**k)/(A**k)\n",
    "    counts = np.zeros(A**k)\n",
    "    base = A**np.arange(k)[::-1]\n",
    "    for t in range(len(idx)-k+1):\n",
    "        code = 0\n",
    "        for j in range(k):\n",
    "            code = code*A + idx[t+j]\n",
    "        counts[code]+=1\n",
    "    p = counts + 1e-9\n",
    "    p /= p.sum()\n",
    "    return p\n",
    "\n",
    "def jsd(p, q):\n",
    "    m = 0.5*(p+q)\n",
    "    def KL(a,b): \n",
    "        mask = (a>0)\n",
    "        return (a[mask]*np.log2(a[mask]/b[mask])).sum()\n",
    "    return 0.5*(KL(p,m)+KL(q,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afd397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# ---------------------------- k-mer utils ------------------------------\n",
    "\n",
    "def build_alpha_index(alphabet: str) -> Dict[str, int]:\n",
    "    return {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "def kmers_counts(seq: str, k: int, alpha_idx: Dict[str,int], A: int) -> np.ndarray:\n",
    "    \"\"\"Return counts vector of length A^k for overlapping k-mers in seq (skip k-mer if any unseen char).\"\"\"\n",
    "    L = len(seq)\n",
    "    if L < k or k == 0:\n",
    "        return np.zeros(A**max(k,1), dtype=np.float64)\n",
    "    v = np.zeros(A**k, dtype=np.float64)\n",
    "    code = -1\n",
    "    for i, ch in enumerate(seq):\n",
    "        if ch not in alpha_idx:\n",
    "            code = -1\n",
    "        else:\n",
    "            x = alpha_idx[ch]\n",
    "            if code == -1:\n",
    "                if i >= k-1:\n",
    "                    ok = True\n",
    "                    code_tmp = 0\n",
    "                    for j in range(i-k+1, i+1):\n",
    "                        c2 = seq[j]\n",
    "                        if c2 not in alpha_idx:\n",
    "                            ok = False; break\n",
    "                        code_tmp = code_tmp * A + alpha_idx[c2]\n",
    "                    if ok:\n",
    "                        code = code_tmp\n",
    "                        v[code] += 1.0\n",
    "            else:\n",
    "                code = (code % (A**(k-1))) * A + x\n",
    "                v[code] += 1.0\n",
    "    return v\n",
    "\n",
    "def kmer_prob(seq: str, k: int, alphabet: str, pseudocount: float = 1e-9) -> np.ndarray:\n",
    "    A = len(alphabet)\n",
    "    idx = build_alpha_index(alphabet)\n",
    "    c = kmers_counts(seq, k, idx, A)\n",
    "    total = c.sum()\n",
    "    if total == 0:\n",
    "        # no valid k-mers: return uniform tiny distribution\n",
    "        p = np.ones(A**k, dtype=np.float64)\n",
    "        p /= p.sum()\n",
    "        return p\n",
    "    p = (c + pseudocount) / (total + pseudocount * c.shape[0])\n",
    "    return p\n",
    "\n",
    "def build_feature_matrix(fasta: List[Tuple[str,str]], alphabet: str, k: int, pseudocount: float):\n",
    "    ids = [name for name,_ in fasta]\n",
    "    seqs = [seq.upper() for _,seq in fasta]\n",
    "    P = np.vstack([kmer_prob(s, k, alphabet, pseudocount=pseudocount) for s in seqs])\n",
    "    return ids, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- KMeans + eval -----------------------------\n",
    "\n",
    "def kmeans_cluster(P: np.ndarray, K: int, n_init: int = 20, max_iter: int = 300, random_state: int = 0):\n",
    "    # KMeans on probability vectors (Euclidean). For probability geometry, you can also sqrt-transform (Hellinger).\n",
    "    model = KMeans(n_clusters=K, n_init=n_init, max_iter=max_iter, random_state=random_state)\n",
    "    labels = model.fit_predict(P)\n",
    "    return labels, model\n",
    "\n",
    "def run_rep(\n",
    "    fasta_path: str, labels_map: Dict[str,str],\n",
    "    alphabet: str, k: int, pseudocount: float,\n",
    "    target_clusters: int, random_state: int, n_init: int, max_iter: int\n",
    "):\n",
    "    fasta = read_fasta(fasta_path)\n",
    "    ids, P = build_feature_matrix(fasta, alphabet, k, pseudocount)\n",
    "\n",
    "    # align labels and filter\n",
    "    y = []\n",
    "    keep = []\n",
    "    for i, sid in enumerate(ids):\n",
    "        if sid in labels_map:\n",
    "            y.append(labels_map[sid])\n",
    "            keep.append(i)\n",
    "    if not keep:\n",
    "        raise ValueError(\"No IDs from FASTA matched labels.tsv\")\n",
    "    ids = [ids[i] for i in keep]\n",
    "    P = P[keep]\n",
    "    y = np.array(y)\n",
    "\n",
    "    uniq = {lab:i for i,lab in enumerate(sorted(set(y)))}\n",
    "    y_int = np.array([uniq[lab] for lab in y], dtype=int)\n",
    "    K = target_clusters or len(uniq)\n",
    "\n",
    "    # KMeans\n",
    "    labels_pred, model = kmeans_cluster(\n",
    "        P, K=K, n_init=n_init, max_iter=max_iter, random_state=random_state\n",
    "    )\n",
    "\n",
    "    ari = adjusted_rand_score(y_int, labels_pred)\n",
    "    nmi = normalized_mutual_info_score(y_int, labels_pred)\n",
    "\n",
    "    return {\n",
    "        \"ids\": ids,\n",
    "        \"P\": P,\n",
    "        \"y_int\": y_int,\n",
    "        \"y_str\": y,\n",
    "        \"labels_pred\": labels_pred,\n",
    "        \"K\": K,\n",
    "        \"ari\": ari,\n",
    "        \"nmi\": nmi,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957a560",
   "metadata": {},
   "source": [
    "## Entropy Rate Estimation with k-order Markov Models\n",
    "\n",
    "This experiment estimates the global entropy rate using k-order Markov models across multiple protein families. The analysis compares two different sequence representations:\n",
    "\n",
    "- **AA sequences**: Traditional amino acid sequences using the 20-letter alphabet\n",
    "- **DSR sequences**: Discrete structure representation using a K-token alphabet\n",
    "\n",
    "### Methodology\n",
    "\n",
    "The experiment uses a **backoff smoothing approach** with cross-validation to estimate entropy rates:\n",
    "\n",
    "1. **k-order Markov modeling**: Models conditional probabilities P(x|context) where context length = k\n",
    "2. **Backoff smoothing**: Handles sparse data by interpolating between different order models (k-gram → (k-1)-gram → ... → unigram)\n",
    "3. **5-fold cross-validation**: Splits sequences by family to avoid overfitting\n",
    "4. **Additive smoothing**: Regularizes maximum likelihood estimates with parameter α\n",
    "\n",
    "### Aggregation Strategy\n",
    "\n",
    "Results are aggregated at two levels:\n",
    "- **Macro-averaging**: Equal weight per family (family-centric view)\n",
    "- **Micro-averaging**: Weight by total tokens (sequence-centric view)\n",
    "\n",
    "This allows comparison of structural vs. sequence-based entropy rates across different context lengths (k=0,1,2,3,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare your data as dict: family -> list of sequences (strings)\n",
    "families_AA  = {\"PF00001\": [\"MKT...\", \"MSS...\"], \"PF00002\": [...], ...}\n",
    "families_DSR = {\"PF00001\": [\"QAB...\", \"QAA...\"], \"PF00002\": [...], ...}\n",
    "\n",
    "# 2) Define alphabets\n",
    "alphabet_AA  = list(\"ACDEFGHIKLMNPQRSTVWY\")       # or include 'X' if you keep it\n",
    "alphabet_DSR = [chr(i) for i in range(65, 65+K)]   # e.g., 'A'.. for K tokens, or your actual token set\n",
    "\n",
    "# 3) Run\n",
    "ks = (0,1,2,3,4)\n",
    "aa_res, aa_agg   = run_entropy_over_families(families_AA,  alphabet_AA,  k_values=ks, alpha=0.1, delta=None, folds=5)\n",
    "dsr_res, dsr_agg = run_entropy_over_families(families_DSR, alphabet_DSR, k_values=ks, alpha=0.1, delta=None, folds=5)\n",
    "\n",
    "# 4) Compare and plot:\n",
    "#   - aa_agg[k]['macro'] vs dsr_agg[k]['macro']\n",
    "#   - per-family deltas: {fam: dsr_res[k][fam]-aa_res[k][fam]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random, math\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ---------- utils\n",
    "def tokenize(seq: str, alpha_map: Dict[str,int]) -> List[int]:\n",
    "    return [alpha_map[c] for c in seq if c in alpha_map]\n",
    "\n",
    "def k_context(stream: List[int], k: int):\n",
    "    # yields (context_tuple, symbol) skipping first k\n",
    "    if k == 0:\n",
    "        for x in stream: yield (), x\n",
    "    else:\n",
    "        ctx = []\n",
    "        for x in stream:\n",
    "            ctx.append(x)\n",
    "            if len(ctx) > k:\n",
    "                yield tuple(ctx[-k-1:-1]), x\n",
    "\n",
    "def add_counts(counts, stream: List[int], k: int):\n",
    "    for ctx, x in k_context(stream, k):\n",
    "        counts[ctx][x] += 1\n",
    "\n",
    "def build_counts(seqs: List[List[int]], k: int, A: int):\n",
    "    counts = defaultdict(lambda: Counter())\n",
    "    total_tokens = 0\n",
    "    for s in seqs:\n",
    "        total_tokens += max(0, len(s)-k)\n",
    "        add_counts(counts, s, k)\n",
    "    return counts, total_tokens\n",
    "\n",
    "# ---------- smoothed conditional with simple backoff\n",
    "class BackoffKModel:\n",
    "    def __init__(self, counts_k_list, A: int, alpha=0.1, delta=None):\n",
    "        \"\"\"\n",
    "        counts_k_list: list where idx j holds counts for order j (0..k)\n",
    "        A: alphabet size\n",
    "        alpha: additive smoothing for MLE\n",
    "        delta: backoff strength; if None, set to A (alphabet size)\n",
    "        \"\"\"\n",
    "        self.counts = counts_k_list\n",
    "        self.A = A\n",
    "        self.alpha = alpha\n",
    "        self.delta = delta if delta is not None else A\n",
    "\n",
    "    def p_cond(self, ctx: Tuple[int,...], x: int) -> float:\n",
    "        k = len(ctx)\n",
    "        return self._p_k(k, ctx, x)\n",
    "\n",
    "    def _p_k(self, k: int, ctx: Tuple[int,...], x: int) -> float:\n",
    "        # base: unigram (order 0)\n",
    "        if k == 0:\n",
    "            cnts0 = self.counts[0][()]\n",
    "            num = cnts0.get(x, 0) + self.alpha\n",
    "            den = sum(cnts0.values()) + self.alpha * self.A\n",
    "            return num / den\n",
    "\n",
    "        # order-k MLE with smoothing\n",
    "        cnts_k = self.counts[k][ctx]\n",
    "        num = cnts_k.get(x, 0) + self.alpha\n",
    "        den = sum(cnts_k.values()) + self.alpha * self.A\n",
    "        p_mle = num / den\n",
    "\n",
    "        # backoff weight\n",
    "        gamma = self.delta / (self.delta + sum(cnts_k.values()))\n",
    "        # suffix context\n",
    "        suffix = ctx[1:]\n",
    "        return (1 - gamma) * p_mle + gamma * self._p_k(k-1, suffix, x)\n",
    "\n",
    "# ---------- cross-entropy (held-out)\n",
    "def cross_entropy_bits(model: BackoffKModel, seqs: List[List[int]], k: int) -> float:\n",
    "    tot_logloss = 0.0\n",
    "    tot_tokens = 0\n",
    "    for s in seqs:\n",
    "        # iterate tokens with contexts; boundaries reset by per-seq processing\n",
    "        for ctx, x in k_context(s, k):\n",
    "            p = model.p_cond(ctx, x)\n",
    "            tot_logloss += -math.log2(max(p, 1e-300))\n",
    "            tot_tokens += 1\n",
    "    return tot_logloss / max(1, tot_tokens)\n",
    "\n",
    "# ---------- 5-fold CV by sequence\n",
    "def entropy_rate_cv(seqs: List[List[int]], A: int, k: int, alpha=0.1, delta=None, folds=5, seed=0):\n",
    "    random.Random(seed).shuffle(seqs)\n",
    "    if len(seqs) < folds: folds = max(2, len(seqs))\n",
    "    fold_size = math.ceil(len(seqs)/folds)\n",
    "    losses = []\n",
    "    for f in range(folds):\n",
    "        test = seqs[f*fold_size:(f+1)*fold_size]\n",
    "        train = seqs[:f*fold_size] + seqs[(f+1)*fold_size:]\n",
    "        # build counts for orders 0..k\n",
    "        counts_k_list = []\n",
    "        for j in range(k+1):\n",
    "            counts_j, _ = build_counts(train, j, A)\n",
    "            counts_k_list.append(counts_j)\n",
    "        model = BackoffKModel(counts_k_list, A, alpha=alpha, delta=delta)\n",
    "        H = cross_entropy_bits(model, test, k)\n",
    "        losses.append((H, sum(max(0, len(s)-k) for s in test)))\n",
    "    # micro-average over folds\n",
    "    num = sum(H*n for H, n in losses)\n",
    "    den = sum(n for _, n in losses) or 1\n",
    "    return num/den\n",
    "\n",
    "# ---------- run over families and ks\n",
    "def run_entropy_over_families(\n",
    "    families: Dict[str, List[str]],\n",
    "    alphabet: List[str],\n",
    "    k_values=(0,1,2,3,4),\n",
    "    alpha=0.1, delta=None, folds=5, seed=0\n",
    "):\n",
    "    alpha_map = {c:i for i,c in enumerate(alphabet)}\n",
    "    A = len(alphabet)\n",
    "\n",
    "    # tokenize\n",
    "    fam_tok = {\n",
    "        fam: [tokenize(s, alpha_map) for s in seqs if len(tokenize(s, alpha_map))>0]\n",
    "        for fam, seqs in families.items()\n",
    "    }\n",
    "\n",
    "    results = {k:{} for k in k_values}\n",
    "    sizes   = {fam: sum(max(0,len(s)-max(k_values)) for s in seqs) for fam, seqs in fam_tok.items()}\n",
    "\n",
    "    for k in k_values:\n",
    "        fam_H = {}\n",
    "        for fam, seqs in fam_tok.items():\n",
    "            if len(seqs)==0: continue\n",
    "            Hk = entropy_rate_cv(seqs, A, k, alpha=alpha, delta=delta, folds=folds, seed=seed)\n",
    "            fam_H[fam] = Hk\n",
    "        results[k] = fam_H\n",
    "\n",
    "    # aggregates\n",
    "    aggregates = {}\n",
    "    for k in k_values:\n",
    "        fam_H = results[k]\n",
    "        fam_list = list(fam_H.items())\n",
    "        if not fam_list:\n",
    "            aggregates[k] = dict(macro=None, micro=None, n_families=0)\n",
    "            continue\n",
    "        macro = sum(h for _,h in fam_list)/len(fam_list)\n",
    "        # micro weight by total tokens (approximate using lengths at this k)\n",
    "        weights = {fam: sum(max(0, len(s)-k) for s in fam_tok[fam]) for fam,_ in fam_list}\n",
    "        num = sum(fam_H[fam]*weights[fam] for fam,_ in fam_list)\n",
    "        den = sum(weights.values()) or 1\n",
    "        micro = num/den\n",
    "        aggregates[k] = dict(macro=macro, micro=micro, n_families=len(fam_list))\n",
    "    return results, aggregates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de83d6",
   "metadata": {},
   "source": [
    "## Cross-Representation Information Analysis\n",
    "\n",
    "This experiment investigates the **mutual information** between amino acid (AA) and discrete structure representation (DSR) sequences by training probabilistic mappings in both directions using local sequence windows.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- **Windowed prediction**: Train neural networks to predict target tokens from source context windows (e.g., predict AA from 7-token DSR window)\n",
    "- **Bidirectional mapping**: Learn both DSR→AA and AA→DSR predictors to estimate cross-entropies\n",
    "- **Information bounds**: Derive mutual information lower bounds using H(target) - H(target|source_window)\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. **Complementarity**: How much structural information is captured in DSR that's not present in AA sequences?\n",
    "2. **Redundancy**: What fraction of sequence information is already encoded in structural representations?\n",
    "3. **Context dependence**: How does prediction accuracy vary with window size and MSA position?\n",
    "\n",
    "The analysis will reveal whether the two representations contain overlapping or complementary information about protein structure and function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Cross-representation information (Strategy 3)\n",
    "- Learn small probabilistic mappers DSR→AA and AA→DSR using windowed tokens\n",
    "- Estimate per-position and global H(AA), H(DSR), and conditional cross-entropies\n",
    "- Derive MI lower bounds: I_hat = H - H_hat(cond)\n",
    "\"\"\"\n",
    "\n",
    "import argparse, math, random\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------- FASTA utils -----------------------------\n",
    "\n",
    "def read_fasta(path: str) -> List[Tuple[str,str]]:\n",
    "    seqs = []\n",
    "    name = None\n",
    "    buf = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            if line.startswith('>'):\n",
    "                if name is not None:\n",
    "                    seqs.append((name, ''.join(buf)))\n",
    "                name = line[1:].strip()\n",
    "                buf = []\n",
    "            else:\n",
    "                buf.append(line)\n",
    "    if name is not None:\n",
    "        seqs.append((name, ''.join(buf)))\n",
    "    return seqs\n",
    "\n",
    "def ensure_same_shape(msa1: List[str], msa2: List[str]):\n",
    "    assert len(msa1) == len(msa2), \"MSA row count mismatch between AA and DSR.\"\n",
    "    L1 = len(msa1[0])\n",
    "    for s in msa1:\n",
    "        assert len(s) == L1, \"AA MSA rows must have equal length.\"\n",
    "    for s in msa2:\n",
    "        assert len(s) == L1, \"DSR MSA must have same number of columns as AA MSA.\"\n",
    "\n",
    "# ------------------------- Sequence reweighting ------------------------\n",
    "\n",
    "def seq_identity(a: str, b: str) -> float:\n",
    "    matches = 0\n",
    "    comps = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == '-' or y == '-':\n",
    "            continue\n",
    "        comps += 1\n",
    "        if x == y:\n",
    "            matches += 1\n",
    "    if comps == 0: return 0.0\n",
    "    return matches / comps\n",
    "\n",
    "def reweight_sequences(msa: List[str], thresh: float = 0.8) -> np.ndarray:\n",
    "    n = len(msa)\n",
    "    w = np.zeros(n, dtype=float)\n",
    "    for i in range(n):\n",
    "        c = 0\n",
    "        for j in range(n):\n",
    "            if seq_identity(msa[i], msa[j]) >= thresh:\n",
    "                c += 1\n",
    "        w[i] = 1.0 / max(1, c)\n",
    "    if w.sum() > 0:\n",
    "        w *= (n / w.sum())\n",
    "    return w\n",
    "\n",
    "# ----------------------- Entropy (empirical) ---------------------------\n",
    "\n",
    "def shannon_entropy_from_counts(counts: np.ndarray, pseudocount: float = 0.0) -> float:\n",
    "    A = counts.shape[0]\n",
    "    total = counts.sum()\n",
    "    p = (counts + pseudocount) / (total + pseudocount * A)\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def per_position_entropy(msa: List[str], alphabet: List[str], weights: np.ndarray,\n",
    "                         occupancy_threshold: float = 0.7,\n",
    "                         pseudocount: float = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    A = len(alphabet)\n",
    "    if pseudocount is None:\n",
    "        pseudocount = 1.0 / A\n",
    "    alpha_index = {c:i for i,c in enumerate(alphabet)}\n",
    "    L = len(msa[0])\n",
    "    ent = np.full(L, np.nan, dtype=float)\n",
    "    mask = np.zeros(L, dtype=bool)\n",
    "\n",
    "    occ = np.zeros(L, dtype=float)\n",
    "    for i, seq in enumerate(msa):\n",
    "        g = np.fromiter((c != '-' for c in seq), dtype=bool, count=L)\n",
    "        occ += weights[i] * g.astype(float)\n",
    "    occ /= max(1e-9, weights.sum())\n",
    "\n",
    "    for t in range(L):\n",
    "        if occ[t] < occupancy_threshold: continue\n",
    "        counts = np.zeros(A, dtype=float)\n",
    "        for i, seq in enumerate(msa):\n",
    "            ch = seq[t]\n",
    "            if ch == '-' or ch not in alpha_index: continue\n",
    "            counts[alpha_index[ch]] += weights[i]\n",
    "        if counts.sum() <= 0: continue\n",
    "        ent[t] = shannon_entropy_from_counts(counts, pseudocount=pseudocount)\n",
    "        mask[t] = True\n",
    "    return ent, mask\n",
    "\n",
    "# ------------------------ Windowed samples -----------------------------\n",
    "\n",
    "def build_window_samples(\n",
    "    src_msa: List[str], tgt_msa: List[str], weights: np.ndarray,\n",
    "    src_alpha: List[str], tgt_alpha: List[str],\n",
    "    pos_mask: np.ndarray, win: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Build samples to predict target token at t from a src window around t.\n",
    "    - Requires no gaps in the src window or target at t.\n",
    "    - pos_mask picks columns to consider (e.g., occupancy intersection).\n",
    "    Returns X (one-hot per-window), y (int labels), w (sample weights), pos_idx (column indices).\n",
    "    \"\"\"\n",
    "    assert win % 2 == 1, \"Window must be odd size.\"\n",
    "    half = win // 2\n",
    "    A_src, A_tgt = len(src_alpha), len(tgt_alpha)\n",
    "    src_idx = {c:i for i,c in enumerate(src_alpha)}\n",
    "    tgt_idx = {c:i for i,c in enumerate(tgt_alpha)}\n",
    "\n",
    "    L = len(src_msa[0])\n",
    "    n = len(src_msa)\n",
    "\n",
    "    feats = []\n",
    "    labels = []\n",
    "    sw = []\n",
    "    pos_idx = []\n",
    "\n",
    "    # precompute valid (non-gap and in alphabet) masks\n",
    "    src_valid = np.array([[ (c!='-' and c in src_idx) for c in row] for row in src_msa], dtype=bool)\n",
    "    tgt_valid = np.array([[ (c!='-' and c in tgt_idx) for c in row] for row in tgt_msa], dtype=bool)\n",
    "\n",
    "    for i in range(n):\n",
    "        w_i = weights[i]\n",
    "        src_row = src_msa[i]\n",
    "        tgt_row = tgt_msa[i]\n",
    "        for t in range(half, L-half):\n",
    "            if not pos_mask[t]: continue\n",
    "            if not tgt_valid[i, t]: continue\n",
    "            # window must be fully valid in src\n",
    "            if not src_valid[i, t-half:t+half+1].all(): continue\n",
    "\n",
    "            # one-hot encode window: concat per-position one-hots\n",
    "            v = np.zeros((win, A_src), dtype=np.float32)\n",
    "            for j, col in enumerate(range(t-half, t+half+1)):\n",
    "                s = src_row[col]\n",
    "                v[j, src_idx[s]] = 1.0\n",
    "            feats.append(v.reshape(-1))  # (win*A_src,)\n",
    "            labels.append(tgt_idx[tgt_row[t]])\n",
    "            sw.append(w_i)\n",
    "            pos_idx.append(t)\n",
    "\n",
    "    if len(feats) == 0:\n",
    "        return (np.zeros((0, win*A_src), dtype=np.float32),\n",
    "                np.zeros((0,), dtype=np.int64),\n",
    "                np.zeros((0,), dtype=np.float32),\n",
    "                np.zeros((0,), dtype=np.int32))\n",
    "    X = np.stack(feats, axis=0)\n",
    "    y = np.array(labels, dtype=np.int64)\n",
    "    w = np.array(sw, dtype=np.float32)\n",
    "    pos_idx = np.array(pos_idx, dtype=np.int32)\n",
    "    return X, y, w, pos_idx\n",
    "\n",
    "# --------------------------- Torch model --------------------------------\n",
    "\n",
    "class SoftmaxLinear(nn.Module):\n",
    "    def __init__(self, D_in: int, C_out: int, bias=True):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(D_in, C_out, bias=bias)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)  # logits\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        self.w = torch.from_numpy(w)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "def split_by_sequence_indices(n_seq: int, seed=0, ratios=(0.7, 0.15, 0.15)):\n",
    "    idx = list(range(n_seq))\n",
    "    random.Random(seed).shuffle(idx)\n",
    "    n_train = int(ratios[0]*n_seq)\n",
    "    n_val   = int(ratios[1]*n_seq)\n",
    "    train_ids = set(idx[:n_train])\n",
    "    val_ids   = set(idx[n_train:n_train+n_val])\n",
    "    test_ids  = set(idx[n_train+n_val:])\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "def mask_samples_by_seqpos(seq_pos_of_sample: List[int], seq_ids_set: set):\n",
    "    return np.array([sp in seq_ids_set for sp in seq_pos_of_sample], dtype=bool)\n",
    "\n",
    "def train_softmax(\n",
    "    X_train, y_train, w_train,\n",
    "    X_val,   y_val,   w_val,\n",
    "    D_in, C_out, lr=1e-2, epochs=20, bs=4096, weight_decay=1e-4, device=\"cpu\"\n",
    "):\n",
    "    model = SoftmaxLinear(D_in, C_out).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    ds_tr = NumpyDataset(X_train, y_train, w_train)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=bs, shuffle=True, drop_last=False)\n",
    "\n",
    "    def eval_ce(X, y, w):\n",
    "        if X.shape[0] == 0: return float('nan')\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.from_numpy(X).to(device)\n",
    "            y_t = torch.from_numpy(y).to(device)\n",
    "            w_t = torch.from_numpy(w).to(device)\n",
    "            logits = model(X_t)\n",
    "            ce = nn.functional.cross_entropy(logits, y_t, reduction='none')\n",
    "            return float((ce * w_t).sum().item() / max(1e-9, w_t.sum().item()))\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    patience, bad = 4, 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb, wb in dl_tr:\n",
    "            xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "            logits = model(xb)\n",
    "            ce = nn.functional.cross_entropy(logits, yb, reduction='none')\n",
    "            loss = (ce * wb).sum() / (wb.sum() + 1e-9)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        val_ce = eval_ce(X_val, y_val, w_val)\n",
    "        if val_ce < best_val - 1e-5:\n",
    "            best_val = val_ce\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def predict_log_probs(model: nn.Module, X: np.ndarray, bs=8192, device=\"cpu\") -> np.ndarray:\n",
    "    if X.shape[0] == 0: return np.zeros((0, model.lin.out_features), dtype=np.float32)\n",
    "    model.eval()\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X.shape[0], bs):\n",
    "            xb = torch.from_numpy(X[i:i+bs]).to(device)\n",
    "            logits = model(xb)\n",
    "            logp = nn.functional.log_softmax(logits, dim=-1)\n",
    "            out.append(logp.detach().cpu().numpy())\n",
    "    return np.vstack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5949766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
