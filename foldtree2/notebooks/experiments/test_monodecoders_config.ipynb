{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c70e0c",
   "metadata": {},
   "source": [
    "# FoldTree2 Model Training and Analysis (with Config Support)\n",
    "\n",
    "This notebook trains a protein structure prediction model using FoldTree2's encoder-decoder architecture. The model learns to encode protein structures into discrete embeddings and decode them back to predict amino acid sequences and structural contacts.\n",
    "\n",
    "**New Features:**\n",
    "- **Consolidated Hyperparameters**: All hyperparameters are now defined in a single cell for easy modification\n",
    "- **Config File Support**: Can load hyperparameters from YAML or JSON config files\n",
    "- **Command-line Override**: Config file values can be overridden with cell parameters\n",
    "\n",
    "## Training Process\n",
    "The notebook demonstrates:\n",
    "- **Vector Quantized Encoding**: Proteins are encoded into discrete embedding sequences using a transformer-based encoder\n",
    "- **Multi-task Decoding**: The decoder predicts amino acid sequences, contact maps, and geometric properties\n",
    "- **Progressive Learning**: Training occurs over multiple epochs with various loss components (reconstruction, contact prediction, VQ regularization)\n",
    "\n",
    "## Training Visualizations\n",
    "During training, the notebook generates comprehensive analysis plots showing:\n",
    "- **Contact Prediction**: Predicted vs. true contact maps for protein residue interactions\n",
    "- **Distance Analysis**: True distance matrices and binary contact classifications\n",
    "- **Performance Metrics**: ROC curves and precision-recall analysis for contact prediction accuracy\n",
    "- **Sequence Embedding**: Color-coded visualization of the discrete embedding alphabet learned by the model\n",
    "- **3D Structure**: Interactive molecular visualization colored by embedding states\n",
    "- **Bond Angles**: Comparison of predicted vs. true backbone bond angles\n",
    "\n",
    "This provides real-time feedback on model performance across sequence, contact, and geometric prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/dmoi/projects/foldtree2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import numpy as np\n",
    "from foldtree2.src import pdbgraph\n",
    "from foldtree2.src import encoder as ecdr\n",
    "from foldtree2.src.losses.losses import recon_loss_diag , aa_reconstruction_loss, angles_reconstruction_loss\n",
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7853fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformers schedulers\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b402f3",
   "metadata": {},
   "source": [
    "## Configuration Management\n",
    "\n",
    "This cell handles loading and merging configurations from:\n",
    "1. Config file (if provided)\n",
    "2. Cell-defined parameters (can override config file)\n",
    "\n",
    "You can specify a config file path below, or leave it as `None` to use only cell parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963eef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONSOLIDATED HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "# All hyperparameters are defined in this single cell for easy management\n",
    "# These can be overridden by loading a config file (YAML or JSON)\n",
    "# ============================================================================\n",
    "\n",
    "# --- CONFIG FILE LOADING ---\n",
    "config_file = None  # Set to path like 'config.yaml' to load from file\n",
    "# Example: config_file = 'config_notebook_1k_epochs.yaml'\n",
    "\n",
    "# --- DATA PARAMETERS ---\n",
    "datadir = '../../datasets/foldtree2/'\n",
    "dataset_path = 'structs_train_final.h5'\n",
    "aapropcsv = './foldtree2/config/aaindex1.csv'\n",
    "\n",
    "# --- MODEL ARCHITECTURE PARAMETERS ---\n",
    "# Alphabet/Embedding parameters\n",
    "num_embeddings = 30\n",
    "embedding_dim = 128\n",
    "\n",
    "# Network size\n",
    "hidden_size = 150\n",
    "\n",
    "# Encoder parameters\n",
    "encoder_type = 'mk1_Encoder'  # Options: 'mk1_Encoder', 'mk1_MuonEncoder'\n",
    "encoder_hidden_channels = [150, 150, 150]\n",
    "encoder_nheads = 16\n",
    "encoder_dropout = 0.005\n",
    "encoder_flavor = 'transformer'\n",
    "encoder_fftin = True\n",
    "encoder_learn_positions = True\n",
    "encoder_concat_positions = False\n",
    "\n",
    "# Decoder parameters\n",
    "use_monodecoder = True  # True for MultiMonoDecoder, False for single decoder\n",
    "use_muon_decoders = True  # Use Muon-compatible decoders\n",
    "\n",
    "# --- TRAINING PARAMETERS ---\n",
    "num_epochs = 300\n",
    "batch_size = 10\n",
    "gradient_accumulation_steps = 2\n",
    "clip_grad = True\n",
    "mask_plddt = True\n",
    "plddt_threshold = 0.3\n",
    "num_workers = 4\n",
    "\n",
    "# Learning rate and scheduler\n",
    "learning_rate = 1e-5\n",
    "scheduler_type = 'plateau'  # Options: 'plateau', 'linear', 'cosine', 'cosine_with_restarts', 'polynomial'\n",
    "warmup_steps = 20\n",
    "warmup_ratio = 0.05  # Alternative: ratio of total training steps for warmup\n",
    "\n",
    "# Optimizer parameters\n",
    "use_muon = True  # Use Muon optimizer (hybrid Muon+AdamW)\n",
    "muon_lr = 0.02  # Learning rate for Muon (hidden weights)\n",
    "adamw_lr = 1e-4  # Learning rate for AdamW (gains/biases/other params)\n",
    "weight_decay = 0.01\n",
    "\n",
    "# --- LOSS WEIGHTS ---\n",
    "edgeweight = 0.1\n",
    "logitweight = 0.1\n",
    "xweight = 0.1\n",
    "fft2weight = 0.01\n",
    "vqweight = 0.005\n",
    "angles_weight = 0.1\n",
    "ss_weight = 0.1\n",
    "\n",
    "# Loss weight scheduler\n",
    "use_weight_scheduler = True\n",
    "loss_scheduler_type = 'linear'  # Options: 'linear', 'cosine', 'cosine_restarts', 'polynomial', 'constant'\n",
    "loss_warmup_steps = 20\n",
    "\n",
    "# --- COMMITMENT COST SCHEDULING ---\n",
    "use_commitment_scheduling = True\n",
    "commitment_cost_final = 0.9\n",
    "commitment_warmup_steps = 1000\n",
    "commitment_schedule = 'linear'  # Options: 'cosine', 'linear', 'none'\n",
    "commitment_start = 0.5\n",
    "\n",
    "# --- MIXED PRECISION TRAINING ---\n",
    "use_mixed_precision = True\n",
    "\n",
    "# --- REPRODUCIBILITY ---\n",
    "random_seed = 0\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD CONFIG FILE (if specified)\n",
    "# ============================================================================\n",
    "if config_file is not None and os.path.exists(config_file):\n",
    "    print(f\"Loading configuration from: {config_file}\")\n",
    "    \n",
    "    with open(config_file, 'r') as f:\n",
    "        if config_file.endswith('.yaml') or config_file.endswith('.yml'):\n",
    "            config = yaml.safe_load(f)\n",
    "        elif config_file.endswith('.json'):\n",
    "            config = json.load(f)\n",
    "        else:\n",
    "            raise ValueError(\"Config file must be YAML (.yaml/.yml) or JSON (.json)\")\n",
    "    \n",
    "    # Override cell parameters with config file values\n",
    "    # Only override if key exists in config\n",
    "    for key, value in config.items():\n",
    "        if key in locals():\n",
    "            locals()[key] = value\n",
    "            print(f\"  {key}: {value} (from config)\")\n",
    "        else:\n",
    "            print(f\"  Warning: Unknown config key '{key}' - ignoring\")\n",
    "    \n",
    "    print(\"Configuration loaded successfully!\")\n",
    "else:\n",
    "    if config_file is not None:\n",
    "        print(f\"Warning: Config file '{config_file}' not found. Using cell parameters.\")\n",
    "    else:\n",
    "        print(\"No config file specified. Using cell parameters.\")\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: {dataset_path}\")\n",
    "print(f\"Num Epochs: {num_epochs}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Gradient Accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Scheduler: {scheduler_type}\")\n",
    "print(f\"Optimizer: {'Muon+AdamW' if use_muon else 'AdamW'}\")\n",
    "print(f\"Num Embeddings: {num_embeddings}\")\n",
    "print(f\"Embedding Dim: {embedding_dim}\")\n",
    "print(f\"Hidden Size: {hidden_size}\")\n",
    "print(f\"Mixed Precision: {use_mixed_precision}\")\n",
    "print(f\"Mask pLDDT: {mask_plddt} (threshold: {plddt_threshold})\")\n",
    "print(\"\\nLoss Weights:\")\n",
    "print(f\"  Edge: {edgeweight}, Logit: {logitweight}, X: {xweight}\")\n",
    "print(f\"  FFT2: {fft2weight}, VQ: {vqweight}, Angles: {angles_weight}, SS: {ss_weight}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Data setup\n",
    "converter = pdbgraph.PDB2PyG(aapropcsv=aapropcsv)\n",
    "struct_dat = pdbgraph.StructureDataset(dataset_path)\n",
    "train_loader = DataLoader(struct_dat, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "data_sample = next(iter(train_loader))\n",
    "\n",
    "# Calculate training steps\n",
    "training_steps = len(train_loader) * num_epochs\n",
    "\n",
    "print(f\"Loaded {len(struct_dat)} structures\")\n",
    "print(f\"Training steps per epoch: {len(train_loader)}\")\n",
    "print(f\"Total training steps: {training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the cuda devices available and their specs\n",
    "if torch.cuda.is_available():\n",
    "\tfor i in range(torch.cuda.device_count()):\n",
    "\t\tprint(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\t\tprint(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)} bytes\")\n",
    "\t\tprint(f\"  Memory Cached: {torch.cuda.memory_reserved(i)} bytes\")\n",
    "else:\n",
    "\tprint(\"No CUDA devices available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions from data sample\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ndim = data_sample['res'].x.shape[1]\n",
    "ndim_godnode = data_sample['godnode'].x.shape[1]\n",
    "ndim_fft2i = data_sample['fourier2di'].x.shape[1]\n",
    "ndim_fft2r = data_sample['fourier2dr'].x.shape[1]\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Residue features dim: {ndim}\")\n",
    "print(f\"Godnode dim: {ndim_godnode}\")\n",
    "print(f\"FFT dimensions: {ndim_fft2r} (real), {ndim_fft2i} (imag)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\t\n",
    "\n",
    "def loss_weight_scheduler(step, total_steps, schedule_type='linear', warmup_steps=0, power=1.0, num_cycles=1):\n",
    "\t\"\"\"\n",
    "\tLoss weight scheduler that modulates weights during training.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tstep: Current training step\n",
    "\t\ttotal_steps: Total number of training steps\n",
    "\t\tschedule_type: Type of schedule ('linear', 'cosine', 'cosine_restarts', 'polynomial', 'constant')\n",
    "\t\twarmup_steps: Number of steps to warmup from 0 to 1\n",
    "\t\tpower: Power for polynomial decay (only used for 'polynomial')\n",
    "\t\tnum_cycles: Number of cycles for cosine with restarts\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tweight: Scalar weight multiplier in range [0, 1]\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Warmup phase\n",
    "\tif step < warmup_steps:\n",
    "\t\treturn step / warmup_steps\n",
    "\t\n",
    "\t# Adjust step for post-warmup scheduling\n",
    "\tprogress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "\tprogress = min(progress, 1.0)\n",
    "\t\n",
    "\tif schedule_type == 'constant':\n",
    "\t\treturn 1.0\n",
    "\t\n",
    "\telif schedule_type == 'linear':\n",
    "\t\t# Linear decay from 1.0 to 0.0\n",
    "\t\treturn 1.0 - progress\n",
    "\t\n",
    "\telif schedule_type == 'cosine':\n",
    "\t\t# Cosine annealing from 1.0 to 0.0\n",
    "\t\treturn 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\t\n",
    "\telif schedule_type == 'cosine_restarts':\n",
    "\t\t# Cosine with hard restarts (SGDR)\n",
    "\t\tcycle_progress = (progress * num_cycles) % 1.0\n",
    "\t\treturn 0.5 * (1.0 + math.cos(math.pi * cycle_progress))\n",
    "\t\n",
    "\telif schedule_type == 'polynomial':\n",
    "\t\t# Polynomial decay\n",
    "\t\treturn (1.0 - progress) ** power\n",
    "\t\n",
    "\telse:\n",
    "\t\traise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "# Define partial functions for each loss weight scheduler\n",
    "if use_weight_scheduler:\n",
    "\tx_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t  total_steps=training_steps, \n",
    "\t\t\t\t\t\t  schedule_type=loss_scheduler_type,\n",
    "\t\t\t\t\t\t  warmup_steps=loss_warmup_steps)\n",
    "\n",
    "\tlogit_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t  total_steps=training_steps, \n",
    "\t\t\t\t\t\t  schedule_type=loss_scheduler_type,\n",
    "\t\t\t\t\t\t  warmup_steps=loss_warmup_steps)\n",
    "\n",
    "\tedgeweight_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t  total_steps=training_steps, \n",
    "\t\t\t\t\t\t  schedule_type='cosine_restarts',\n",
    "\t\t\t\t\t\t  num_cycles=3,\n",
    "\t\t\t\t\t\t  warmup_steps=loss_warmup_steps)\n",
    "\n",
    "\tss_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t  total_steps=training_steps, \n",
    "\t\t\t\t\t\t  schedule_type=loss_scheduler_type,\n",
    "\t\t\t\t\t\t  warmup_steps=loss_warmup_steps,\n",
    "\t\t\t\t\t\t  power=2.0)\n",
    "\n",
    "\tvq_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t  total_steps=training_steps, \n",
    "\t\t\t\t\t\t  schedule_type='cosine_restarts',\t\n",
    "\t\t\t\t\t\t  num_cycles=10,\n",
    "\t\t\t\t\t\t  warmup_steps=loss_warmup_steps)\n",
    "\t\n",
    "\tfft2_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t  total_steps=training_steps, \n",
    "\t\t\t\t\t\t  schedule_type='cosine_restarts',\t\n",
    "\t\t\t\t\t\t  num_cycles=5,\n",
    "\t\t\t\t\t\t  warmup_steps=loss_warmup_steps)\n",
    "\t\n",
    "\tangles_scheduler = partial(loss_weight_scheduler,\n",
    "\t\t\t\t\t\t\ttotal_steps=training_steps,\n",
    "\t\t\t\t\t\t\tschedule_type=loss_scheduler_type,\n",
    "\t\t\t\t\t\t\twarmup_steps=loss_warmup_steps)\n",
    "\t\n",
    "\tprint(\"Loss weight schedulers initialized\")\n",
    "else:\n",
    "\tprint(\"Loss weight scheduling disabled - using constant weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, scheduler_type, num_warmup_steps, num_training_steps, **kwargs):\n",
    "\tif scheduler_type == 'linear':\n",
    "\t\treturn get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\telif scheduler_type == 'cosine':\n",
    "\t\treturn get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\telif scheduler_type == 'cosine_with_restarts':\n",
    "\t\treturn get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\telif scheduler_type == 'polynomial':\n",
    "\t\treturn get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, lr_end=0.0, power=1.0)\n",
    "\telif scheduler_type == 'plateau':\n",
    "\t\t# ReduceLROnPlateau doesn't require distributed process groups - it only monitors loss values\n",
    "\t\treturn torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, **kwargs)\n",
    "\telse:\n",
    "\t\traise ValueError(f\"Unknown scheduler type: {scheduler_type}\")\n",
    "\n",
    "# Calculate warmup steps\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "if isinstance(warmup_ratio, float) and warmup_ratio > 0:\n",
    "\tnum_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "else:\n",
    "\tnum_warmup_steps = warmup_steps\n",
    "\n",
    "print(f\"Scheduler: {scheduler_type}\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a82e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Encoder\n",
    "print(f\"Initializing encoder: {encoder_type}\")\n",
    "\n",
    "encoder = ecdr.mk1_Encoder(\n",
    "\tin_channels=ndim,\n",
    "\thidden_channels=encoder_hidden_channels,\n",
    "\tout_channels=embedding_dim,\n",
    "\tmetadata={'edge_types': [('res','contactPoints','res')]},\n",
    "\tnum_embeddings=num_embeddings,\n",
    "\tcommitment_cost=commitment_cost_final,\n",
    "\tedge_dim=1,\n",
    "\tencoder_hidden=hidden_size,\n",
    "\tEMA=True,\n",
    "\tnheads=encoder_nheads,\n",
    "\tdropout_p=encoder_dropout,\n",
    "\treset_codes=False,\n",
    "\tflavor=encoder_flavor,\n",
    "\tfftin=encoder_fftin,\n",
    "\tuse_commitment_scheduling=use_commitment_scheduling,\n",
    "\tcommitment_warmup_steps=commitment_warmup_steps,\n",
    "\tcommitment_schedule=commitment_schedule,\n",
    "\tcommitment_start=commitment_start,\n",
    "\tconcat_positions=encoder_concat_positions,\n",
    "\tlearn_positions=encoder_learn_positions\n",
    ")\n",
    "\n",
    "print(encoder)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foldtree2.src.mono_decoders import MultiMonoDecoder\n",
    "\n",
    "# Initialize Decoder\n",
    "if use_monodecoder:\n",
    "\tprint(\"Initializing MultiMonoDecoder\")\n",
    "\t\n",
    "\tmono_configs = {\n",
    "\t\t'sequence_transformer': {\n",
    "\t\t\t'in_channels': {'res': embedding_dim},\n",
    "\t\t\t'xdim': 20,\n",
    "\t\t\t'concat_positions': False,\n",
    "\t\t\t'hidden_channels': {('res','backbone','res'): [hidden_size], ('res','backbonerev','res'): [hidden_size]},\n",
    "\t\t\t'layers': 2,\n",
    "\t\t\t'AAdecoder_hidden': [hidden_size, hidden_size, hidden_size//2], \n",
    "\t\t\t'amino_mapper': converter.aaindex,\n",
    "\t\t\t'nheads': 2,\n",
    "\t\t\t'dropout': 0.001,\n",
    "\t\t\t'normalize': False,\n",
    "\t\t\t'residual': False,\n",
    "\t\t\t'use_cnn_decoder': False,\n",
    "\t\t\t'output_ss': False,\n",
    "\t\t\t'learn_positions': True,\n",
    "\t\t\t'concat_positions': False\n",
    "\t\t},\n",
    "\t\t\n",
    "\t\t'geometry_transformer': {\n",
    "\t\t\t'in_channels': {'res': embedding_dim},\n",
    "\t\t\t'concat_positions': False,\n",
    "\t\t\t'hidden_channels': {('res','backbone','res'): [hidden_size], ('res','backbonerev','res'): [hidden_size]},\n",
    "\t\t\t'layers': 2,\n",
    "\t\t\t'nheads': 2,\n",
    "\t\t\t'RTdecoder_hidden': [hidden_size, hidden_size, hidden_size//2],\n",
    "\t\t\t'ssdecoder_hidden': [hidden_size, hidden_size, hidden_size//2],\n",
    "\t\t\t'anglesdecoder_hidden': [hidden_size, hidden_size, hidden_size//2],\n",
    "\t\t\t'dropout': 0.001,\n",
    "\t\t\t'normalize': False,\n",
    "\t\t\t'residual': False,\n",
    "\t\t\t'learn_positions': True,\n",
    "\t\t\t'use_cnn_decoder': True,\n",
    "\t\t\t'concat_positions': False,\n",
    "\t\t\t'output_rt': False,\n",
    "\t\t\t'output_ss': True,\n",
    "\t\t\t'output_angles': True\n",
    "\t\t},\n",
    "\t\t\n",
    "\t\t'geometry_cnn': {\n",
    "\t\t\t'in_channels': {'res': embedding_dim, 'godnode4decoder': ndim_godnode, 'foldx': 23, 'fft2r': ndim_fft2r, 'fft2i': ndim_fft2i},\n",
    "\t\t\t'concat_positions': False,\n",
    "\t\t\t'conv_channels': [2*hidden_size, hidden_size, hidden_size],\n",
    "\t\t\t'kernel_sizes': [3, 3, 3],\n",
    "\t\t\t'FFT2decoder_hidden': [hidden_size//2, hidden_size//2],\n",
    "\t\t\t'contactdecoder_hidden': [hidden_size//2, hidden_size//4],\n",
    "\t\t\t'ssdecoder_hidden': [hidden_size//2, hidden_size//2],\n",
    "\t\t\t'Xdecoder_hidden': [hidden_size, hidden_size], \n",
    "\t\t\t'anglesdecoder_hidden': [hidden_size, hidden_size, hidden_size//2],\n",
    "\t\t\t'RTdecoder_hidden': [hidden_size//2, hidden_size//4],\n",
    "\t\t\t'metadata': converter.metadata, \n",
    "\t\t\t'dropout': 0.001,\n",
    "\t\t\t'output_fft': False,\n",
    "\t\t\t'output_rt': False,\n",
    "\t\t\t'output_angles': False,\n",
    "\t\t\t'output_ss': False,\n",
    "\t\t\t'normalize': True,\n",
    "\t\t\t'residual': False,\n",
    "\t\t\t'output_edge_logits': True,\n",
    "\t\t\t'ncat': 8,\n",
    "\t\t\t'contact_mlp': False,\n",
    "\t\t\t'pool_type': 'global_mean',\n",
    "\t\t\t'learn_positions': True,\n",
    "\t\t\t'concat_positions': False\n",
    "\t\t},\n",
    "\t}\n",
    "\n",
    "\tdecoder = MultiMonoDecoder(configs=mono_configs)\n",
    "else:\n",
    "\tprint(\"Initializing single HeteroGAE_Decoder\")\n",
    "\tdecoder = ecdr.HeteroGAE_Decoder(\n",
    "\t\tin_channels={'res': embedding_dim, 'godnode4decoder': ndim_godnode, 'foldx': 23},\n",
    "\t\tconcat_positions=False,\n",
    "\t\thidden_channels={('res','backbone','res'): [hidden_size]*5, ('res','backbonerev','res'): [hidden_size]*5},\n",
    "\t\tlayers=3,\n",
    "\t\tAAdecoder_hidden=[hidden_size, hidden_size, hidden_size//2],\n",
    "\t\tXdecoder_hidden=[hidden_size, hidden_size, hidden_size],\n",
    "\t\tcontactdecoder_hidden=[hidden_size//2, hidden_size//2],\n",
    "\t\tanglesdecoder_hidden=[hidden_size//2, hidden_size//2, hidden_size//4],\n",
    "\t\tnheads=5,\n",
    "\t\tamino_mapper=converter.aaindex,\n",
    "\t\tflavor='sage',\n",
    "\t\tdropout=0.005,\n",
    "\t\tnormalize=True,\n",
    "\t\tresidual=False,\n",
    "\t\tcontact_mlp=False\n",
    "\t)\n",
    "\n",
    "decoder = decoder.to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eacd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop setup\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from muon import MuonWithAuxAdam\n",
    "\n",
    "encoder.device = device\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "if not use_muon:\n",
    "\tprint(\"Using AdamW optimizer\")\n",
    "\toptimizer = torch.optim.AdamW(\n",
    "\t\tlist(encoder.parameters()) + list(decoder.parameters()), \n",
    "\t\tlr=learning_rate,\n",
    "\t\tweight_decay=weight_decay\n",
    "\t)\n",
    "else:\n",
    "\tprint(\"Using Muon+AdamW hybrid optimizer\")\n",
    "\thidden_weights = []\n",
    "\thidden_gains_biases = []\n",
    "\tnonhidden_params = []\n",
    "\t\n",
    "\t# Helper function to check if a model has modular structure\n",
    "\tdef has_modular_structure(model):\n",
    "\t\treturn hasattr(model, 'input') and hasattr(model, 'body') and hasattr(model, 'head')\n",
    "\t\n",
    "\t# Process encoder\n",
    "\tif has_modular_structure(encoder):\n",
    "\t\tprint(\"  Encoder: modular structure detected\")\n",
    "\t\thidden_weights += [p for p in encoder.body.parameters() if p.ndim >= 2]\n",
    "\t\thidden_gains_biases += [p for p in encoder.body.parameters() if p.ndim < 2]\n",
    "\t\tnonhidden_params += [*encoder.head.parameters(), *encoder.input.parameters()]\n",
    "\telse:\n",
    "\t\tprint(\"  Encoder: non-modular, using AdamW\")\n",
    "\t\tnonhidden_params += list(encoder.parameters())\n",
    "\t\n",
    "\t# Process decoder\n",
    "\tif hasattr(decoder, 'decoders'):\n",
    "\t\tprint(f\"  Decoder: MultiMonoDecoder with {len(decoder.decoders)} sub-decoders\")\n",
    "\t\tfor name, subdecoder in decoder.decoders.items():\n",
    "\t\t\tif has_modular_structure(subdecoder):\n",
    "\t\t\t\thidden_weights += [p for p in subdecoder.body.parameters() if p.ndim >= 2]\n",
    "\t\t\t\thidden_gains_biases += [p for p in subdecoder.body.parameters() if p.ndim < 2]\n",
    "\t\t\t\tnonhidden_params += [*subdecoder.head.parameters(), *subdecoder.input.parameters()]\n",
    "\t\t\telse:\n",
    "\t\t\t\tnonhidden_params += list(subdecoder.parameters())\n",
    "\telif has_modular_structure(decoder):\n",
    "\t\tprint(\"  Decoder: modular structure detected\")\n",
    "\t\thidden_weights += [p for p in decoder.body.parameters() if p.ndim >= 2]\n",
    "\t\thidden_gains_biases += [p for p in decoder.body.parameters() if p.ndim < 2]\n",
    "\t\tnonhidden_params += [*decoder.head.parameters(), *decoder.input.parameters()]\n",
    "\telse:\n",
    "\t\tprint(\"  Decoder: non-modular, using AdamW\")\n",
    "\t\tnonhidden_params += list(decoder.parameters())\n",
    "\t\n",
    "\tprint(f\"\\nParameter groups:\")\n",
    "\tprint(f\"  Hidden weights (Muon): {len(hidden_weights)} tensors\")\n",
    "\tprint(f\"  Hidden gains/biases (AdamW): {len(hidden_gains_biases)} tensors\")\n",
    "\tprint(f\"  Non-hidden params (AdamW): {len(nonhidden_params)} tensors\")\n",
    "\t\n",
    "\tparam_groups = [\n",
    "\t\tdict(params=hidden_weights, use_muon=True, lr=muon_lr, weight_decay=weight_decay),\n",
    "\t\tdict(params=hidden_gains_biases+nonhidden_params, use_muon=False,\n",
    "\t\t\t lr=adamw_lr, betas=(0.9, 0.95), weight_decay=weight_decay),\n",
    "\t]\n",
    "\n",
    "\toptimizer = MuonWithAuxAdam(param_groups)\n",
    "\n",
    "# Initialize scheduler\n",
    "scheduler = get_scheduler(\n",
    "\toptimizer, \n",
    "\tscheduler_type=scheduler_type,\n",
    "\tnum_training_steps=num_training_steps,\n",
    "\tnum_warmup_steps=num_warmup_steps,\n",
    ")\n",
    "\n",
    "scheduler_step_mode = 'epoch' if scheduler_type == 'plateau' else 'step'\n",
    "\n",
    "print(f\"\\nScheduler: {scheduler_type} (step mode: {scheduler_step_mode})\")\n",
    "print(f\"Mixed precision: {use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8bb3b",
   "metadata": {},
   "source": [
    "## Muon Optimizer Training\n",
    "\n",
    "The Muon optimizer uses a hybrid approach:\n",
    "- **Muon optimizer** for hidden layer weights (2D+ tensors in body modules) - uses momentum-based Newton updates\n",
    "- **AdamW optimizer** for gains, biases, and non-hidden parameters (input/head modules)\n",
    "\n",
    "This configuration is optimal for deep networks with modular architectures where:\n",
    "- Body modules contain the core transformations (graph convolutions, CNNs, transformers)\n",
    "- Input/Head modules handle preprocessing and task-specific outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Muon setup\n",
    "if use_muon:\n",
    "\tprint(\"\\n\" + \"=\"*60)\n",
    "\tprint(\"MUON OPTIMIZER INITIALIZATION\")\n",
    "\tprint(\"=\"*60)\n",
    "\t\n",
    "\timport os as dist_os\n",
    "\timport torch.distributed as dist\n",
    "\t\n",
    "\t# Clean up any existing process group\n",
    "\tif dist.is_initialized():\n",
    "\t\tdist.destroy_process_group()\n",
    "\t\tprint(\"Destroyed existing process group\")\n",
    "\t\n",
    "\t# Find an available port\n",
    "\timport socket\n",
    "\tdef find_free_port():\n",
    "\t\twith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "\t\t\ts.bind(('', 0))\n",
    "\t\t\ts.listen(1)\n",
    "\t\t\tport = s.getsockname()[1]\n",
    "\t\treturn port\n",
    "\t\n",
    "\tfree_port = find_free_port()\n",
    "\t\n",
    "\tdist_os.environ['MASTER_ADDR'] = 'localhost'\n",
    "\tdist_os.environ['MASTER_PORT'] = str(free_port)\n",
    "\tdist_os.environ['RANK'] = '0'\n",
    "\tdist_os.environ['WORLD_SIZE'] = '1'\n",
    "\tdist.init_process_group(backend='gloo', init_method='env://')\n",
    "\tprint(f\"Initialized process group on port {free_port}\")\n",
    "\tprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mixed precision scaler if enabled\n",
    "if use_mixed_precision:\n",
    "\tscaler = GradScaler()\n",
    "\tprint(\"Mixed precision training enabled (GradScaler initialized)\")\n",
    "else:\n",
    "\tscaler = None\n",
    "\tprint(\"Mixed precision training disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac01b1b",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop below continues from the original notebook but uses the consolidated hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample from the dataloader for testing\n",
    "train_loader_test = DataLoader(struct_dat, batch_size=1, shuffle=True, num_workers=4)\n",
    "randint = random.randint(0, len(train_loader_test) - 1)\n",
    "print(f\"Randomly selected batch index: {randint}\")\n",
    "data_sample = struct_dat[randint]\n",
    "print(data_sample)\n",
    "data = data_sample.to(device)\n",
    "optimizer.zero_grad()\n",
    "z, vqloss = encoder(data, debug=True)\n",
    "print('Encoded z shape:', z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter proteins with at least 200 amino acids and average plddt > 0.7\n",
    "import json\n",
    "check_plddt = False\n",
    "if check_plddt or not os.path.exists('plddt_dataset.json'):\n",
    "\t# Collect valid protein indices\n",
    "\tvalid_indices = []\n",
    "\tfor idx in range(len(struct_dat)):\n",
    "\t\ttry:\n",
    "\t\t\tdata = struct_dat[idx]\n",
    "\t\t\tnum_residues = data['AA'].x.shape[0]\n",
    "\t\t\tavg_plddt = data['plddt'].x.mean().item()\n",
    "\t\t\t\n",
    "\t\t\tif num_residues >= 200 and avg_plddt > 0.7:\n",
    "\t\t\t\tvalid_indices.append(idx)\n",
    "\t\t\t\tprint(f\"Index {idx}: {data.identifier}, {num_residues} residues, avg pLDDT={avg_plddt:.3f}\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\tprint(f\"\\nFound {len(valid_indices)} proteins with ≥200 residues and avg pLDDT > 0.7\")\n",
    "\twith open('plddt_dataset.json', 'w') as fileout:\n",
    "\t\tfileout.write(json.dumps(valid_indices))\n",
    "else:\n",
    "\twith open('plddt_dataset.json') as fileout:\n",
    "\t\tvalid_indices = json.load(fileout)\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "\t# Select random protein from valid ones\n",
    "\tselected_idx = random.choice(valid_indices)\n",
    "\tselected_protein = struct_dat[selected_idx]\n",
    "\t\n",
    "\tprint(f\"\\n{'='*60}\")\n",
    "\tprint(f\"SELECTED PROTEIN:\")\n",
    "\tprint(f\"  Identifier: {selected_protein.identifier}\")\n",
    "\tprint(f\"  Number of residues: {selected_protein['AA'].x.shape[0]}\")\n",
    "\tprint(f\"  Average pLDDT: {selected_protein['plddt'].x.mean().item():.3f}\")\n",
    "\tprint(f\"  Dataset index: {selected_idx}\")\n",
    "\tprint(f\"{'='*60}\")\n",
    "else:\n",
    "\tprint(\"No proteins found matching the criteria!\")\n",
    "\tselected_protein = None\n",
    "\tselected_idx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "from Bio.PDB import PDBParser\n",
    "from foldtree2.src.AFDB_tools import grab_struct\n",
    "\n",
    "def getCAatoms(pdb_file):\n",
    "\tparser = PDBParser(QUIET=True)\n",
    "\tstructure = parser.get_structure('structure', pdb_file)\n",
    "\tca_atoms = []\n",
    "\tfor model in structure:\n",
    "\t\tfor chain in model:\n",
    "\t\t\tfor residue in chain:\n",
    "\t\t\t\tif 'CA' in residue and PDB.is_aa(residue):\n",
    "\t\t\t\t\tca_atoms.append(residue['CA'])\n",
    "\treturn ca_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader, HeteroData\n",
    "from scipy import sparse\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "def get_backbone(naa):\n",
    "\tbackbone_mat = np.zeros((naa, naa))\n",
    "\tbackbone_rev_mat = np.zeros((naa, naa))\n",
    "\tnp.fill_diagonal(backbone_mat[1:], 1)\n",
    "\tnp.fill_diagonal(backbone_rev_mat[:, 1:], 1)\n",
    "\treturn backbone_mat, backbone_rev_mat\n",
    "\n",
    "def sparse2pairs(sparsemat):\n",
    "\tsparsemat = sparse.find(sparsemat)\n",
    "\treturn np.vstack([sparsemat[0], sparsemat[1]])\n",
    "\n",
    "def decoder_reconstruction2aa(ords, device, verbose=False):\n",
    "\tdecoder.eval()\n",
    "\tif verbose:\n",
    "\t\tprint(ords)\n",
    "\t\n",
    "\tz = encoder.vector_quantizer.embeddings(ords).to('cpu')\n",
    "\t\n",
    "\tedge_index = torch.tensor([[i, j] for i in range(z.shape[0]) for j in range(z.shape[0])], dtype=torch.long).T\n",
    "\tgodnode_index = np.vstack([np.zeros(z.shape[0]), [i for i in range(z.shape[0])]])\n",
    "\tgodnode_rev = np.vstack([[i for i in range(z.shape[0])], np.zeros(z.shape[0])])\n",
    "\t\n",
    "\t# Generate a backbone for the decoder\n",
    "\tdata = HeteroData()\n",
    "\t\n",
    "\tdata['res'].x = z\n",
    "\tbackbone, backbone_rev = get_backbone(z.shape[0])\n",
    "\tbackbone = sparse.csr_matrix(backbone)\n",
    "\tbackbone_rev = sparse.csr_matrix(backbone_rev)\n",
    "\tbackbone = sparse2pairs(backbone)\n",
    "\tbackbone_rev = sparse2pairs(backbone_rev)\n",
    "\tpositional_encoding = converter.get_positional_encoding(z.shape[0], 256)\n",
    "\t\n",
    "\tif verbose:\n",
    "\t\tprint('positional encoding shape:', positional_encoding.shape)\n",
    "\t\n",
    "\tdata['res'].batch = torch.tensor([0 for i in range(z.shape[0])], dtype=torch.long)\n",
    "\tdata['positions'].x = torch.tensor(positional_encoding, dtype=torch.float32)\n",
    "\tdata['res', 'backbone', 'res'].edge_index = torch.tensor(backbone, dtype=torch.long)\n",
    "\tdata['res', 'backbone_rev', 'res'].edge_index = torch.tensor(backbone_rev, dtype=torch.long)\n",
    "\t\n",
    "\tif verbose:\n",
    "\t\tprint(data['res'].x.shape)\n",
    "\t\n",
    "\t# Add the godnode\n",
    "\tdata['godnode'].x = torch.tensor(np.ones((1, 5)), dtype=torch.float32)\n",
    "\tdata['godnode4decoder'].x = torch.tensor(np.ones((1, 5)), dtype=torch.float32)\n",
    "\tdata['godnode4decoder', 'informs', 'res'].edge_index = torch.tensor(godnode_index, dtype=torch.long)\n",
    "\tdata['res', 'informs', 'godnode4decoder'].edge_index = torch.tensor(godnode_rev, dtype=torch.long)\n",
    "\tdata['res', 'informs', 'godnode'].edge_index = torch.tensor(godnode_rev, dtype=torch.long)\n",
    "\tedge_index = edge_index.to(device)\n",
    "\t\n",
    "\tif verbose:\n",
    "\t\tprint(data)\n",
    "\t\n",
    "\tdata = data.to(device)\n",
    "\tallpairs = torch.tensor([[i, j] for i in range(z.shape[0]) for j in range(z.shape[0])], dtype=torch.long).T\n",
    "\tout = decoder(data, allpairs)\n",
    "\trecon_x = out['aa'] if 'aa' in out else None\n",
    "\tedge_probs = out['edge_probs'] if 'edge_probs' in out else None\n",
    "\tlogits = out['edge_logits'] if 'edge_logits' in out else None\n",
    "\n",
    "\tif verbose and edge_probs is not None:\n",
    "\t\tprint(edge_probs.shape)\n",
    "\t\n",
    "\taastr = None\n",
    "\t\n",
    "\tif edge_probs is not None:\n",
    "\t\tedge_probs = edge_probs.reshape((z.shape[0], z.shape[0]))\n",
    "\tif logits is not None:\n",
    "\t\tlogits = torch.sum(logits, dim=1).squeeze()\n",
    "\t\tlogits = logits.reshape((z.shape[0], z.shape[0]))\n",
    "\t\n",
    "\treturn aastr, edge_probs, logits, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23742ea1",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "These functions provide comprehensive visualization of encoder-decoder performance:\n",
    "- Contact map predictions vs true contacts\n",
    "- ROC curves and precision-recall analysis\n",
    "- Embedding sequence visualization\n",
    "- Bond angle and secondary structure predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef423e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logits_sequence_on_ax(selected_indices, num_embeddings, ax):\n",
    "    \"\"\"\n",
    "    Visualize embedding sequence as colored bands\n",
    "    \n",
    "    Args:\n",
    "        selected_indices: Discrete embedding indices\n",
    "        num_embeddings: Total number of embeddings\n",
    "        ax: Matplotlib axis to plot on\n",
    "    \"\"\"\n",
    "    from colour import Color\n",
    "    \n",
    "    # Generate color gradient\n",
    "    ord_colors = Color(\"red\").range_to(Color(\"blue\"), num_embeddings)\n",
    "    ord_colors = np.array([c.get_rgb() for c in ord_colors])\n",
    "    \n",
    "    # Map indices to colors\n",
    "    sequence_colors = ord_colors[selected_indices.cpu().numpy()]\n",
    "    \n",
    "    # Create canvas with wrapping for long sequences\n",
    "    max_width = 64\n",
    "    seq_len = len(sequence_colors)\n",
    "    rows = int(np.ceil(seq_len / max_width))\n",
    "    canvas = np.ones((rows, max_width, 3))\n",
    "    \n",
    "    for i in range(rows):\n",
    "        start = i * max_width\n",
    "        end = min((i + 1) * max_width, seq_len)\n",
    "        row_colors = sequence_colors[start:end]\n",
    "        canvas[i, :len(row_colors), :] = row_colors\n",
    "    \n",
    "    ax.imshow(canvas, aspect='auto')\n",
    "    ax.set_title('FT2 Alphabet State Sequence')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6012fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decoder_reconstruction(encoder, decoder, data_sample, device, num_embeddings, converter, epoch=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of decoder reconstruction performance\n",
    "    \n",
    "    Creates a 3x3 subplot grid showing:\n",
    "    - Row 1: Contact maps and distance matrices\n",
    "    - Row 2: ROC curves, precision-recall curves, correlation plots\n",
    "    - Row 3: Bond angles/SS prediction, edge logits, embedding sequence\n",
    "    \n",
    "    Args:\n",
    "        encoder: Trained encoder model\n",
    "        decoder: Trained decoder model\n",
    "        data_sample: Input data sample\n",
    "        device: PyTorch device\n",
    "        num_embeddings: Number of discrete embeddings\n",
    "        converter: PDB2PyG converter\n",
    "        epoch: Current epoch (optional, for title)\n",
    "        save_path: Path to save figure (optional)\n",
    "    \n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "        metrics_dict: Dictionary of computed metrics\n",
    "        sample_out: Decoder outputs\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "    from scipy.stats import pearsonr\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through encoder\n",
    "        z, vqloss = encoder(data_sample)\n",
    "        \n",
    "        # Get discrete indices\n",
    "        selected_indices = encoder.vector_quantizer.discretize_z(z.detach())[0]\n",
    "        \n",
    "        # Reconstruct using decoder\n",
    "        aastr, edge_probs, logits, sample_out = decoder_reconstruction2aa(selected_indices, device)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    \n",
    "    # Title\n",
    "    epoch_str = f\"Epoch {epoch} - \" if epoch is not None else \"\"\n",
    "    fig.suptitle(f'{epoch_str}Decoder Reconstruction Analysis', fontsize=16, y=0.995)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ROW 1: CONTACT MAPS AND DISTANCE MATRICES\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Plot 1: True contact map\n",
    "    true_contacts = data_sample.edge_index_dict[('res', 'contactPoints', 'res')].cpu().numpy()\n",
    "    naa = data_sample['AA'].x.shape[0]\n",
    "    true_contact_mat = np.zeros((naa, naa))\n",
    "    true_contact_mat[true_contacts[0], true_contacts[1]] = 1\n",
    "    \n",
    "    im0 = axs[0, 0].imshow(true_contact_mat, cmap='hot', interpolation='nearest')\n",
    "    axs[0, 0].set_title('True Contacts')\n",
    "    axs[0, 0].set_xlabel('Residue Index')\n",
    "    axs[0, 0].set_ylabel('Residue Index')\n",
    "    plt.colorbar(im0, ax=axs[0, 0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot 2: Predicted contact probabilities\n",
    "    im1 = axs[0, 1].imshow(edge_probs, cmap='hot', interpolation='nearest')\n",
    "    axs[0, 1].set_title('Predicted Contact Probabilities')\n",
    "    axs[0, 1].set_xlabel('Residue Index')\n",
    "    axs[0, 1].set_ylabel('Residue Index')\n",
    "    plt.colorbar(im1, ax=axs[0, 1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot 3: True distance matrix\n",
    "    coords = data_sample['coords'].x.cpu().numpy()\n",
    "    distance_matrix = np.sqrt(((coords[:, None, :] - coords[None, :, :]) ** 2).sum(axis=2))\n",
    "    \n",
    "    im2 = axs[0, 2].imshow(distance_matrix, cmap='viridis', interpolation='nearest')\n",
    "    axs[0, 2].set_title('True Distance Matrix (Å)')\n",
    "    axs[0, 2].set_xlabel('Residue Index')\n",
    "    axs[0, 2].set_ylabel('Residue Index')\n",
    "    plt.colorbar(im2, ax=axs[0, 2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ROW 2: ROC CURVES, PRECISION-RECALL, CORRELATION\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Flatten matrices for ROC/PR curves\n",
    "    true_flat = true_contact_mat.flatten()\n",
    "    pred_flat = edge_probs.flatten()\n",
    "    \n",
    "    # Plot 4: ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(true_flat, pred_flat)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axs[1, 0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axs[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axs[1, 0].set_xlim([0.0, 1.0])\n",
    "    axs[1, 0].set_ylim([0.0, 1.05])\n",
    "    axs[1, 0].set_xlabel('False Positive Rate')\n",
    "    axs[1, 0].set_ylabel('True Positive Rate')\n",
    "    axs[1, 0].set_title('ROC Curve - Contact Prediction')\n",
    "    axs[1, 0].legend(loc=\"lower right\")\n",
    "    axs[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(true_flat, pred_flat)\n",
    "    avg_precision = average_precision_score(true_flat, pred_flat)\n",
    "    \n",
    "    axs[1, 1].plot(recall, precision, color='blue', lw=2, label=f'PR (AP = {avg_precision:.3f})')\n",
    "    axs[1, 1].set_xlim([0.0, 1.0])\n",
    "    axs[1, 1].set_ylim([0.0, 1.05])\n",
    "    axs[1, 1].set_xlabel('Recall')\n",
    "    axs[1, 1].set_ylabel('Precision')\n",
    "    axs[1, 1].set_title('Precision-Recall Curve')\n",
    "    axs[1, 1].legend(loc=\"lower left\")\n",
    "    axs[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Correlation plot - predicted vs true distances\n",
    "    # Use contact probabilities vs true distances (inverted)\n",
    "    contact_threshold = 15.0  # Ångströms\n",
    "    true_distances_contacts = distance_matrix[distance_matrix < contact_threshold]\n",
    "    pred_probs_contacts = edge_probs[distance_matrix < contact_threshold]\n",
    "    \n",
    "    if len(true_distances_contacts) > 0:\n",
    "        correlation, _ = pearsonr(true_distances_contacts, pred_probs_contacts)\n",
    "        axs[1, 2].scatter(true_distances_contacts, pred_probs_contacts, alpha=0.5, s=10)\n",
    "        axs[1, 2].set_xlabel('True Distance (Å)')\n",
    "        axs[1, 2].set_ylabel('Predicted Contact Probability')\n",
    "        axs[1, 2].set_title(f'Distance vs Contact Prob (r = {correlation:.3f})')\n",
    "        axs[1, 2].grid(alpha=0.3)\n",
    "    else:\n",
    "        axs[1, 2].text(0.5, 0.5, 'No contacts < 15Å', ha='center', va='center', transform=axs[1, 2].transAxes)\n",
    "        axs[1, 2].set_title('Distance vs Contact Prob')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ROW 3: BOND ANGLES/SS, EDGE LOGITS, EMBEDDING SEQUENCE\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Plot 7: Bond angles and secondary structure\n",
    "    if 'angles' in sample_out and sample_out['angles'] is not None:\n",
    "        angles = sample_out['angles'].cpu().numpy()\n",
    "        true_angles = data_sample['bondangles'].x.cpu().numpy()\n",
    "        \n",
    "        # Plot predicted and true angles\n",
    "        for i, angle_name in enumerate(['Phi', 'Psi', 'Omega']):\n",
    "            axs[2, 0].plot(angles[:, i], label=f'Pred {angle_name}', alpha=0.7, linestyle='--')\n",
    "            axs[2, 0].plot(true_angles[:, i], label=f'True {angle_name}', alpha=0.7)\n",
    "        \n",
    "        axs[2, 0].set_xlabel('Residue Index')\n",
    "        axs[2, 0].set_ylabel('Angle (radians)')\n",
    "        axs[2, 0].set_title('Bond Angles Prediction')\n",
    "        axs[2, 0].legend(loc='upper right', fontsize=8)\n",
    "        axs[2, 0].grid(alpha=0.3)\n",
    "    else:\n",
    "        axs[2, 0].text(0.5, 0.5, 'No angles predicted', ha='center', va='center', transform=axs[2, 0].transAxes)\n",
    "        axs[2, 0].set_title('Bond Angles Prediction')\n",
    "    \n",
    "    # Plot 8: Edge logits heatmap\n",
    "    if logits is not None and logits.size > 0:\n",
    "        im7 = axs[2, 1].imshow(logits, cmap='hot', interpolation='nearest')\n",
    "        axs[2, 1].set_title('Edge Logits')\n",
    "        axs[2, 1].set_xlabel('Residue Index')\n",
    "        axs[2, 1].set_ylabel('Residue Index')\n",
    "        plt.colorbar(im7, ax=axs[2, 1], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axs[2, 1].text(0.5, 0.5, 'No edge logits', ha='center', va='center', transform=axs[2, 1].transAxes)\n",
    "        axs[2, 1].set_title('Edge Logits')\n",
    "    \n",
    "    # Plot 9: Embedding sequence visualization\n",
    "    plot_logits_sequence_on_ax(selected_indices, num_embeddings, axs[2, 2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics_dict = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'avg_precision': avg_precision,\n",
    "        'num_residues': naa,\n",
    "        'num_true_contacts': true_contact_mat.sum(),\n",
    "        'vq_loss': float(vqloss) if isinstance(vqloss, torch.Tensor) else vqloss\n",
    "    }\n",
    "    \n",
    "    if len(true_distances_contacts) > 0:\n",
    "        metrics_dict['distance_correlation'] = correlation\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    return fig, metrics_dict, sample_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617498",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "The training loop performs the following:\n",
    "1. Forward pass through encoder and decoder\n",
    "2. Compute multi-task losses (AA sequence, contacts, angles, secondary structure)\n",
    "3. Apply gradient accumulation and clipping\n",
    "4. Update learning rate according to schedule\n",
    "5. Generate visualizations and save checkpoints every N epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook\n",
    "# Reset dataloader with configured batch size\n",
    "train_loader = DataLoader(struct_dat, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "figurestack = []\n",
    "metrics_history = []\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_x = 0\n",
    "    total_loss_edge = 0\n",
    "    total_vq = 0\n",
    "    total_angles_loss = 0\n",
    "    total_loss_fft2 = 0\n",
    "    total_logit_loss = 0\n",
    "    total_ss_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm.notebook.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Forward pass with autocast for mixed precision\n",
    "        if use_mixed_precision:\n",
    "            with autocast():\n",
    "                z, vqloss = encoder(data)\n",
    "                data['res'].x = z\n",
    "                \n",
    "                # Forward pass through decoder\n",
    "                out = decoder(data, None)\n",
    "                edge_index = data.edge_index_dict.get(('res', 'contactPoints', 'res')) if hasattr(data, 'edge_index_dict') else None\n",
    "\n",
    "                # Edge reconstruction loss\n",
    "                logitloss = torch.tensor(0.0, device=device)\n",
    "                edgeloss = torch.tensor(0.0, device=device)\n",
    "                if edge_index is not None:\n",
    "                    edgeloss, logitloss = recon_loss_diag(data, edge_index, decoder, plddt=mask_plddt, key='edge_probs')\n",
    "                \n",
    "                # Amino acid reconstruction loss\n",
    "                xloss = aa_reconstruction_loss(data['AA'].x, out['aa'])\n",
    "                \n",
    "                # FFT2 loss\n",
    "                fft2loss = torch.tensor(0.0, device=device)\n",
    "                if 'fft2pred' in out and out['fft2pred'] is not None:\n",
    "                    fft2loss = F.smooth_l1_loss(torch.cat([data['fourier2dr'].x, data['fourier2di'].x], axis=1), out['fft2pred'])\n",
    "\n",
    "                # Angles loss\n",
    "                angles_loss = torch.tensor(0.0, device=device)\n",
    "                if out.get('angles') is not None:\n",
    "                    angles_loss = angles_reconstruction_loss(out['angles'], data['bondangles'].x, plddt_mask=data['plddt'].x if mask_plddt else None)\n",
    "\n",
    "                # Secondary structure loss\n",
    "                ss_loss = torch.tensor(0.0, device=device)\n",
    "                if out.get('ss_pred') is not None:\n",
    "                    if mask_plddt:\n",
    "                        mask = (data['plddt'].x >= plddt_threshold).squeeze()\n",
    "                        if mask.sum() > 0:\n",
    "                            ss_loss = F.cross_entropy(out['ss_pred'][mask], data['ss'].x[mask])\n",
    "                    else:\n",
    "                        ss_loss = F.cross_entropy(out['ss_pred'], data['ss'].x)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = (xweight * xloss + edgeweight * edgeloss + vqweight * vqloss + \n",
    "                        fft2weight * fft2loss + angles_weight * angles_loss + \n",
    "                        ss_weight * ss_loss + logitweight * logitloss)\n",
    "                \n",
    "                # Scale loss by gradient accumulation steps\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "        else:\n",
    "            # Non-mixed precision path\n",
    "            z, vqloss = encoder(data)\n",
    "            data['res'].x = z\n",
    "            \n",
    "            out = decoder(data, None)\n",
    "            edge_index = data.edge_index_dict.get(('res', 'contactPoints', 'res')) if hasattr(data, 'edge_index_dict') else None\n",
    "\n",
    "            logitloss = torch.tensor(0.0, device=device)\n",
    "            edgeloss = torch.tensor(0.0, device=device)\n",
    "            if edge_index is not None:\n",
    "                edgeloss, logitloss = recon_loss_diag(data, edge_index, decoder, plddt=mask_plddt, key='edge_probs')\n",
    "            \n",
    "            xloss = aa_reconstruction_loss(data['AA'].x, out['aa'])\n",
    "            \n",
    "            fft2loss = torch.tensor(0.0, device=device)\n",
    "            if 'fft2pred' in out and out['fft2pred'] is not None:\n",
    "                fft2loss = F.smooth_l1_loss(torch.cat([data['fourier2dr'].x, data['fourier2di'].x'], axis=1), out['fft2pred'])\n",
    "\n",
    "            angles_loss = torch.tensor(0.0, device=device)\n",
    "            if out.get('angles') is not None:\n",
    "                angles_loss = angles_reconstruction_loss(out['angles'], data['bondangles'].x, plddt_mask=data['plddt'].x if mask_plddt else None)\n",
    "\n",
    "            ss_loss = torch.tensor(0.0, device=device)\n",
    "            if out.get('ss_pred') is not None:\n",
    "                if mask_plddt:\n",
    "                    mask = (data['plddt'].x >= plddt_threshold).squeeze()\n",
    "                    if mask.sum() > 0:\n",
    "                        ss_loss = F.cross_entropy(out['ss_pred'][mask], data['ss'].x[mask])\n",
    "                else:\n",
    "                    ss_loss = F.cross_entropy(out['ss_pred'], data['ss'].x)\n",
    "\n",
    "            loss = (xweight * xloss + edgeweight * edgeloss + vqweight * vqloss + \n",
    "                    fft2weight * fft2loss + angles_weight * angles_loss + \n",
    "                    ss_weight * ss_loss + logitweight * logitloss)\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        if use_mixed_precision:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # Only update weights every gradient_accumulation_steps\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            if clip_grad:\n",
    "                if use_mixed_precision:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Step optimizer with scaler\n",
    "            if use_mixed_precision:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Step scheduler if it's a step-based scheduler\n",
    "            if scheduler_step_mode == 'step':\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Accumulate losses (unscaled for reporting)\n",
    "        total_loss_x += xloss.item()\n",
    "        total_logit_loss += logitloss.item()\n",
    "        total_loss_edge += edgeloss.item()\n",
    "        total_loss_fft2 += fft2loss.item()\n",
    "        total_angles_loss += angles_loss.item()\n",
    "        total_vq += vqloss.item() if isinstance(vqloss, torch.Tensor) else float(vqloss)\n",
    "        total_ss_loss += ss_loss.item()\n",
    "    \n",
    "    # Clean up any remaining gradients at epoch end\n",
    "    if len(train_loader) % gradient_accumulation_steps != 0:\n",
    "        if clip_grad:\n",
    "            if use_mixed_precision:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n",
    "        if use_mixed_precision:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Update learning rate for epoch-based schedulers\n",
    "    if scheduler_step_mode == 'epoch':\n",
    "        if scheduler_type == 'plateau':\n",
    "            scheduler.step(total_loss_x)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Get current learning rate for logging\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Compute average losses\n",
    "    avg_losses = {\n",
    "        'aa_loss': total_loss_x / len(train_loader),\n",
    "        'edge_loss': total_loss_edge / len(train_loader),\n",
    "        'vq_loss': total_vq / len(train_loader),\n",
    "        'fft2_loss': total_loss_fft2 / len(train_loader),\n",
    "        'angles_loss': total_angles_loss / len(train_loader),\n",
    "        'ss_loss': total_ss_loss / len(train_loader),\n",
    "        'logit_loss': total_logit_loss / len(train_loader)\n",
    "    }\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Learning Rate: {current_lr:.2e}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  AA Loss:     {avg_losses['aa_loss']:.4f}\")\n",
    "    print(f\"  Edge Loss:   {avg_losses['edge_loss']:.4f}\")\n",
    "    print(f\"  VQ Loss:     {avg_losses['vq_loss']:.4f}\")\n",
    "    print(f\"  FFT2 Loss:   {avg_losses['fft2_loss']:.4f}\")\n",
    "    print(f\"  Angles Loss: {avg_losses['angles_loss']:.4f}\")\n",
    "    print(f\"  SS Loss:     {avg_losses['ss_loss']:.4f}\")\n",
    "    print(f\"  Logit Loss:  {avg_losses['logit_loss']:.4f}\")\n",
    "    \n",
    "    # Save checkpoints and visualize every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Saving checkpoint at epoch {epoch+1}...\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        # Save models\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        torch.save(encoder.state_dict(), f'models/notebook_encoder_epoch_{epoch+1}.pt')\n",
    "        torch.save(decoder.state_dict(), f'models/notebook_decoder_epoch_{epoch+1}.pt')\n",
    "        \n",
    "        # Generate visualization\n",
    "        print(f\"Generating reconstruction visualization...\")\n",
    "        os.makedirs('figures', exist_ok=True)\n",
    "        \n",
    "        # Use selected protein or random sample\n",
    "        viz_sample = selected_protein if selected_protein is not None else data_sample\n",
    "        \n",
    "        fig, metrics, sample_output = visualize_decoder_reconstruction(\n",
    "            encoder, decoder, viz_sample, device, num_embeddings, \n",
    "            converter, epoch=epoch+1, save_path=f'figures/reconstruction_epoch_{epoch+1}.png'\n",
    "        )\n",
    "        \n",
    "        figurestack.append(fig)\n",
    "        \n",
    "        # Add epoch info to metrics\n",
    "        metrics['epoch'] = epoch + 1\n",
    "        metrics.update(avg_losses)\n",
    "        metrics_history.append(metrics)\n",
    "        \n",
    "        # Print reconstruction metrics\n",
    "        print(f\"\\nReconstruction Metrics:\")\n",
    "        print(f\"  ROC AUC:           {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"  Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "        if 'distance_correlation' in metrics:\n",
    "            print(f\"  Distance Corr:     {metrics['distance_correlation']:.4f}\")\n",
    "        print(f\"  Num Residues:      {metrics['num_residues']}\")\n",
    "        print(f\"  True Contacts:     {metrics['num_true_contacts']:.0f}\")\n",
    "        \n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Generated {len(figurestack)} visualization figures\")\n",
    "print(f\"\\nFinal Training Summary:\")\n",
    "print(f\"{'─'*80}\")\n",
    "for m in metrics_history[-5:]:  # Show last 5 checkpoints\n",
    "    print(f\"Epoch {m['epoch']:3d}: ROC AUC={m['roc_auc']:.4f}, AP={m['avg_precision']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48face3c",
   "metadata": {},
   "source": [
    "## Training Metrics Visualization\n",
    "\n",
    "This cell generates comprehensive plots showing how all metrics evolved during training:\n",
    "- ROC AUC and Average Precision\n",
    "- All loss components over time\n",
    "- Summary table of key metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(metrics_history) > 0:\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "    fig.suptitle('Training Metrics Over Time', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract data\n",
    "    epochs = [m['epoch'] for m in metrics_history]\n",
    "    roc_auc = [m['roc_auc'] for m in metrics_history]\n",
    "    avg_precision = [m['avg_precision'] for m in metrics_history]\n",
    "    aa_loss = [m['aa_loss'] for m in metrics_history]\n",
    "    edge_loss = [m['edge_loss'] for m in metrics_history]\n",
    "    vq_loss = [m['vq_loss'] for m in metrics_history]\n",
    "    angles_loss = [m['angles_loss'] for m in metrics_history]\n",
    "    ss_loss = [m['ss_loss'] for m in metrics_history]\n",
    "    logit_loss = [m['logit_loss'] for m in metrics_history]\n",
    "    \n",
    "    # ROC AUC\n",
    "    axes[0, 0].plot(epochs, roc_auc, 'b-o', linewidth=2, markersize=6)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('ROC AUC')\n",
    "    axes[0, 0].set_title('ROC AUC Score')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average Precision\n",
    "    axes[0, 1].plot(epochs, avg_precision, 'g-o', linewidth=2, markersize=6)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Average Precision')\n",
    "    axes[0, 1].set_title('Average Precision Score')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distance Correlation (if available)\n",
    "    if 'distance_correlation' in metrics_history[0]:\n",
    "        distance_corr = [m.get('distance_correlation', 0) for m in metrics_history]\n",
    "        axes[0, 2].plot(epochs, distance_corr, 'purple', marker='o', linewidth=2, markersize=6)\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('Correlation')\n",
    "        axes[0, 2].set_title('Distance Correlation')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 2].text(0.5, 0.5, 'No correlation data', ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "    \n",
    "    # AA Loss\n",
    "    axes[1, 0].plot(epochs, aa_loss, 'orange', marker='o', linewidth=2, markersize=6)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Amino Acid Reconstruction Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Edge Loss\n",
    "    axes[1, 1].plot(epochs, edge_loss, 'cyan', marker='o', linewidth=2, markersize=6)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Contact Prediction Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # VQ Loss\n",
    "    axes[1, 2].plot(epochs, vq_loss, 'magenta', marker='o', linewidth=2, markersize=6)\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Loss')\n",
    "    axes[1, 2].set_title('Vector Quantization Loss')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Angles Loss\n",
    "    axes[2, 0].plot(epochs, angles_loss, 'red', marker='o', linewidth=2, markersize=6)\n",
    "    axes[2, 0].set_xlabel('Epoch')\n",
    "    axes[2, 0].set_ylabel('Loss')\n",
    "    axes[2, 0].set_title('Bond Angles Reconstruction Loss')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SS Loss\n",
    "    axes[2, 1].plot(epochs, ss_loss, 'brown', marker='o', linewidth=2, markersize=6)\n",
    "    axes[2, 1].set_xlabel('Epoch')\n",
    "    axes[2, 1].set_ylabel('Loss')\n",
    "    axes[2, 1].set_title('Secondary Structure Prediction Loss')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Logit Loss\n",
    "    axes[2, 2].plot(epochs, logit_loss, 'green', marker='o', linewidth=2, markersize=6)\n",
    "    axes[2, 2].set_xlabel('Epoch')\n",
    "    axes[2, 2].set_ylabel('Loss')\n",
    "    axes[2, 2].set_title('Edge Logit Loss')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/training_metrics_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved training metrics summary to figures/training_metrics_summary.png\")\n",
    "else:\n",
    "    print(\"No metrics history available. Train for at least one checkpoint epoch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318877e",
   "metadata": {},
   "source": [
    "## Save Training Summary\n",
    "\n",
    "Save a comprehensive JSON summary of the training run for future analysis and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create comprehensive training summary\n",
    "training_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config_file': config_file if config_file else None,\n",
    "    'hyperparameters': {\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'gradient_accumulation_steps': gradient_accumulation_steps,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_embeddings': num_embeddings,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'use_mixed_precision': use_mixed_precision,\n",
    "        'mask_plddt': mask_plddt,\n",
    "        'plddt_threshold': plddt_threshold if mask_plddt else None,\n",
    "        'use_muon': use_muon,\n",
    "        'muon_lr': muon_lr if use_muon else None,\n",
    "        'adamw_lr': adamw_lr if use_muon else None,\n",
    "        'use_commitment_scheduling': use_commitment_scheduling,\n",
    "        'commitment_cost_final': commitment_cost_final,\n",
    "        'commitment_warmup_steps': commitment_warmup_steps if use_commitment_scheduling else None,\n",
    "        'scheduler_type': scheduler_type,\n",
    "        'warmup_steps': warmup_steps,\n",
    "    },\n",
    "    'loss_weights': {\n",
    "        'edgeweight': edgeweight,\n",
    "        'logitweight': logitweight,\n",
    "        'xweight': xweight,\n",
    "        'fft2weight': fft2weight,\n",
    "        'vqweight': vqweight,\n",
    "        'angles_weight': angles_weight,\n",
    "        'ss_weight': ss_weight,\n",
    "    },\n",
    "    'dataset': {\n",
    "        'path': dataset_path,\n",
    "        'total_samples': len(struct_dat),\n",
    "    },\n",
    "    'metrics_history': metrics_history,\n",
    "    'final_metrics': metrics_history[-1] if metrics_history else None,\n",
    "}\n",
    "\n",
    "# Save summary to JSON\n",
    "os.makedirs('models', exist_ok=True)\n",
    "summary_filename = f\"models/training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to: {summary_filename}\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "if metrics_history:\n",
    "    final = metrics_history[-1]\n",
    "    print(f\"  Epoch: {final['epoch']}\")\n",
    "    print(f\"  ROC AUC: {final['roc_auc']:.4f}\")\n",
    "    print(f\"  Average Precision: {final['avg_precision']:.4f}\")\n",
    "    print(f\"  AA Loss: {final['aa_loss']:.4f}\")\n",
    "    print(f\"  Edge Loss: {final['edge_loss']:.4f}\")\n",
    "    print(f\"  VQ Loss: {final['vq_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"  No metrics available\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
