{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5edb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100976af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "PyTorch Lightning version: 2.6.0\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch Lightning imports\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Project imports\n",
    "from torch_geometric.data import DataLoader\n",
    "from foldtree2.src import pdbgraph\n",
    "from foldtree2.src import encoder as ecdr\n",
    "from foldtree2.src.mono_decoders import MultiMonoDecoder\n",
    "from foldtree2.src.losses.losses import recon_loss_diag, aa_reconstruction_loss, angles_reconstruction_loss\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "#if torch.cuda.is_available():\n",
    "#    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "#    for i in range(torch.cuda.device_count()):\n",
    "#        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdf7f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  dataset_path: structs_train_final.h5\n",
      "  batch_size: 15\n",
      "  num_workers: 4\n",
      "  num_embeddings: 40\n",
      "  embedding_dim: 256\n",
      "  hidden_size: 256\n",
      "  num_epochs: 300\n",
      "  learning_rate: 5e-05\n",
      "  gradient_accumulation_steps: 1\n",
      "  clip_grad: True\n",
      "  mask_plddt: True\n",
      "  plddt_threshold: 0.3\n",
      "  scheduler_type: plateau\n",
      "  warmup_steps: 20\n",
      "  warmup_ratio: 0.05\n",
      "  edgeweight: 0.1\n",
      "  logitweight: 0.1\n",
      "  xweight: 1.0\n",
      "  fft2weight: 0.01\n",
      "  vqweight: 0.1\n",
      "  angles_weight: 0.1\n",
      "  ss_weight: 0.1\n",
      "  use_muon: True\n",
      "  muon_lr: 0.02\n",
      "  adamw_lr: 0.0001\n",
      "  weight_decay: 0.01\n",
      "  accelerator: gpu\n",
      "  devices: 1\n",
      "  strategy: auto\n",
      "  precision: 16-mixed\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 1.0\n",
      "  save_top_k: 3\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "config = {\n",
    "    # Data parameters\n",
    "    'dataset_path': 'structs_train_final.h5',\n",
    "    'batch_size': 15,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Model architecture parameters\n",
    "    'num_embeddings': 40,\n",
    "    'embedding_dim': 256,\n",
    "    'hidden_size': 256,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 300,\n",
    "    'learning_rate': 5e-5,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'clip_grad': True,\n",
    "    'mask_plddt': True,\n",
    "    'plddt_threshold': 0.3,\n",
    "    \n",
    "    # Scheduler parameters\n",
    "    'scheduler_type': 'plateau',  # Options: 'plateau', 'linear', 'cosine', 'cosine_with_restarts', 'polynomial'\n",
    "    'warmup_steps': 20,\n",
    "    'warmup_ratio': 0.05,\n",
    "    \n",
    "    # Loss weights\n",
    "    'edgeweight': 0.1,\n",
    "    'logitweight': 0.1,\n",
    "    'xweight': 1.0,\n",
    "    'fft2weight': 0.01,\n",
    "    'vqweight': 0.1,\n",
    "    'angles_weight': 0.1,\n",
    "    'ss_weight': 0.1,\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    'use_muon': True,\n",
    "    'muon_lr': 0.02,\n",
    "    'adamw_lr': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # Multi-GPU parameters\n",
    "    'accelerator': 'gpu',  # 'gpu', 'cpu', or 'auto'\n",
    "    'devices': 1,  # Use 1 GPU in Jupyter to avoid multiprocessing issues\n",
    "    'strategy': 'auto',  # 'auto' selects best strategy for single GPU\n",
    "    'precision': '16-mixed',  # Use mixed precision training\n",
    "    \n",
    "    # Logging parameters\n",
    "    'log_every_n_steps': 10,\n",
    "    'val_check_interval': 1.0,  # Validate every epoch\n",
    "    'save_top_k': 3,  # Save top 3 checkpoints\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4977a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmoi/projects/foldtree2\n"
     ]
    }
   ],
   "source": [
    "cd /home/dmoi/projects/foldtree2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97214822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmoi/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 4999 structures\n",
      "Feature dimensions:\n",
      "  Residue features: 857\n",
      "  Godnode features: 5\n",
      "  FFT2 (imaginary): 1300\n",
      "  FFT2 (real): 1300\n",
      "\n",
      "Sample data structure:\n",
      "HeteroDataBatch(\n",
      "  identifier=[5],\n",
      "  AA={\n",
      "    x=[1405, 20],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  R_true={\n",
      "    x=[1405, 3, 3],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  bondangles={\n",
      "    x=[1405, 3],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  coords={\n",
      "    x=[1405, 3],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  fourier1di={\n",
      "    x=[1405, 80],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  fourier1dr={\n",
      "    x=[1405, 80],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  fourier2di={\n",
      "    x=[5, 1300],\n",
      "    batch=[5],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  fourier2dr={\n",
      "    x=[5, 1300],\n",
      "    batch=[5],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  godnode={\n",
      "    x=[5, 5],\n",
      "    batch=[5],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  godnode4decoder={\n",
      "    x=[5, 5],\n",
      "    batch=[5],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  plddt={\n",
      "    x=[1405, 1],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  positions={\n",
      "    x=[1405, 256],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  res={\n",
      "    x=[1405, 857],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  ss={\n",
      "    x=[1405, 3],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  t_true={\n",
      "    x=[1405, 3],\n",
      "    batch=[1405],\n",
      "    ptr=[6],\n",
      "  },\n",
      "  (godnode4decoder, informs, res)={ edge_index=[2, 1405] },\n",
      "  (godnode, informs, res)={ edge_index=[2, 1405] },\n",
      "  (res, backbone, res)={\n",
      "    edge_index=[2, 2805],\n",
      "    edge_attr=[1400],\n",
      "  },\n",
      "  (res, backbonerev, res)={\n",
      "    edge_index=[2, 2805],\n",
      "    edge_attr=[1400],\n",
      "  },\n",
      "  (res, contactPoints, res)={\n",
      "    edge_index=[2, 41404],\n",
      "    edge_attr=[41404],\n",
      "  },\n",
      "  (res, hbond, res)={\n",
      "    edge_index=[2, 1602],\n",
      "    edge_attr=[1602],\n",
      "  },\n",
      "  (res, informs, godnode)={ edge_index=[2, 1405] },\n",
      "  (res, informs, godnode4decoder)={ edge_index=[2, 1405] },\n",
      "  (res, window, res)={\n",
      "    edge_index=[2, 2790],\n",
      "    edge_attr=[2790],\n",
      "  },\n",
      "  (res, windowrev, res)={\n",
      "    edge_index=[2, 1400],\n",
      "    edge_attr=[1400],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize data converter and dataset\n",
    "converter = pdbgraph.PDB2PyG(aapropcsv='./foldtree2/config/aaindex1.csv')\n",
    "struct_dat = pdbgraph.StructureDataset(config['dataset_path'])\n",
    "\n",
    "# Get sample data for model initialization\n",
    "temp_loader = DataLoader(struct_dat, batch_size=5, shuffle=False, num_workers=1)\n",
    "data_sample = next(iter(temp_loader))\n",
    "\n",
    "ndim = data_sample['res'].x.shape[1]\n",
    "ndim_godnode = data_sample['godnode'].x.shape[1]\n",
    "ndim_fft2i = data_sample['fourier2di'].x.shape[1]\n",
    "ndim_fft2r = data_sample['fourier2dr'].x.shape[1]\n",
    "\n",
    "print(f\"Dataset loaded: {len(struct_dat)} structures\")\n",
    "print(f\"Feature dimensions:\")\n",
    "print(f\"  Residue features: {ndim}\")\n",
    "print(f\"  Godnode features: {ndim_godnode}\")\n",
    "print(f\"  FFT2 (imaginary): {ndim_fft2i}\")\n",
    "print(f\"  FFT2 (real): {ndim_fft2r}\")\n",
    "print(f\"\\nSample data structure:\")\n",
    "print(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be49a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinDataModule defined\n"
     ]
    }
   ],
   "source": [
    "# Lightning Data Module\n",
    "class ProteinDataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for protein structure data.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=8, num_workers=4, train_split=0.9):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_split = train_split\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Split dataset into train and validation sets.\"\"\"\n",
    "        dataset_size = len(self.dataset)\n",
    "        train_size = int(self.train_split * dataset_size)\n",
    "        val_size = dataset_size - train_size\n",
    "        \n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n",
    "            self.dataset, [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset split: {train_size} train, {val_size} validation\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True if self.num_workers > 0 else False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True if self.num_workers > 0 else False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "print(\"ProteinDataModule defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d48ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinStructureModel defined\n"
     ]
    }
   ],
   "source": [
    "# Lightning Module for Protein Structure Model\n",
    "class ProteinStructureModel(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for encoder-decoder protein structure prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, ndim, ndim_godnode, ndim_fft2i, ndim_fft2r, converter):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['converter'])\n",
    "        self.config = config\n",
    "        self.converter = converter\n",
    "        \n",
    "        # Initialize encoder\n",
    "        self.encoder = ecdr.mk1_Encoder(\n",
    "            in_channels=ndim,\n",
    "            hidden_channels=[2*config['hidden_size'], 2*config['hidden_size'], 2*config['hidden_size']],\n",
    "            out_channels=config['embedding_dim'],\n",
    "            metadata={'edge_types': [('res','contactPoints','res')]},\n",
    "            num_embeddings=config['num_embeddings'],\n",
    "            commitment_cost=0.9,\n",
    "            edge_dim=1,\n",
    "            encoder_hidden=config['hidden_size'],\n",
    "            EMA=True,\n",
    "            nheads=10,\n",
    "            dropout_p=0.01,\n",
    "            reset_codes=False,\n",
    "            flavor='transformer',\n",
    "            fftin=True,\n",
    "            use_commitment_scheduling=True,\n",
    "            commitment_warmup_steps=1000,\n",
    "            commitment_schedule='cosine_with_restart',\n",
    "            commitment_start=0.5,\n",
    "            concat_positions=True\n",
    "        )\n",
    "        \n",
    "        # Initialize decoder (MultiMonoDecoder)\n",
    "        mono_configs = {\n",
    "            'sequence_transformer': {\n",
    "                'in_channels': {'res': config['embedding_dim']},\n",
    "                'xdim': 20,\n",
    "                'concat_positions': True,\n",
    "                'hidden_channels': {\n",
    "                    ('res','backbone','res'): [config['hidden_size']], \n",
    "                    ('res','backbonerev','res'): [config['hidden_size']]\n",
    "                },\n",
    "                'layers': 2,\n",
    "                'AAdecoder_hidden': [config['hidden_size'], config['hidden_size'], config['hidden_size']//2],\n",
    "                'amino_mapper': converter.aaindex,\n",
    "                'flavor': 'sage',\n",
    "                'nheads': 4,\n",
    "                'dropout': 0.001,\n",
    "                'normalize': False,\n",
    "                'residual': False,\n",
    "                'use_cnn_decoder': True,\n",
    "                'output_ss': False\n",
    "            },\n",
    "            'geometry_cnn': {\n",
    "                'in_channels': {\n",
    "                    'res': config['embedding_dim'], \n",
    "                    'godnode4decoder': ndim_godnode, \n",
    "                    'foldx': 23,\n",
    "                    'fft2r': ndim_fft2r, \n",
    "                    'fft2i': ndim_fft2i\n",
    "                },\n",
    "                'concat_positions': False,\n",
    "                'conv_channels': [config['hidden_size'], config['hidden_size']//2, config['hidden_size']//2],\n",
    "                'kernel_sizes': [3, 3, 3],\n",
    "                'FFT2decoder_hidden': [config['hidden_size']//2, config['hidden_size']//2],\n",
    "                'contactdecoder_hidden': [config['hidden_size']//2, config['hidden_size']//4],\n",
    "                'ssdecoder_hidden': [config['hidden_size']//2, config['hidden_size']//2],\n",
    "                'Xdecoder_hidden': [config['hidden_size']//2, config['hidden_size']//4],\n",
    "                'anglesdecoder_hidden': [config['hidden_size']//2, config['hidden_size']//4],\n",
    "                'RTdecoder_hidden': [config['hidden_size']//2, config['hidden_size']//4],\n",
    "                'metadata': converter.metadata,\n",
    "                'dropout': 0.001,\n",
    "                'output_fft': False,\n",
    "                'output_rt': False,\n",
    "                'output_angles': False,\n",
    "                'output_ss': True,\n",
    "                'normalize': True,\n",
    "                'residual': False,\n",
    "                'output_edge_logits': True,\n",
    "                'ncat': 8,\n",
    "                'contact_mlp': False,\n",
    "                'pool_type': 'global_mean'\n",
    "            },\n",
    "        }\n",
    "        self.decoder = MultiMonoDecoder(configs=mono_configs)\n",
    "        \n",
    "        # Store loss weights\n",
    "        self.automatic_optimization = False  # Manual optimization for gradient accumulation\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass through encoder and decoder.\"\"\"\n",
    "        z, vqloss = self.encoder(data)\n",
    "        data['res'].x = z\n",
    "        out = self.decoder(data, None)\n",
    "        return out, vqloss\n",
    "    \n",
    "    def compute_losses(self, batch, out, vqloss):\n",
    "        \"\"\"Compute all loss components.\"\"\"\n",
    "        device = self.device\n",
    "        \n",
    "        # Edge reconstruction loss\n",
    "        edge_index = batch.edge_index_dict.get(('res', 'contactPoints', 'res')) if hasattr(batch, 'edge_index_dict') else None\n",
    "        logitloss = torch.tensor(0.0, device=device)\n",
    "        edgeloss = torch.tensor(0.0, device=device)\n",
    "        if edge_index is not None:\n",
    "            edgeloss, logitloss = recon_loss_diag(\n",
    "                batch, edge_index, self.decoder, \n",
    "                plddt=self.config['mask_plddt'], \n",
    "                key='edge_probs'\n",
    "            )\n",
    "        \n",
    "        # Amino acid reconstruction loss\n",
    "        xloss = aa_reconstruction_loss(batch['AA'].x, out['aa'])\n",
    "        \n",
    "        # FFT2 loss\n",
    "        fft2loss = torch.tensor(0.0, device=device)\n",
    "        if 'fft2pred' in out and out['fft2pred'] is not None:\n",
    "            fft2loss = F.smooth_l1_loss(\n",
    "                torch.cat([batch['fourier2dr'].x, batch['fourier2di'].x], axis=1), \n",
    "                out['fft2pred']\n",
    "            )\n",
    "        \n",
    "        # Angles loss\n",
    "        angles_loss = torch.tensor(0.0, device=device)\n",
    "        if out.get('angles') is not None:\n",
    "            angles_loss = angles_reconstruction_loss(\n",
    "                out['angles'], \n",
    "                batch['bondangles'].x,\n",
    "                plddt_mask=batch['plddt'].x if self.config['mask_plddt'] else None\n",
    "            )\n",
    "        \n",
    "        # Secondary structure loss\n",
    "        ss_loss = torch.tensor(0.0, device=device)\n",
    "        if out.get('ss_pred') is not None:\n",
    "            if self.config['mask_plddt']:\n",
    "                mask = (batch['plddt'].x >= self.config['plddt_threshold']).squeeze()\n",
    "                ss_loss = F.cross_entropy(out['ss_pred'][mask], batch['ss'].x[mask])\n",
    "            else:\n",
    "                ss_loss = F.cross_entropy(out['ss_pred'], batch['ss'].x)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            self.config['xweight'] * xloss +\n",
    "            self.config['edgeweight'] * edgeloss +\n",
    "            self.config['vqweight'] * vqloss +\n",
    "            self.config['fft2weight'] * fft2loss +\n",
    "            self.config['angles_weight'] * angles_loss +\n",
    "            self.config['ss_weight'] * ss_loss +\n",
    "            self.config['logitweight'] * logitloss\n",
    "        )\n",
    "        \n",
    "        losses = {\n",
    "            'loss': total_loss,\n",
    "            'aa_loss': xloss,\n",
    "            'edge_loss': edgeloss,\n",
    "            'vq_loss': vqloss if isinstance(vqloss, torch.Tensor) else torch.tensor(vqloss, device=device),\n",
    "            'fft2_loss': fft2loss,\n",
    "            'angles_loss': angles_loss,\n",
    "            'ss_loss': ss_loss,\n",
    "            'logit_loss': logitloss\n",
    "        }\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step with manual optimization.\"\"\"\n",
    "        opt = self.optimizers()\n",
    "        \n",
    "        # Forward pass\n",
    "        out, vqloss = self(batch)\n",
    "        \n",
    "        # Compute losses\n",
    "        losses = self.compute_losses(batch, out, vqloss)\n",
    "        \n",
    "        # Scale loss by gradient accumulation steps\n",
    "        scaled_loss = losses['loss'] / self.config['gradient_accumulation_steps']\n",
    "        \n",
    "        # Manual backward\n",
    "        self.manual_backward(scaled_loss)\n",
    "        \n",
    "        # Update weights every gradient_accumulation_steps\n",
    "        if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:\n",
    "            if self.config['clip_grad']:\n",
    "                self.clip_gradients(opt, gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\")\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Step scheduler if using step-based\n",
    "            sch = self.lr_schedulers()\n",
    "            if sch is not None and self.config['scheduler_type'] != 'plateau':\n",
    "                sch.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        for key, value in losses.items():\n",
    "            self.log(f'train/{key}', value, on_step=True, on_epoch=True, prog_bar=(key == 'loss'), sync_dist=True)\n",
    "        \n",
    "        # Log learning rate\n",
    "        self.log('lr', opt.param_groups[0]['lr'], on_step=True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "        return losses['loss']\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        out, vqloss = self(batch)\n",
    "        losses = self.compute_losses(batch, out, vqloss)\n",
    "        \n",
    "        # Log validation metrics\n",
    "        for key, value in losses.items():\n",
    "            self.log(f'val/{key}', value, on_epoch=True, prog_bar=(key == 'loss'), sync_dist=True)\n",
    "        \n",
    "        return losses['loss']\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Update scheduler at end of validation epoch if using plateau scheduler.\"\"\"\n",
    "        sch = self.lr_schedulers()\n",
    "        if sch is not None and self.config['scheduler_type'] == 'plateau':\n",
    "            # Get validation loss\n",
    "            val_loss = self.trainer.callback_metrics.get('val/loss')\n",
    "            if val_loss is not None:\n",
    "                sch.step(val_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers and learning rate schedulers.\"\"\"\n",
    "        from transformers import (\n",
    "            get_linear_schedule_with_warmup,\n",
    "            get_cosine_schedule_with_warmup,\n",
    "            get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "            get_polynomial_decay_schedule_with_warmup\n",
    "        )\n",
    "        \n",
    "        if self.config['use_muon']:\n",
    "            from muon import MuonWithAuxAdam\n",
    "            \n",
    "            # Separate parameters for Muon optimizer\n",
    "            hidden_weights = []\n",
    "            hidden_gains_biases = []\n",
    "            nonhidden_params = []\n",
    "            \n",
    "            def has_modular_structure(model):\n",
    "                return hasattr(model, 'input') and hasattr(model, 'body') and hasattr(model, 'head')\n",
    "            \n",
    "            # Process encoder\n",
    "            if has_modular_structure(self.encoder):\n",
    "                hidden_weights += [p for p in self.encoder.body.parameters() if p.ndim >= 2]\n",
    "                hidden_gains_biases += [p for p in self.encoder.body.parameters() if p.ndim < 2]\n",
    "                nonhidden_params += [*self.encoder.head.parameters(), *self.encoder.input.parameters()]\n",
    "            else:\n",
    "                nonhidden_params += list(self.encoder.parameters())\n",
    "            \n",
    "            # Process decoder\n",
    "            if hasattr(self.decoder, 'decoders'):\n",
    "                for name, subdecoder in self.decoder.decoders.items():\n",
    "                    if has_modular_structure(subdecoder):\n",
    "                        hidden_weights += [p for p in subdecoder.body.parameters() if p.ndim >= 2]\n",
    "                        hidden_gains_biases += [p for p in subdecoder.body.parameters() if p.ndim < 2]\n",
    "                        nonhidden_params += [*subdecoder.head.parameters(), *subdecoder.input.parameters()]\n",
    "                    else:\n",
    "                        nonhidden_params += list(subdecoder.parameters())\n",
    "            else:\n",
    "                nonhidden_params += list(self.decoder.parameters())\n",
    "            \n",
    "            param_groups = [\n",
    "                dict(params=hidden_weights, use_muon=True,\n",
    "                     lr=self.config['muon_lr'], weight_decay=self.config['weight_decay']),\n",
    "                dict(params=hidden_gains_biases + nonhidden_params, use_muon=False,\n",
    "                     lr=self.config['adamw_lr'], betas=(0.9, 0.95), weight_decay=self.config['weight_decay']),\n",
    "            ]\n",
    "            optimizer = MuonWithAuxAdam(param_groups)\n",
    "        else:\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                list(self.encoder.parameters()) + list(self.decoder.parameters()),\n",
    "                lr=self.config['learning_rate']\n",
    "            )\n",
    "        \n",
    "        # Calculate total training steps\n",
    "        if self.trainer.max_epochs:\n",
    "            num_training_steps = self.trainer.estimated_stepping_batches\n",
    "        else:\n",
    "            num_training_steps = self.config['num_epochs'] * len(self.trainer.datamodule.train_dataloader())\n",
    "        \n",
    "        num_warmup_steps = self.config['warmup_steps'] if self.config['warmup_steps'] else int(num_training_steps * self.config['warmup_ratio'])\n",
    "        \n",
    "        # Configure scheduler\n",
    "        scheduler_type = self.config['scheduler_type']\n",
    "        \n",
    "        if scheduler_type == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n",
    "        elif scheduler_type == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n",
    "        elif scheduler_type == 'cosine_with_restarts':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n",
    "        elif scheduler_type == 'polynomial':\n",
    "            scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, lr_end=0.0, power=1.0)\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n",
    "        elif scheduler_type == 'plateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'val/loss'}}\n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "print(\"ProteinStructureModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff0201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 4499 train, 500 validation\n",
      "Data module initialized and ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data module\n",
    "data_module = ProteinDataModule(\n",
    "    dataset=struct_dat,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    train_split=0.9\n",
    ")\n",
    "\n",
    "# Setup the data module to get train/val splits\n",
    "data_module.setup()\n",
    "\n",
    "print(\"Data module initialized and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a197a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/dmoi/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing decoder for task: sequence_transformer\n",
      "False True False False False\n",
      "256 4 2 0.001\n",
      "Initializing decoder for task: geometry_cnn\n",
      "False False False False False\n",
      "Model initialized\n",
      "Encoder: mk1_Encoder\n",
      "Decoder: MultiMonoDecoder\n",
      "\n",
      "Parameter counts:\n",
      "  Encoder: 19,198,372\n",
      "  Decoder: 3,875,231\n",
      "  Total:   23,073,603\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ProteinStructureModel(\n",
    "    config=config,\n",
    "    ndim=ndim,\n",
    "    ndim_godnode=ndim_godnode,\n",
    "    ndim_fft2i=ndim_fft2i,\n",
    "    ndim_fft2r=ndim_fft2r,\n",
    "    converter=converter\n",
    ")\n",
    "\n",
    "print(\"Model initialized\")\n",
    "print(f\"Encoder: {type(model.encoder).__name__}\")\n",
    "print(f\"Decoder: {type(model.decoder).__name__}\")\n",
    "\n",
    "# Count parameters\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "decoder_params = sum(p.numel() for p in model.decoder.parameters())\n",
    "total_params = encoder_params + decoder_params\n",
    "\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Encoder: {encoder_params:,}\")\n",
    "print(f\"  Decoder: {decoder_params:,}\")\n",
    "print(f\"  Total:   {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ed9938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks and logger configured\n",
      "  Checkpoints will be saved to: models/lightning_checkpoints\n",
      "  TensorBoard logs will be saved to: runs/protein_lightning\n",
      "  Monitoring: val/loss (save top 3 models)\n"
     ]
    }
   ],
   "source": [
    "# Configure callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='models/lightning_checkpoints',\n",
    "    filename='protein-{epoch:02d}-{val/loss:.4f}',\n",
    "    monitor='val/loss',\n",
    "    mode='min',\n",
    "    save_top_k=config['save_top_k'],\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Configure logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='runs',\n",
    "    name='protein_lightning',\n",
    "    default_hp_metric=False\n",
    ")\n",
    "\n",
    "print(\"Callbacks and logger configured\")\n",
    "print(f\"  Checkpoints will be saved to: models/lightning_checkpoints\")\n",
    "print(f\"  TensorBoard logs will be saved to: runs/protein_lightning\")\n",
    "print(f\"  Monitoring: val/loss (save top {config['save_top_k']} models)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651aa9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Lightning Trainer configured\n",
      "\n",
      "Training Configuration:\n",
      "  Max epochs: 300\n",
      "  Accelerator: gpu\n",
      "  Strategy: auto\n",
      "  Precision: 16-mixed\n",
      "  Gradient accumulation steps: 1\n",
      "  Gradient clipping: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nif torch.cuda.is_available():\\n    print(f\"\\nAvailable GPUs: {torch.cuda.device_count()}\")\\n    for i in range(torch.cuda.device_count()):\\n        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\\n        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the Trainer for multi-GPU training\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config['num_epochs'],\n",
    "    accelerator=config['accelerator'],\n",
    "    devices=config['devices'],  # -1 uses all available GPUs\n",
    "    strategy=config['strategy'],  # DDP for multi-GPU\n",
    "    precision=config['precision'],  # Mixed precision training\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=config['log_every_n_steps'],\n",
    "    val_check_interval=config['val_check_interval'],\n",
    "    #gradient_clip_val=,\n",
    "    accumulate_grad_batches=config['gradient_accumulation_steps'],\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "print(\"PyTorch Lightning Trainer configured\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Max epochs: {config['num_epochs']}\")\n",
    "print(f\"  Accelerator: {config['accelerator']}\")\n",
    "#print(f\"  Devices: {config['devices']} ({'all GPUs' if config['devices'] == -1 else f'{config['devices']} GPUs'})\")\n",
    "print(f\"  Strategy: {config['strategy']}\")\n",
    "print(f\"  Precision: {config['precision']}\")\n",
    "print(f\"  Gradient accumulation steps: {config['gradient_accumulation_steps']}\")\n",
    "print(f\"  Gradient clipping: {config['clip_grad']}\")\n",
    "\n",
    "''' \n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nAvailable GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf73cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX PRO 4000 Blackwell') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING MULTI-GPU TRAINING WITH PYTORCH LIGHTNING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 4499 train, 500 validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmoi/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "/home/dmoi/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type             | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------\n",
      "0 | encoder | mk1_Encoder      | 19.2 M | train | 0    \n",
      "1 | decoder | MultiMonoDecoder | 3.9 M  | train | 0    \n",
      "-------------------------------------------------------------\n",
      "23.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.1 M    Total params\n",
      "92.294    Total estimated model params size (MB)\n",
      "141       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5baf8abd99470da694d287e0ae36eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.74 GiB. GPU 0 has a total capacity of 23.43 GiB of which 6.71 GiB is free. Including non-PyTorch memory, this process has 16.68 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 873.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTARTING MULTI-GPU TRAINING WITH PYTORCH LIGHTNING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAINING COMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    623\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    624\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    626\u001b[0m     ckpt_path,\n\u001b[1;32m    627\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m )\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1079\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1121\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1121\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1150\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1150\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:146\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:441\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    435\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    440\u001b[0m )\n\u001b[0;32m--> 441\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:329\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 329\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    332\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 206\u001b[0m, in \u001b[0;36mProteinStructureModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validation step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     out, vqloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_losses(batch, out, vqloss)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# Log validation metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 93\u001b[0m, in \u001b[0;36mProteinStructureModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through encoder and decoder.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     z, vqloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m z\n\u001b[1;32m     95\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(data, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/foldtree2/foldtree2/src/encoder.py:212\u001b[0m, in \u001b[0;36mmk1_Encoder.forward\u001b[0;34m(self, data, edge_attr_dict, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \tx_list \u001b[38;5;241m=\u001b[39m [conv(x, edge_index_dict[\u001b[38;5;28mtuple\u001b[39m(edge_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))], \n\u001b[1;32m    209\u001b[0m \t\t\t\t  edge_attr_dict[\u001b[38;5;28mtuple\u001b[39m(edge_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))]) \n\u001b[1;32m    210\u001b[0m \t\t\t \u001b[38;5;28;01mfor\u001b[39;00m edge_type, conv \u001b[38;5;129;01min\u001b[39;00m convs\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m \tx_list \u001b[38;5;241m=\u001b[39m [conv(x, edge_index_dict[\u001b[38;5;28mtuple\u001b[39m(edge_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))]) \n\u001b[1;32m    213\u001b[0m \t\t\t \u001b[38;5;28;01mfor\u001b[39;00m edge_type, conv \u001b[38;5;129;01min\u001b[39;00m convs\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    215\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    216\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(x)\n",
      "File \u001b[0;32m~/projects/foldtree2/foldtree2/src/encoder.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \tx_list \u001b[38;5;241m=\u001b[39m [conv(x, edge_index_dict[\u001b[38;5;28mtuple\u001b[39m(edge_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))], \n\u001b[1;32m    209\u001b[0m \t\t\t\t  edge_attr_dict[\u001b[38;5;28mtuple\u001b[39m(edge_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))]) \n\u001b[1;32m    210\u001b[0m \t\t\t \u001b[38;5;28;01mfor\u001b[39;00m edge_type, conv \u001b[38;5;129;01min\u001b[39;00m convs\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m \tx_list \u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43medge_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    213\u001b[0m \t\t\t \u001b[38;5;28;01mfor\u001b[39;00m edge_type, conv \u001b[38;5;129;01min\u001b[39;00m convs\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    215\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    216\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py:229\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    225\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor,\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m#                  edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                     \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.transformer_conv_TransformerConv_propagate_ccfam6_k.py:286\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, query, key, value, edge_attr, size)\u001b[0m\n\u001b[1;32m    274\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    275\u001b[0m                 query_i\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mquery_i,\n\u001b[1;32m    276\u001b[0m                 key_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkey_j,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    283\u001b[0m             )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:594\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maggregate\u001b[39m(\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    579\u001b[0m     inputs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:139\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dim_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()):\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered invalid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:131\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:185\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/foldtree2/lib/python3.9/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.74 GiB. GPU 0 has a total capacity of 23.43 GiB of which 6.71 GiB is free. Including non-PyTorch memory, this process has 16.68 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 873.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Start training!\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING MULTI-GPU TRAINING WITH PYTORCH LIGHTNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best model checkpoint: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best validation loss: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea9868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint for evaluation\n",
    "best_model = ProteinStructureModel.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path,\n",
    "    config=config,\n",
    "    ndim=ndim,\n",
    "    ndim_godnode=ndim_godnode,\n",
    "    ndim_fft2i=ndim_fft2i,\n",
    "    ndim_fft2r=ndim_fft2r,\n",
    "    converter=converter\n",
    ")\n",
    "\n",
    "best_model.eval()\n",
    "best_model.freeze()\n",
    "\n",
    "print(f\"Loaded best model from: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best validation loss: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1652c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training metrics from TensorBoard logs\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "def load_tensorboard_logs(log_dir):\n",
    "    \"\"\"Load and parse TensorBoard event files.\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(log_dir)\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Get all scalar tags\n",
    "    tags = ea.Tags()['scalars']\n",
    "    \n",
    "    data = {}\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        data[tag] = pd.DataFrame([(e.step, e.value) for e in events], columns=['step', tag])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Plot training curves\n",
    "def plot_training_metrics():\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Training Metrics - Multi-GPU Lightning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # This is a placeholder - actual implementation would read from TensorBoard logs\n",
    "    # or use the trainer's logged metrics\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('train/aa_loss', 'val/aa_loss', 'Amino Acid Loss'),\n",
    "        ('train/edge_loss', 'val/edge_loss', 'Edge Loss'),\n",
    "        ('train/vq_loss', 'val/vq_loss', 'VQ Loss'),\n",
    "        ('train/angles_loss', 'val/angles_loss', 'Angles Loss'),\n",
    "        ('train/ss_loss', 'val/ss_loss', 'Secondary Structure Loss'),\n",
    "        ('lr', None, 'Learning Rate')\n",
    "    ]\n",
    "    \n",
    "    for idx, (train_metric, val_metric, title) in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"Visualization functions defined\")\n",
    "print(\"Note: Run plot_training_metrics() after training to visualize results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05888c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model on a sample\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "# Get a random sample from validation set\n",
    "sample_idx = random.randint(0, len(data_module.val_dataset) - 1)\n",
    "data_sample = data_module.val_dataset[sample_idx]\n",
    "data_sample = data_sample.to(device)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    out, vqloss = best_model(data_sample)\n",
    "    \n",
    "print(f\"Sample ID: {data_sample.identifier}\")\n",
    "print(f\"\\nModel outputs:\")\n",
    "for key, value in out.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key}: {value.shape if hasattr(value, 'shape') else type(value)}\")\n",
    "print(f\"\\nVQ Loss: {vqloss.item() if isinstance(vqloss, torch.Tensor) else vqloss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d625540",
   "metadata": {},
   "source": [
    "## How to View TensorBoard Logs\n",
    "\n",
    "To visualize training metrics in TensorBoard, run the following command in a terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs/protein_lightning\n",
    "```\n",
    "\n",
    "Then open your browser to `http://localhost:6006` to view:\n",
    "- Training and validation loss curves\n",
    "- Learning rate schedules\n",
    "- Individual loss components (AA, edge, VQ, angles, SS, etc.)\n",
    "- GPU utilization and memory usage\n",
    "\n",
    "## Multi-GPU Training Benefits\n",
    "\n",
    "This Lightning implementation provides:\n",
    "1. **Automatic Data Parallelism**: Data is automatically distributed across all available GPUs using DDP\n",
    "2. **Gradient Accumulation**: Effective batch size = batch_size  gradient_accumulation_steps  num_gpus\n",
    "3. **Mixed Precision**: Automatic mixed precision (FP16) for faster training and reduced memory\n",
    "4. **Efficient Checkpointing**: Automatic model checkpointing with best model selection\n",
    "5. **Distributed Logging**: All metrics are properly synchronized across GPUs\n",
    "6. **Fault Tolerance**: Training can be resumed from checkpoints\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Monitor training progress in TensorBoard\n",
    "- Adjust hyperparameters in the config dictionary\n",
    "- Experiment with different schedulers and optimizers\n",
    "- Evaluate model on test set\n",
    "- Generate comprehensive visualizations of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47636826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of training configuration\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-GPU TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Data Configuration:\")\n",
    "print(f\"   Dataset: {config['dataset_path']}\")\n",
    "print(f\"   Total structures: {len(struct_dat):,}\")\n",
    "print(f\"   Training structures: {len(data_module.train_dataset):,}\")\n",
    "print(f\"   Validation structures: {len(data_module.val_dataset):,}\")\n",
    "print(f\"   Batch size per GPU: {config['batch_size']}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n  Hardware Configuration:\")\n",
    "    print(f\"   Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"   Effective batch size: {config['batch_size'] * torch.cuda.device_count() * config['gradient_accumulation_steps']}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "print(f\"\\n Model Configuration:\")\n",
    "print(f\"   Encoder: mk1_Encoder\")\n",
    "print(f\"   Decoder: MultiMonoDecoder\")\n",
    "print(f\"   Hidden size: {config['hidden_size']}\")\n",
    "print(f\"   Embedding dim: {config['embedding_dim']}\")\n",
    "print(f\"   Num embeddings: {config['num_embeddings']}\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\n  Training Configuration:\")\n",
    "print(f\"   Max epochs: {config['num_epochs']}\")\n",
    "print(f\"   Optimizer: {'Muon + AdamW' if config['use_muon'] else 'AdamW'}\")\n",
    "print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   Scheduler: {config['scheduler_type']}\")\n",
    "print(f\"   Mixed precision: {config['precision']}\")\n",
    "print(f\"   Gradient clipping: {config['clip_grad']}\")\n",
    "print(f\"   Gradient accumulation steps: {config['gradient_accumulation_steps']}\")\n",
    "\n",
    "print(f\"\\n Output Locations:\")\n",
    "print(f\"   Checkpoints: models/lightning_checkpoints/\")\n",
    "print(f\"   TensorBoard logs: runs/protein_lightning/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea87e12",
   "metadata": {},
   "source": [
    "## Visualization and Analysis\n",
    "\n",
    "The following cells provide utilities for visualizing model predictions and analyzing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a08154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c473386",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/dmoi/projects/foldtree2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b4da1",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# FoldTree2 Multi-GPU Training with PyTorch Lightning\n",
    "\n",
    "This notebook trains a protein structure prediction model using PyTorch Lightning for efficient multi-GPU training. The implementation replicates the logic from test_monodecoders.ipynb but leverages Lightning's distributed training capabilities.\n",
    "\n",
    "## Key Features\n",
    "- **Multi-GPU Support**: Automatically uses all available GPUs with DDP (Distributed Data Parallel)\n",
    "- **Vector Quantized Encoding**: Proteins encoded into discrete embedding sequences\n",
    "- **Multi-task Decoding**: Predicts amino acid sequences, contact maps, and geometric properties\n",
    "- **Mixed Precision Training**: Automatic mixed precision for faster training\n",
    "- **Advanced Optimizers**: Support for Muon and AdamW optimizers\n",
    "- **Learning Rate Scheduling**: Multiple scheduler options (linear, cosine, plateau, etc.)\n",
    "\n",
    "## Training Components\n",
    "The notebook demonstrates:\n",
    "- Custom LightningModule for encoder-decoder architecture\n",
    "- LightningDataModule for efficient data loading\n",
    "- Multi-GPU distributed training with DDP strategy\n",
    "- Comprehensive logging and visualization\n",
    "- Automatic checkpointing and model saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foldtree2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
